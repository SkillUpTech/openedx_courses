0
00:00:00,660 --> 00:00:02,800
OK! To create our model let's first

1
00:00:02,900 --> 00:00:06,100
review the big picture of what we're doing here?

2
00:00:06,200 --> 00:00:07,410
We're taking each record of

3
00:00:07,480 --> 00:00:11,060
784 pixels that's our input to our model.

4
00:00:11,330 --> 00:00:14,160
The active part of our model is going to be

5
00:00:14,280 --> 00:00:17,640
consisting of ten nodes each of them

6
00:00:17,700 --> 00:00:19,180
fully connected to the inputs

7
00:00:19,680 --> 00:00:21,160
each with their own set of weights.

8
00:00:21,220 --> 00:00:24,740
So we'll end up with ten by 784 weights.

9
00:00:25,220 --> 00:00:28,300
And then we'll have a bias for each of the nodes also over here

10
00:00:28,360 --> 00:00:31,980
ten bias values. And the output of that

11
00:00:32,120 --> 00:00:35,400
will then be used in a softmax

12
00:00:35,520 --> 00:00:37,960
and cross-entropy for a loss function

13
00:00:38,100 --> 00:00:39,500
we'll explain that when we get to it.

14
00:00:39,820 --> 00:00:42,720
Let's scroll down here a little also up.

15
00:00:43,600 --> 00:00:46,320
We have this notion of an input variable

16
00:00:46,400 --> 00:00:48,100
that we'll be using and that's where we

17
00:00:48,160 --> 00:00:50,300
can bind data to the model

18
00:00:51,020 --> 00:00:54,700
after we built the model. So the first thing we do

19
00:00:54,760 --> 00:00:57,800
is we say we're going to create an input variable

20
00:00:57,880 --> 00:01:02,180
whose shape is input dim which is 784.

21
00:01:02,600 --> 00:01:04,180
And we're going to call that input

22
00:01:04,220 --> 00:01:05,940
that's so we're actually put in our features.

23
00:01:06,000 --> 00:01:08,640
And next input variable is

24
00:01:08,760 --> 00:01:12,660
shaped or sized as num output classes which is 10.

25
00:01:12,760 --> 00:01:15,980
And that's going to be associated with our labeled data

26
00:01:17,060 --> 00:01:18,720
and then here's our model creation

27
00:01:18,800 --> 00:01:21,060
it's pretty small because this is a really simple model.

28
00:01:21,680 --> 00:01:23,453
Essentially, what we're doing is this

29
00:01:23,480 --> 00:01:26,420
kind of boilerplate code here

30
00:01:26,560 --> 00:01:29,720
where we say let's use default options

31
00:01:30,020 --> 00:01:32,260
and we're going to initialize our weights

32
00:01:32,440 --> 00:01:36,240
with this function called glorot uniform,

33
00:01:36,306 --> 00:01:38,840
a uniform distribution of values, random values.

34
00:01:39,340 --> 00:01:42,706
And then we're going to create layer called dense

35
00:01:42,760 --> 00:01:45,420
which means it's a fully called connected layer.

36
00:01:46,060 --> 00:01:47,500
The number of nodes in it is going to be

37
00:01:47,580 --> 00:01:49,640
the number of output classes which is 10.

38
00:01:50,120 --> 00:01:53,100
And then our activation function is none

39
00:01:53,180 --> 00:01:55,140
that's because we're going to take care

40
00:01:55,220 --> 00:01:59,180
of the softmax kind of activation inside the loss function.

41
00:01:59,680 --> 00:02:01,740
And we're passing it our input

42
00:02:01,820 --> 00:02:04,440
features here. And then we return

43
00:02:04,540 --> 00:02:07,680
the network that we've created, the model we've created is r.

44
00:02:08,666 --> 00:02:10,540
Here, we're going to actually call that,

45
00:02:10,653 --> 00:02:13,420
create model passing in our input binding

46
00:02:13,600 --> 00:02:15,200
for features called input

47
00:02:15,460 --> 00:02:18,800
and we'll do a divide of 255 right up front

48
00:02:19,080 --> 00:02:22,200
which will see normalize our input data because our

49
00:02:22,300 --> 00:02:24,640
input data is grayscale pixel values

50
00:02:24,690 --> 00:02:29,080
between 0 and 255. So by dividing by 255

51
00:02:29,120 --> 00:02:32,200
we'll end up with normalized values between 0 and 1

52
00:02:32,360 --> 00:02:35,040
which help our training work better.

53
00:02:38,140 --> 00:02:41,820
So next step is creating our loss function

54
00:02:41,980 --> 00:02:45,040
this will be used to guide our optimizer

55
00:02:45,100 --> 00:02:49,600
as we adjust weights and we're going to use cross entropy with softmax

56
00:02:49,740 --> 00:02:51,460
which we've discussed in the lecture

57
00:02:51,540 --> 00:02:53,800
it's-- it's a good loss function

58
00:02:53,860 --> 00:02:55,560
when you're doing with categorical data.

59
00:02:55,900 --> 00:02:58,400
The input to this is Z which is

60
00:02:58,460 --> 00:03:01,280
the output of our model and label

61
00:03:01,340 --> 00:03:03,820
which is our binding for our label data.

62
00:03:05,040 --> 00:03:08,520
And for our metric for evaluation

63
00:03:08,600 --> 00:03:11,660
we're going to use something called classification error

64
00:03:11,920 --> 00:03:14,340
which is just returns a 1 if the

65
00:03:14,400 --> 00:03:17,160
labels are the same and a 0 if they're different.

66
00:03:18,200 --> 00:03:21,140
And we'll store that again we pass in Z and label

67
00:03:21,560 --> 00:03:23,320
and we'll store that in label error.

68
00:03:23,940 --> 00:03:26,400
So now we have our loss in our error function

69
00:03:26,500 --> 00:03:28,500
we can actually create a trainer.

70
00:03:29,440 --> 00:03:31,020
So let's work backwards here

71
00:03:31,120 --> 00:03:32,640
this is the trainer we want to create

72
00:03:32,900 --> 00:03:35,780
CNTK dot trainer is the function to do it.

73
00:03:35,900 --> 00:03:39,026
We pass in Z we pass in our loss

74
00:03:39,120 --> 00:03:40,600
and our label error which we already have.

75
00:03:40,680 --> 00:03:43,740
And then we pass in-- in the list

76
00:03:43,820 --> 00:03:45,300
because you could have multiple learners.

77
00:03:45,380 --> 00:03:48,520
We pass in our learner and that's created just above here

78
00:03:49,000 --> 00:03:52,080
for that we're using CNTK dot sgd

79
00:03:52,260 --> 00:03:54,400
or Stochastic gradient descent.

80
00:03:55,100 --> 00:03:58,470
We give it our model Z and then dot parameters

81
00:03:58,540 --> 00:04:00,420
that returns the parameters of our model

82
00:04:00,500 --> 00:04:02,040
because it's going to have to adjust those

83
00:04:02,720 --> 00:04:05,160
And it's going to use a learning rate schedule

84
00:04:05,240 --> 00:04:07,340
that we've defined right up above here.

85
00:04:07,820 --> 00:04:10,986
And we created that with the standard

86
00:04:11,120 --> 00:04:13,260
learning rate schedule from CNTK.

87
00:04:13,380 --> 00:04:16,460
We gave it a learning rate of 0.2

88
00:04:17,000 --> 00:04:20,000
and then we said the style of learning

89
00:04:20,060 --> 00:04:24,000
is going to be minibatch. Alright!

90
00:04:24,380 --> 00:04:27,980
We've got a model, we've got a trainer.

91
00:04:28,100 --> 00:04:31,386
Here, we're going to define a couple of helper functions.

92
00:04:31,780 --> 00:04:33,920
This first one is called moving average

93
00:04:33,980 --> 00:04:35,573
and this is going to be used to

94
00:04:35,653 --> 00:04:38,060
plot some data that we gather during training.

95
00:04:38,820 --> 00:04:41,800
Essentially, what it does is this

96
00:04:42,560 --> 00:04:46,520
little messy kind of expression here

97
00:04:46,720 --> 00:04:51,120
is doing a sliding window average of five.

98
00:04:51,220 --> 00:04:53,880
So every five elements it

99
00:04:54,000 --> 00:04:55,813
creates a new average that slides over,

100
00:04:55,866 --> 00:04:57,460
and creates a new average on that.

101
00:04:57,986 --> 00:05:01,160
So that's just going to smooth out our data for plotting

102
00:05:01,280 --> 00:05:04,240
then over here we have print training progress

103
00:05:04,380 --> 00:05:08,120
we pass in a trainer minibatch number,

104
00:05:08,260 --> 00:05:11,000
the frequency with which we want to print

105
00:05:11,060 --> 00:05:13,420
and verbose flag set to one.

106
00:05:14,720 --> 00:05:18,600
And we say, hey if the minibatch mod frequency is zero

107
00:05:19,020 --> 00:05:22,720
that means, it's time to print. We will--

108
00:05:23,300 --> 00:05:25,020
take off the trainer object

109
00:05:25,140 --> 00:05:28,000
a field called previous minibatch loss average

110
00:05:28,100 --> 00:05:31,500
and previous minibatch evaluation average,

111
00:05:31,720 --> 00:05:33,760
will store those in two variables here.

112
00:05:34,000 --> 00:05:38,320
And if it's verbose to set which is by default,

113
00:05:38,440 --> 00:05:41,640
we will print out this information minibatch

114
00:05:41,740 --> 00:05:45,520
the number, the loss value, the four digits

115
00:05:45,700 --> 00:05:48,960
and the error value with a two decimal digits.

116
00:05:49,080 --> 00:05:51,200
And then there are those parameters

117
00:05:51,520 --> 00:05:53,920
and then finally we'll return

118
00:05:54,060 --> 00:05:56,280
all three of those values.

119
00:05:56,940 --> 00:05:59,020
Now we're ready to run the trainer,

120
00:05:59,920 --> 00:06:04,800
so, the first thing we're going to do is set some parameters the minibatch size

121
00:06:04,890 --> 00:06:11,570
is going to be 64. The number of samples per sweep that's 1 walk

122
00:06:11,640 --> 00:06:13,800
through all of our data is going to be 60,000.

123
00:06:13,880 --> 00:06:16,920
The number of sweeps to train with is 10.

124
00:06:16,990 --> 00:06:19,600
So that means we're going to go through all of our data ten times for a

125
00:06:19,640 --> 00:06:25,170
a total of 600,000 samples and the number of minibatches to train,

126
00:06:25,240 --> 00:06:29,500
we can calculate as a number of samples per sweep

127
00:06:29,550 --> 00:06:34,800
times the number of sweeps divided by the minibatch size. Ok,

128
00:06:34,890 --> 00:06:40,550
so now we've come to the actual minibatch loop, so we create our reader

129
00:06:40,600 --> 00:06:44,450
start and reader train, we give it our training file name

130
00:06:44,520 --> 00:06:48,770
we say True because we're training we give it the input dimension 784.

131
00:06:48,840 --> 00:06:53,120
The number of output classes 10, now we create an input map,

132
00:06:53,240 --> 00:06:58,550
this maps between our input variables is called label and input

133
00:06:58,620 --> 00:07:03,220
and the data in the stream that will be passing called the streams.label

134
00:07:03,290 --> 00:07:06,670
and streams.features, this is coming from our reader.

135
00:07:06,730 --> 00:07:12,750
We're going to set our progress output to 500 that mean every 500 minibatches

136
00:07:12,810 --> 00:07:18,470
will print the progress, we're going to set up a variable called plotdata,

137
00:07:18,520 --> 00:07:23,700
that's going to be holding it says batchsize,

138
00:07:23,760 --> 00:07:27,420
but it's really the minibatch number and then the loss value

139
00:07:27,470 --> 00:07:29,970
and the error value for that minibatch

140
00:07:30,060 --> 00:07:35,500
and then here's the--- the actual actual loop right here.

141
00:07:35,560 --> 00:07:41,220
So we set up a range to go from 0 to the number of minibatches to train minus 1,

142
00:07:41,340 --> 00:07:47,600
we call on the reader object, reader train, we call next minibatch,

143
00:07:47,740 --> 00:07:50,650
we give it a minibatch size and our input map.

144
00:07:50,730 --> 00:07:55,420
So that will actually get us the next, since minibatch size is 64

145
00:07:55,550 --> 00:08:02,120
it'll give us our next 64 records of training data and labels.

146
00:08:02,170 --> 00:08:08,650
And then we call trainer.train minibatch passing the data.

147
00:08:08,730 --> 00:08:11,870
So this is where our weights actually get adjusted based on

148
00:08:11,930 --> 00:08:16,370
the current predictions that it makes with the current set of weights.

149
00:08:16,440 --> 00:08:19,220
Then we'll call print training progress

150
00:08:19,290 --> 00:08:21,470
and we'll pass it the needed parameters

151
00:08:21,520 --> 00:08:27,150
and that will print every 500 minibatches

152
00:08:27,230 --> 00:08:29,420
and it will also return these three values

153
00:08:29,480 --> 00:08:31,850
that we're going to need to store in plotdata.

154
00:08:31,920 --> 00:08:34,750
So that we can plot them later

155
00:08:34,840 --> 00:08:38,370
and here you can see from the previous run,

156
00:08:38,430 --> 00:08:42,550
we have minibatch going from 0, 500,000 all the way to 9000.

157
00:08:42,640 --> 00:08:47,920
The law starts at 2.2 and goes lower and higher in some places

158
00:08:48,010 --> 00:08:50,400
and then back downed again seems to be converging

159
00:08:50,460 --> 00:08:54,120
and like wise the error starts pretty high at 80%

160
00:08:54,240 --> 00:08:59,700
goes down, goes up to 12, down to 4 up to 9 back down to 3.

161
00:08:59,770 --> 00:09:04,220
So it looks like it's converging to be great, if we could see this plotted

162
00:09:04,300 --> 00:09:09,120
so that's the next section of code. This uses Matplotlib

163
00:09:09,210 --> 00:09:13,600
to do the actual plotting, first though we're going to take our plotdata

164
00:09:13,710 --> 00:09:16,770
and we're going to run it through our moving average helper function

165
00:09:16,880 --> 00:09:20,700
that'll smooth out our data. So we start with plotdata loss,

166
00:09:20,760 --> 00:09:23,620
we store it in something called plotdata average loss

167
00:09:23,710 --> 00:09:28,450
and we take our plotdata error and we stored in plotdata avgerror.

168
00:09:28,540 --> 00:09:34,120
Now we import matplotlib.pyplot as we're going to call that plt

169
00:09:34,200 --> 00:09:38,000
and this will create a figure that we can plot into

170
00:09:38,090 --> 00:09:44,470
this is a one of two plots and we'll start with the first one

171
00:09:44,560 --> 00:09:48,300
and we're going to plot the plotdata batchsize,

172
00:09:48,380 --> 00:09:52,890
which is the minibatch number against,

173
00:09:52,970 --> 00:09:56,000
so this will be in the X and the Y would be the average loss

174
00:09:56,060 --> 00:10:02,420
and that's what we see here. And so this is loss and we can see it, converging down.

175
00:10:02,800 --> 00:10:07,720
We also only set the X label the Y label on the title.

176
00:10:07,740 --> 00:10:12,250
Let me show it then we go to the second plot and we do a similar thing

177
00:10:12,270 --> 00:10:17,400
or here we're back we're plotting the Minibatch number against,

178
00:10:17,420 --> 00:10:23,320
the average error and we set the labels and we set the title and we show it

179
00:10:23,340 --> 00:10:26,790
and so we can see that here.

180
00:10:26,810 --> 00:10:31,750
All right so it looks like both our loss and our error are converging

181
00:10:31,770 --> 00:10:38,270
and let's see how that model now holds up against test data.

182
00:10:38,290 --> 00:10:42,470
So to evaluate it on test data we're going to have another

183
00:10:42,490 --> 00:10:45,870
Minibatch loop like we did before it's going to look very similar only

184
00:10:45,890 --> 00:10:49,350
things we used to say train are now going to say test.

185
00:10:49,370 --> 00:10:55,020
So we have a reader underscore test the creating reader

186
00:10:55,040 --> 00:11:00,120
from test underscore file and this time this parameter for training is false

187
00:11:00,140 --> 00:11:04,120
and these two are the same. We're going to have a test input map

188
00:11:04,140 --> 00:11:08,400
that associates our input variable label

189
00:11:08,420 --> 00:11:12,600
with our streams.labels and our input variable called input

190
00:11:12,620 --> 00:11:15,350
with our stream stream.features.

191
00:11:15,370 --> 00:11:18,700
We set some parameters this time we're using

192
00:11:18,740 --> 00:11:24,800
any batchsize of 512 and that's okay.

193
00:11:24,820 --> 00:11:29,050
Because this is just test data we can actually run through as many

194
00:11:29,070 --> 00:11:33,370
records at a time as we want as long as they can all fit into memory, 512

195
00:11:33,390 --> 00:11:37,370
should be no problem at all. The number of samples is going to be 10,000

196
00:11:37,390 --> 00:11:40,550
that's how many samples we have in our test set

197
00:11:40,570 --> 00:11:42,650
and the number of Minibatches to test

198
00:11:42,670 --> 00:11:46,400
we're going to say is the number of samples divided by the Minibatch size

199
00:11:46,420 --> 00:11:51,300
and so far we're going to accumulate an average test result first a total

200
00:11:51,320 --> 00:11:53,450
and then we'll divide it by the number of entries.

201
00:11:53,470 --> 00:11:58,150
So it's going to start at 0 here's our for loop again,

202
00:11:58,170 --> 00:12:01,370
this time we're controlling it by the number of Minibatches to test.

203
00:12:01,390 --> 00:12:05,320
Here's our reader test getting the next batch of data,

204
00:12:05,340 --> 00:12:08,170
next Minibatch passing the size and the map.

205
00:12:08,190 --> 00:12:13,450
Then once we have the data we can call trainer.test this time last time

206
00:12:13,490 --> 00:12:17,320
it was just want to point this out this

207
00:12:17,340 --> 00:12:24,070
difference last time it was trainer.train Minibatch.

208
00:12:24,090 --> 00:12:30,270
So that actually does weight adjustment this call trainer.test Minibatch

209
00:12:30,290 --> 00:12:35,600
only does evaluation and it gives us back an error, evaluation error

210
00:12:35,620 --> 00:12:39,870
that's our metric and we're going to just accumulate that and test result

211
00:12:39,890 --> 00:12:42,070
and then at the end here.

212
00:12:42,090 --> 00:12:46,350
We'll say divide it by the number of Minibatches to test,

213
00:12:46,370 --> 00:12:50,200
that gives us an average we multiply it by a hundred to get a percentage

214
00:12:50,220 --> 00:12:54,200
and we print that out average test error and it with two decimal places

215
00:12:54,220 --> 00:12:57,950
and that's shown here 7.4 percent.

216
00:12:57,970 --> 00:13:01,800
So that's not bad for logistic regression really the simplest

217
00:13:01,820 --> 00:13:05,550
kind of model you can imagine,where we're in

218
00:13:05,570 --> 00:13:10,550
single-digit error ballpark that's pretty good.

219
00:13:10,570 --> 00:13:13,670
So now we're going to do a little prediction

220
00:13:13,690 --> 00:13:16,120
this kind of simulates a deployment environment.

221
00:13:16,140 --> 00:13:20,520
We want to take some data and predict just what the values are. So,

222
00:13:20,540 --> 00:13:25,100
we're going to extend our models with one node,

223
00:13:25,120 --> 00:13:30,150
we're going to actually put a soft max output now on z,

224
00:13:30,190 --> 00:13:32,800
which is the previous last node of the model and it's

225
00:13:32,820 --> 00:13:35,750
now going to give us a new output called out.

