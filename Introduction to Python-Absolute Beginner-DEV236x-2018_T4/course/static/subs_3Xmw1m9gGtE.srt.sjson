{
  "start": [
    670, 
    5190, 
    8110, 
    12270, 
    16337, 
    24000, 
    28095, 
    31110, 
    34390, 
    39370, 
    44320, 
    47790, 
    50790, 
    54040, 
    58550, 
    62010, 
    64970, 
    69220, 
    72170, 
    75680, 
    78650, 
    81820, 
    87020, 
    89672, 
    92525, 
    95887, 
    100650, 
    103590, 
    108440, 
    111086, 
    114410, 
    116130, 
    119520, 
    124238, 
    128940, 
    132710, 
    135240, 
    139510, 
    143300, 
    152120, 
    154245, 
    159230, 
    167330, 
    174070, 
    178372, 
    180434, 
    186525, 
    192797, 
    200417, 
    204127, 
    207320, 
    210834, 
    212910, 
    217659, 
    220040, 
    224600, 
    229344, 
    233446, 
    236758, 
    240950, 
    245121, 
    248527, 
    251354, 
    254777, 
    256350, 
    259330, 
    261650, 
    266720, 
    273520, 
    276090, 
    279060, 
    282270, 
    285780, 
    288700, 
    292830
  ], 
  "end": [
    5190, 
    8110, 
    12270, 
    16337, 
    21430, 
    28095, 
    31110, 
    34390, 
    39370, 
    42779, 
    47790, 
    50790, 
    54040, 
    58550, 
    60440, 
    64970, 
    67560, 
    72170, 
    75680, 
    78650, 
    81820, 
    87020, 
    89672, 
    92525, 
    95887, 
    100650, 
    103590, 
    108440, 
    111086, 
    114410, 
    116130, 
    119520, 
    124238, 
    128940, 
    132710, 
    135240, 
    139510, 
    143300, 
    148110, 
    154245, 
    159230, 
    167330, 
    174070, 
    178372, 
    180434, 
    186525, 
    192797, 
    196850, 
    204127, 
    207320, 
    210834, 
    212910, 
    217659, 
    220040, 
    224600, 
    229344, 
    233446, 
    236758, 
    240950, 
    245121, 
    248527, 
    251354, 
    254777, 
    256350, 
    259330, 
    261650, 
    266720, 
    272030, 
    276090, 
    279060, 
    282270, 
    285780, 
    286860, 
    292830, 
    293843
  ], 
  "text": [
    "Now that we have understood the details of the model,", 
    "we went into a little bit of depth.", 
    "How the parameters are calculated for each layers and", 
    "how they influence our decisions as to what the model", 
    "architecture should be, let's see how we can train this model.", 
    "Just like before, we create a loss function here,", 
    "cross_entropy_with_softmax.", 
    "And we also create an error function,", 
    "which will evaluate how many of the predictions we made", 
    "are correct or incorrect in this case.", 
    "We encapsulate that inside our criterion function.", 
    "We are just going to do this because, unlike the previous", 
    "tutorial, we are going to train a couple of models, and", 
    "you want to be able to reduce code duplication as always, so", 
    "we create some helper functions.", 
    "The following helper function helps us print the training", 
    "progress, which remains the same as the previous tutorial.", 
    "In this case, we are combining the training and", 
    "testing in one function itself, because you are not", 
    "quite familiar with the training and test process.", 
    "So we will do that, train and", 
    "test in one pass itself, in one function, one code block itself.", 
    "So we have the train_reader, we need the data for", 
    "reading the test value so that's the test_reader.", 
    "model func is the modeled function that you want to", 
    "train on, and then the number of sweeps you want to train on.", 
    "We scale the input values to a range of zero to one just like", 
    "before here, and then we instantiate loss and", 
    "the label_error functions using", 
    "the create_criterion_function here.", 
    "Rest of it is pretty boilerplate,", 
    "just like before you set the learning rate.", 
    "Instantiate the learner and then in this case you just", 
    "trainer.train_minibatch(data).", 
    "This is straight out of the train test predict workflow.", 
    "Where in the first stage, where we are training the model.", 
    "Once the model is trained, you can then take that model and", 
    "evaluate how well it did in previously unforeseen data", 
    "from the test batch, and we print the average test error.", 
    "So once we run the model,", 
    "here you can see we are calling the train_test function.", 
    "And we can see that the loss decreases over iterations and", 
    "the error also vastly reduces, and here the average tester.", 
    "Remember that in the previous MLP model we had errors", 
    "something like 1.7.", 
    "And with this simple convolution network", 
    "we have reduced the error by about .2%.", 
    "Let's see how we can do even better.", 
    "Before we do that, I would like you to also convince yourself", 
    "that the model indeed has learned some values.", 
    "So previously the z.classify.b.value had", 
    "the parameter set as zero.", 
    "Now you can see it has learned these values given", 
    "the training process.", 
    "So we have successfully been able to learn the parameters,", 
    "sometimes investigating the parameters is quite useful when", 
    "trying to debug models and see what's going on if your", 
    "results are not coming the way you expect.", 
    "The evaluation and prediction is the same as before.", 
    "We create our model, in this case, because we are going to", 
    "evaluate we just take the output of the model and", 
    "pass it through the soft max function.", 
    "And then we can do the evaluation just like that,", 
    "just like before.", 
    "Here you can see that most of the predictions", 
    "are correct in this sample.", 
    "The error rate is fairly, fairly low so we, it's like 1.52 which", 
    "means you'll probably get two, three errors in 200 samples.", 
    "It might be a worthwhile exercise to see what all images", 
    "were falsely labeled and you might be able to see some", 
    "interesting pattern why they might be falsely labeled.", 
    "Now see as a human whether that task would be easy for", 
    "you to do or not.", 
    "Here are randomly sampled data mined it was one pretty simple.", 
    "Looks pretty good."
  ]
}