0
00:00:01,370 --> 00:00:02,240
Congratulation, so

1
00:00:02,240 --> 00:00:07,570
you have built your first text classification model end to end.

2
00:00:08,650 --> 00:00:10,830
Let's see what other models may be possible

3
00:00:12,620 --> 00:00:13,790
with these technologies.

4
00:00:15,230 --> 00:00:18,550
This is one of the very popular ones.

5
00:00:18,550 --> 00:00:23,030
You may have heard about machine translation.

6
00:00:23,030 --> 00:00:25,340
This is where you'll give the input a certain sequence.

7
00:00:26,480 --> 00:00:29,780
The model would process that sequence and

8
00:00:29,780 --> 00:00:32,670
emit an output, say, in a completely different language.

9
00:00:34,720 --> 00:00:37,530
And that is how you would represent it in terms

10
00:00:37,530 --> 00:00:38,310
of network.

11
00:00:39,850 --> 00:00:41,140
Each of these words,

12
00:00:41,140 --> 00:00:45,630
hello how are you in English is pass to a recurrent unit.

13
00:00:47,190 --> 00:00:50,930
It composes history as you all know.

14
00:00:50,930 --> 00:00:55,910
This can be also called vectors.

15
00:00:55,910 --> 00:00:59,321
And once you do that, in response to a beginning of

16
00:00:59,321 --> 00:01:02,010
sentence token it would emit, hello.

17
00:01:03,150 --> 00:01:06,330
You can then take that hello and fill it as input

18
00:01:06,330 --> 00:01:09,590
to the next recurrent unit, then it would say, wie.

19
00:01:10,930 --> 00:01:14,548
You take the wie into the next recurrent unit and

20
00:01:14,548 --> 00:01:18,454
it would emit geht as your with the end of sequence.

21
00:01:18,454 --> 00:01:22,087
Sounds a bit magical, try out one of the tutorials that we

22
00:01:22,087 --> 00:01:26,020
have on our GitHub site, really, really works.

23
00:01:26,020 --> 00:01:29,045
And you should be able to have a lot of fun.

24
00:01:29,045 --> 00:01:33,332
Note that the number of input here and the number of output,

25
00:01:33,332 --> 00:01:37,446
they are not the same, so there are some tricks you have to

26
00:01:37,446 --> 00:01:40,050
apply to get that machinery going.

27
00:01:40,050 --> 00:01:44,390
We haven't discussed that in this particular set of lectures.

28
00:01:44,390 --> 00:01:47,340
In the more advanced lectures in the future,

29
00:01:47,340 --> 00:01:49,270
we will walk you through those details.

30
00:01:50,300 --> 00:01:54,420
Another area of sequence generation you may have run into

31
00:01:54,420 --> 00:01:56,480
is automatic caption generation.

32
00:01:56,480 --> 00:01:59,710
So here is person flying a kite.

33
00:02:00,720 --> 00:02:06,230
You can encode this image into a numeric code vector.

34
00:02:06,230 --> 00:02:10,222
You know that, you are seeing convolution networks, so

35
00:02:10,222 --> 00:02:13,466
you should be able to generate encoding of this

36
00:02:13,466 --> 00:02:16,313
image into a set of numerical entities.

37
00:02:16,313 --> 00:02:21,339
Which you then feed into a recurring block which would then

38
00:02:21,339 --> 00:02:26,071
generate text like a person on the beach flying a kite.

39
00:02:26,071 --> 00:02:30,626
So this would be an instance of having one input and

40
00:02:30,626 --> 00:02:33,020
generates many outputs.

41
00:02:34,170 --> 00:02:37,519
Here are some of the examples that you can find,

42
00:02:37,519 --> 00:02:41,138
where, this is a person flying a kite on the beach.

43
00:02:41,138 --> 00:02:43,290
This is a train.

44
00:02:43,290 --> 00:02:46,000
And the fact that this is a black-and-white photo can be

45
00:02:46,000 --> 00:02:49,260
recognized by these types of models.

46
00:02:49,260 --> 00:02:51,211
There's a person skiing down the slope.

47
00:02:51,211 --> 00:02:54,650
And there's a bunch of giraffes standing next to each other.

48
00:02:54,650 --> 00:02:58,380
All of these are texts that are generated

49
00:02:58,380 --> 00:03:01,420
from these images by a model like this.

50
00:03:02,740 --> 00:03:06,340
Details can be found out in this particular

51
00:03:06,340 --> 00:03:08,800
literature that you will have at the bottom of the screen.

52
00:03:10,810 --> 00:03:15,574
And this concludes our Module 6.

