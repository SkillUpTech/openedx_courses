{
  "start": [
    790, 
    6222, 
    10820, 
    14250, 
    15880, 
    18400, 
    24900, 
    29310, 
    34690, 
    38940, 
    43510, 
    49790, 
    53430, 
    58500, 
    62040, 
    65270, 
    69120, 
    72690, 
    76710, 
    82348, 
    85923, 
    89423, 
    92638, 
    95924, 
    99850, 
    104450, 
    109995, 
    111860, 
    114560, 
    116700, 
    124030, 
    129146, 
    134680, 
    140630, 
    145230, 
    149350, 
    153033, 
    157706, 
    160247, 
    165636, 
    170410, 
    172991, 
    177839, 
    182358, 
    187316, 
    197553, 
    200570, 
    209524, 
    212679, 
    217800, 
    222210, 
    228424, 
    232070, 
    236230, 
    242230, 
    246800, 
    249430, 
    252000, 
    255690, 
    257830, 
    261820, 
    263780, 
    269002, 
    276871, 
    282430, 
    291130, 
    295210, 
    297600, 
    300910, 
    305200, 
    307770, 
    309990, 
    313040, 
    314540, 
    318320, 
    324400, 
    329290, 
    333385, 
    338775, 
    341545, 
    343455, 
    346955, 
    353400, 
    357560, 
    363930, 
    368075, 
    370936, 
    375559, 
    379757, 
    384616, 
    388823, 
    390937, 
    396365, 
    400042, 
    403795, 
    409421, 
    414339, 
    418969, 
    423116, 
    427379, 
    431474, 
    436982, 
    439697, 
    443479, 
    445947, 
    450260, 
    453425, 
    458180, 
    463970, 
    472480, 
    474740, 
    476233, 
    480979, 
    485353, 
    488290, 
    492300
  ], 
  "end": [
    6222, 
    10820, 
    14250, 
    15880, 
    18400, 
    22120, 
    28125, 
    34690, 
    38940, 
    43510, 
    47660, 
    53430, 
    58500, 
    62040, 
    62880, 
    69120, 
    72690, 
    74850, 
    82348, 
    85923, 
    89423, 
    92638, 
    95924, 
    98010, 
    104450, 
    108030, 
    111860, 
    114560, 
    115620, 
    124030, 
    129146, 
    134680, 
    138790, 
    145230, 
    149350, 
    151673, 
    157706, 
    158986, 
    165636, 
    170410, 
    172991, 
    177839, 
    182358, 
    187316, 
    191410, 
    200570, 
    204725, 
    212679, 
    217800, 
    220920, 
    224435, 
    232070, 
    236230, 
    242230, 
    246800, 
    249430, 
    252000, 
    255690, 
    257830, 
    261820, 
    263780, 
    269002, 
    275364, 
    282430, 
    287480, 
    295210, 
    296350, 
    300910, 
    305200, 
    307770, 
    309990, 
    313040, 
    314540, 
    318320, 
    324400, 
    329290, 
    331935, 
    338775, 
    341545, 
    343455, 
    346955, 
    353400, 
    357560, 
    362900, 
    368075, 
    370936, 
    375559, 
    379757, 
    384616, 
    388823, 
    390937, 
    396365, 
    400042, 
    403795, 
    407415, 
    414339, 
    418969, 
    423116, 
    427379, 
    431474, 
    435335, 
    439697, 
    443479, 
    445947, 
    450260, 
    453425, 
    458180, 
    463970, 
    467050, 
    474740, 
    476233, 
    480979, 
    485353, 
    488290, 
    490200, 
    495775
  ], 
  "text": [
    "So what we do with this bowl-like function?", 
    "We first start with a randomly initialized parameter value.", 
    "Say this theta was initialized to", 
    "theta 1 using some random value.", 
    "So this would mean that the weights and the corresponding", 
    "biases would be initialized with some random value, say, here.", 
    "And we want to get to the bottom of the bowl.", 
    "If we flatten this, that we are starting from a point here,", 
    "and we want to get to this center point where", 
    "the loss function has the least difference", 
    "between the observed labels and the predicted labels.", 
    "If you think about the way one would do it is, start", 
    "marching along the slope of this bowl where it is the steepest.", 
    "That would lead to the fastest way you can reach this", 
    "minimum point.", 
    "Mathematically one will do here is compute the gradient", 
    "at any given point, so this would correspond to", 
    "this point on this surface that you see here.", 
    "You'll compute the gradient, the slope at that point and", 
    "you would step towards the minimum.", 
    "You'll take a step, and the direction of the step would be", 
    "on the opposite direction where you have the steepest", 
    "ascent because we want to descend along the curve, and", 
    "hence the name gradient descent.", 
    "You can see this value is referred to as the gradient", 
    "of the loss with respect to that particular parameter, set theta.", 
    "Mu is a new parameter, and", 
    "it's extremely important that you recognize this parameter.", 
    "It's called the learning rate.", 
    "This controls how much our first step you want to take.", 
    "If this mu is .001, you take", 
    "a tiny step, very small step.", 
    "And if you say, maybe at 1, then it will be a whole lot bigger.", 
    "This is an important parameter in deep learning and one has", 
    "to play with this parameter in a variety of different ways.", 
    "That you'll learn as you go along in this course.", 
    "This greatly helps finding the new set of parameters", 
    "theta prime.", 
    "So what we will do here is as shown in this animation,", 
    "you would start with the point say here you will", 
    "update the parameters.", 
    "And in each iteration you would inch your way towards", 
    "that final point in this lost function space that", 
    "would have the least error between the label data Y and", 
    "the predictions that are made by the model.", 
    "However computing this total loss for", 
    "large data set is expensive, and often redundant.", 
    "So typically with deep learning,", 
    "you do not compute the total loss across all the images.", 
    "In this case, n, as we have introduced so far it was 60k.", 
    "All the images in your training set.", 
    "There are more details that you can find on this link below.", 
    "But let me quickly take you to the variant of the gradient", 
    "descent which has vastly changed the way", 
    "you can scale up the learnings of very, very deep model.", 
    "So there are two variants of", 
    "the basic Gradient Descent Technique.", 
    "The first one is called Stochastic Gradient Descent.", 
    "So in Stochastic Gradient Descent,", 
    "you update the parameters much more frequently than you would", 
    "do in regular gradient descent.", 
    "So theta 1 to theta 2 in the case of gradient", 
    "descent was done using all the 60,000 images.", 
    "In Stochastic Gradient Descent, you update theta", 
    "1 to theta 2 by looking at one image at a time.", 
    "Consequently, what happens is, say your loss for", 
    "theta 1 was here.", 
    "And since you are looking at only one image,", 
    "the way the loss function gets reduced is very noisy.", 
    "In the sense that sometimes after one iteration", 
    "you would move in this direction.", 
    "The new loss point would be here, then the second one,", 
    "it might come here.", 
    "Third one might even go in the wrong direction.", 
    "But what we have seen is if you do this kind of optimization,", 
    "with one image at a time, one of the short comings is that it", 
    "might take a long time to reach the minimum.", 
    "So here is a variant of SGD, which is a hybrid of the concept", 
    "for Gradient Descent and the Stochastic Gradient Descent.", 
    "And what I mean by that is,", 
    "in this case you update the parameters using a mini-batch", 
    "set, which means that you don't update it with only one image.", 
    "You do not update it with the whole set either, but you can", 
    "define, say, at any given time you're going to compute the loss", 
    "from the first image to the 32nd image.", 
    "So for a mini batch set of 32,", 
    "this mini batch size indicates that there are 32", 
    "samples of images that you are going to take.", 
    "And the average loss for each of these images is gonna", 
    "be your loss for that particular iteration.", 
    "And based on this loss,", 
    "you will update your theta 1 to the corresponding data two.", 
    "Consequently what happens is you can start with a point,", 
    "it will still be wiggly, it won't be as smooth as it was for", 
    "the Gradient Descent, but it will be somewhat less so.", 
    "And the fact that you can load a certain set of data points in to", 
    "the memory rather quickly, the computational load that", 
    "comes across is very easily parallelizable.", 
    "I won't go into the details because that's beyond the scope", 
    "of the course that we have here, but you can read a lot about", 
    "the learners in the link that I have provided you here.", 
    "One of the important things, though,", 
    "to keep in mind is that SGD is not the only learner that is", 
    "available to you as a programmer and", 
    "as a person who is training deep learning models.", 
    "Here are some of the popular learners that you have.", 
    "Momentum-SGD, Nesterov, Adagrad, Adadelta,", 
    "and Adam, and you can see here in this animation", 
    "that they have different kinds of convergence property.", 
    "Some of them wiggle more then the others,", 
    "others are much smoother.", 
    "And at the end of the day all of them find the optimal operating", 
    "point which in this case is marked by the star this will be", 
    "your final value of the lose.", 
    "And this will be your model parameter.", 
    "You will choose in your final selection of the model."
  ]
}