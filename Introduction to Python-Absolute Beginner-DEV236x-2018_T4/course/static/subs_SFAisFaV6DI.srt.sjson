{
  "start": [
    550, 
    4380, 
    10540, 
    13330, 
    15820, 
    20618, 
    25712, 
    27956, 
    29700, 
    34880, 
    42720, 
    47529, 
    51690, 
    56942, 
    62191, 
    66958, 
    71756, 
    75556, 
    78976, 
    82168, 
    85580, 
    88230, 
    90790, 
    96180, 
    100390, 
    104230, 
    108340, 
    111510, 
    115500, 
    119630, 
    123680, 
    127616, 
    131472, 
    134380, 
    138896, 
    143939, 
    148091, 
    152160, 
    154175, 
    156900, 
    159930, 
    164328, 
    168724, 
    171975, 
    176540, 
    181271, 
    185504, 
    188615, 
    194354, 
    197229, 
    200244, 
    203395, 
    205879, 
    208915, 
    210985, 
    212860, 
    215205
  ], 
  "end": [
    4380, 
    10540, 
    13330, 
    14160, 
    20618, 
    22610, 
    27956, 
    29700, 
    34880, 
    40350, 
    47529, 
    51690, 
    56942, 
    62191, 
    66958, 
    71756, 
    75556, 
    78976, 
    82168, 
    85580, 
    88230, 
    90790, 
    96180, 
    100390, 
    102300, 
    108340, 
    111510, 
    115500, 
    119630, 
    123680, 
    127616, 
    131472, 
    134380, 
    138896, 
    140583, 
    148091, 
    152160, 
    154175, 
    156900, 
    159930, 
    164328, 
    168724, 
    171975, 
    176540, 
    181271, 
    185504, 
    188615, 
    194354, 
    197229, 
    200244, 
    203395, 
    205879, 
    208915, 
    210985, 
    212860, 
    215205, 
    218466
  ], 
  "text": [
    "In the context of the A test data that I have walked you", 
    "through, it is shown at the top right corner of the slide here.", 
    "Let's see how we can visualize the network", 
    "that we are gonna train.", 
    "So here is the data and we create the recurring steps for", 
    "each of these tokens.", 
    "The position of the tokens does matter,", 
    "as I've eluded to before.", 
    "So beginning of sentence, from Boston to Pittsburgh on Thursday", 
    "of the next week should be classified into these tags.", 
    "And that the common step that I showed before expands", 
    "this entire network for you automatically.", 
    "This is unique to this toolkit because think about, if you had,", 
    "instead of, how many tokens we have here, so 11, if you had,", 
    "say, 20 tokens, you would have to manually create this", 
    "unrolling for 20 times if the sentence length was 20.", 
    "In another instance, if you had a sentence that was, say,", 
    "length of 10 words, then you'll have to create and", 
    "manually roll out these recurrences shown here,", 
    "which can be really, really painful.", 
    "So in this toolkit that you are using,", 
    "that task is abstracted away from you.", 
    "It's automatically figured out how much to expand this graph", 
    "to achieve the task of tagging each of these word tokens", 
    "into their corresponding labels.", 
    "So, here, let's see what it means in terms of processing", 
    "the data as you would be feeding it into", 
    "the toolkit machinery that we are gonna use for the tutorials.", 
    "So say, you have a sentence, beginning of sentence from", 
    "Boston to Pittsburgh on Thursday of next week, end of sentence.", 
    "There are 11 tokens, each of them are one hard encoded.", 
    "Remember, we are using the one hard encoding as our input.", 
    "It's an array of 943 elements.", 
    "So each of these blocks are composed of an array with", 
    "943 elements.", 
    "Okay, it is very important to recognize the data format and", 
    "how the encodings are being passed into the entire trained", 
    "test and predict workflow.", 
    "So this is my text token encoding.", 
    "And then, it's pretty standard, what we have been using.", 
    "For the rest of the network, the corresponding label that", 
    "you will have is also gonna be a one hard encoded label set,", 
    "which is gonna be of the dimension 129.", 
    "Because there, we'll have 129 classes and this unit is gonna", 
    "magically unroll the recurrences across the 11th times steps and", 
    "show that the later time, your sequence change to different", 
    "length to automatically take care of that.", 
    "With that, let's compute the error or the loss function.", 
    "Because at the end of the day, you want to minimize", 
    "the difference between the emitted probability for", 
    "the 129 classes with the corresponding labels that", 
    "are provided as a part of the training set.", 
    "And this is pretty standard thing that we have used in", 
    "the past with all our other models,", 
    "especially with the MS date up.", 
    "And this is will be our lost function.", 
    "[BLANK AUDIO]"
  ]
}