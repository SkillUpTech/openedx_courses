0
00:00:01,530 --> 00:00:06,560
In this section, we will train a logistic regression model

1
00:00:06,560 --> 00:00:07,900
followed by testing it.

2
00:00:09,440 --> 00:00:13,950
And we will then have the final model that can be deployed

3
00:00:13,950 --> 00:00:16,070
as a web service or on the phone or

4
00:00:16,070 --> 00:00:21,845
any application while given an unforeseen digit.

5
00:00:21,845 --> 00:00:27,160
We will be able to classify it into the corresponding

6
00:00:27,160 --> 00:00:29,200
digit going from zero to nine.

7
00:00:31,600 --> 00:00:33,144
Let's start with our model here.

8
00:00:33,144 --> 00:00:37,939
Z, which is weight

9
00:00:37,939 --> 00:00:42,136
matrix times our

10
00:00:42,136 --> 00:00:47,550
input feature x + b.

11
00:00:47,550 --> 00:00:52,370
We will then need to compare the prediction made by the model

12
00:00:53,680 --> 00:00:55,860
with the labeled data.

13
00:00:55,860 --> 00:00:59,260
So each of these excess come with the corresponding labels.

14
00:00:59,260 --> 00:01:05,000
This is same as what we had called y, so what

15
00:01:05,000 --> 00:01:11,460
we are going to do is the loss or L for that particular image.

16
00:01:11,460 --> 00:01:14,400
Say we are looking at the I of instance so

17
00:01:14,400 --> 00:01:18,430
would be the loss function, whatever that might be,

18
00:01:18,430 --> 00:01:22,020
between the prediction made by the model which is for

19
00:01:22,020 --> 00:01:26,200
the sample, and the corresponding label.

20
00:01:30,440 --> 00:01:32,360
Now in the previous module,

21
00:01:32,360 --> 00:01:35,550
we had come up with different parameters.

22
00:01:35,550 --> 00:01:39,120
At that time, we had said it was m1b1.

23
00:01:40,150 --> 00:01:45,680
Then we had another one, m2 b2 all

24
00:01:45,680 --> 00:01:51,170
the way up to (mn, bn).

25
00:01:51,170 --> 00:01:52,340
In this case,

26
00:01:52,340 --> 00:01:56,520
the model parameters are going to be w and b.

27
00:01:57,820 --> 00:01:59,890
We'll refer to them as tita.

28
00:02:01,470 --> 00:02:07,890
Our goal is to find tita one, tita two, all the way to tita n,

29
00:02:07,890 --> 00:02:12,720
such that the last one is greater than loss two.

30
00:02:12,720 --> 00:02:15,680
Is all the way down to loss 10.

31
00:02:15,680 --> 00:02:19,410
That means we reduce the loss as we find out

32
00:02:19,410 --> 00:02:23,220
new model parameters that minimize the difference between

33
00:02:23,220 --> 00:02:26,260
the prediction made by the model and the corresponding label.

34
00:02:28,090 --> 00:02:30,550
Now we could arbitrarily search for

35
00:02:30,550 --> 00:02:33,930
these model parameters, randomly looking for them.

36
00:02:34,930 --> 00:02:39,540
However, that is time consuming and may not be very efficient.

37
00:02:40,930 --> 00:02:45,320
There are better ways of finding out these model parameters.

38
00:02:47,080 --> 00:02:51,030
In the world of machine learning, you probably have been

39
00:02:51,030 --> 00:02:54,700
introduced to the concept of gradient descent.

40
00:02:57,100 --> 00:03:00,140
We will use a variant of this gradient descent

41
00:03:00,140 --> 00:03:01,880
that is amenable to deep learning.

42
00:03:03,610 --> 00:03:08,515
And we'll see how that variant can help us build

43
00:03:08,515 --> 00:03:13,025
large models and find parameters where instead of in this

44
00:03:13,025 --> 00:03:16,875
case there are only two sets of parameters, weights and biases.

45
00:03:16,875 --> 00:03:19,965
These weight matrices can have larger number of

46
00:03:19,965 --> 00:03:24,055
elements inside it, which is the parameter of the model.

47
00:03:24,055 --> 00:03:28,305
But in deep learning these sets of parameters can be large.

48
00:03:28,305 --> 00:03:32,265
And the number of elements within those parameter sets make

49
00:03:32,265 --> 00:03:33,580
it even larger.

50
00:03:33,580 --> 00:03:37,450
Some of the models have hundred million plus parameters.

51
00:03:40,680 --> 00:03:43,770
How can we efficiently find the perimeter values

52
00:03:43,770 --> 00:03:47,139
that reduces the loss over multiple iterations.

53
00:03:48,930 --> 00:03:53,270
These are facilitated by techniques that are often

54
00:03:53,270 --> 00:03:55,550
leveraged from the optimization literature's.

55
00:03:55,550 --> 00:03:58,680
In the case of deep learning literature you will see them

56
00:03:58,680 --> 00:04:03,330
appear as learners, optimizers or solvers.

57
00:04:03,330 --> 00:04:04,673
They are synonymous.

58
00:04:07,063 --> 00:04:12,500
So what we do is, first compute the loss for a given image.

59
00:04:12,500 --> 00:04:16,280
For one image you have the probability of a given

60
00:04:16,280 --> 00:04:20,350
handwritten digit belonging to the particular class here,

61
00:04:20,350 --> 00:04:22,474
in this case, the handwritten digit is 3.

62
00:04:23,510 --> 00:04:26,720
And that is reflected with the large value that you can see

63
00:04:26,720 --> 00:04:29,420
here relative to the other values in this array.

64
00:04:29,420 --> 00:04:34,007
And this last function we had called it the cross entropy.

65
00:04:36,141 --> 00:04:39,889
You know that this probability vector is generated has

66
00:04:39,889 --> 00:04:42,420
the output of the soft max function.

67
00:04:44,220 --> 00:04:48,000
Later in the lecture, you will see that we will use a joint

68
00:04:48,000 --> 00:04:51,620
function called cross entropy with softmax that combines

69
00:04:51,620 --> 00:04:56,240
the cross entropy loss and the softmax operation in one step.

70
00:04:58,070 --> 00:05:02,160
But before we get to that stage, let's understand how we can

71
00:05:02,160 --> 00:05:06,590
identify parameters of our model that lead to

72
00:05:06,590 --> 00:05:09,530
that curve which we want to get to the loss function.

73
00:05:09,530 --> 00:05:15,580
Which, as we iterate, the loss would reduce progressively.

74
00:05:15,580 --> 00:05:17,290
So we start with computing the loss for

75
00:05:17,290 --> 00:05:19,790
one function where we take the first sample,

76
00:05:21,590 --> 00:05:23,880
compute its prediction made by the model,

77
00:05:23,880 --> 00:05:29,550
with some value of theta, and we compare it with the label.

78
00:05:30,660 --> 00:05:34,790
This label is the one hand-encoded label and

79
00:05:34,790 --> 00:05:38,740
we some it up across all the different classes that we want

80
00:05:38,740 --> 00:05:43,130
to predict and get the value of loss for that 1 image.

81
00:05:45,050 --> 00:05:49,040
We can add up the loss across all the images in our data set.

82
00:05:49,040 --> 00:05:53,650
In this case, there are 60,000 images in our training set and

83
00:05:53,650 --> 00:05:55,110
we can compute the total loss.

84
00:05:56,500 --> 00:05:59,569
This we will call it,

85
00:05:59,569 --> 00:06:05,377
as I is equal to 1 to mli moving forward.

86
00:06:09,021 --> 00:06:12,726
Now if you plot this last function over the different sets

87
00:06:12,726 --> 00:06:16,354
of parameter values that this variable theta can take,

88
00:06:16,354 --> 00:06:19,760
you'll see it comes like a bowl like function here.

89
00:06:21,670 --> 00:06:23,850
In this case, it's called a convex function,

90
00:06:23,850 --> 00:06:27,770
and a convex function can have 1 and only 1 minimum.

91
00:06:27,770 --> 00:06:31,230
The bowl like function is particularly true for

92
00:06:31,230 --> 00:06:36,170
convex function, not always you'll be the bowl like function

93
00:06:36,170 --> 00:06:40,610
but for logistic regression we have convex function and

94
00:06:40,610 --> 00:06:42,170
it has the shape of a bowl.

