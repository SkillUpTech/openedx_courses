{
  "start": [
    560, 
    5393, 
    9658, 
    16182, 
    17680, 
    19940, 
    23290, 
    28950, 
    31030, 
    36210, 
    40000, 
    47020, 
    50650, 
    52670, 
    58010, 
    62170, 
    66130, 
    69500, 
    74790, 
    78190, 
    83180, 
    87540, 
    92980, 
    98540, 
    104390, 
    110230, 
    116067, 
    120639, 
    124779, 
    127060, 
    133200, 
    138000, 
    140600, 
    145930, 
    148989, 
    155885, 
    160501, 
    163867, 
    167820, 
    172072, 
    177800, 
    181830, 
    187550, 
    190014, 
    195740, 
    199820, 
    203050, 
    207820, 
    213070, 
    218230, 
    224690, 
    228930, 
    234040, 
    236600, 
    239477, 
    245980, 
    248210, 
    251850, 
    256426, 
    263836, 
    269262, 
    275016, 
    280220, 
    282100, 
    286800, 
    291594, 
    298744, 
    304030
  ], 
  "end": [
    5393, 
    9658, 
    14018, 
    17680, 
    19940, 
    23290, 
    28950, 
    29760, 
    36210, 
    40000, 
    44840, 
    50650, 
    52670, 
    58010, 
    60320, 
    66130, 
    69500, 
    74790, 
    78190, 
    83180, 
    85370, 
    92980, 
    98540, 
    104390, 
    110230, 
    116067, 
    120639, 
    124779, 
    125570, 
    130190, 
    138000, 
    140600, 
    145930, 
    148989, 
    153232, 
    160501, 
    163867, 
    167820, 
    172072, 
    176444, 
    181830, 
    185668, 
    190014, 
    195740, 
    198570, 
    203050, 
    205808, 
    210930, 
    218230, 
    222910, 
    228930, 
    234040, 
    235030, 
    239477, 
    244470, 
    248210, 
    251850, 
    256426, 
    261461, 
    269262, 
    275016, 
    280220, 
    282100, 
    286800, 
    291594, 
    296487, 
    304030, 
    307530
  ], 
  "text": [
    "Let's start walking through the process where we", 
    "can learn this model, in our case, mx + b.", 
    "You want to learn these parameters m and b.", 
    "So this is Ystar.", 
    "This is given an arbitrary x.", 
    "We will predict the output of the solar panel.", 
    "You can use least square fit to come up with the estimates for", 
    "the m and b.", 
    "However, in this course, we want to use a machinery that can", 
    "not just fit a model with few variables, in this case", 
    "x was just a simple scalar, But rather large number of features.", 
    "So this machinery I'm gonna walk you through is gonna be used", 
    "throughout the course, and", 
    "it's fundamental to be able to train deep learning models.", 
    "So we'll take some time and walk you through the process.", 
    "So what you do is you sample certain amount of data.", 
    "So you take the readings for a certain", 
    "set of observations of X, which is my daytime temperatures for", 
    "whatever period of time you may have and", 
    "the corresponding predictions Y, okay.", 
    "This set is gonna be called my training set.", 
    "Now, what I want to do, I'm gonna take this training set and", 
    "predate the output of the solar panel using the model x + b.", 
    "So I'm gonna take the X which is the daytime temperature", 
    "as my input and predict these output of the solar panel.", 
    "Then you say, but wait a minute, how would I start with m and b?", 
    "So what we typically do in deep learning and machine learning,", 
    "one of the approaches is to start with a random assignment", 
    "of m and b.", 
    "So say we have m1 and b1.", 
    "What we get now is we have Y, and we have Ystar.", 
    "We can take the difference of that because for", 
    "each of these X's I know the corresponding Y's.", 
    "So I can take the difference and", 
    "I can square them and this becomes my loss.", 
    "Further more, I have a set of observations here, say n,", 
    "that means we draw a sample here, right,", 
    "which means we have n samples that are drawn.", 
    "So we can sum up the losses, 1 to n, and that is gonna", 
    "be the value that I would get in this curve right here.", 
    "Now imagine that I try out some other set of parameters.", 
    "So instead of m1 and b1, I get m2, b2.", 
    "And that gives me a loss l2,", 
    "loss 2,or let's make it easier on us, we'll call it l2.", 
    "So this becomes l2 falls right there.", 
    "We can continue to iterate and", 
    "search for different parameters mn, bn.", 
    "And at this point, say we got ln.", 
    "So the idea is that we find these m2, b2's, mn,", 
    "bn's such that the loss gets lower and lower.", 
    "So you would say that this would be my choice", 
    "of the model because it fits my training data with the least", 
    "amount of difference.", 
    "Now that's a big, big pitfall", 
    "that you want to prevent yourself from falling into.", 
    "What I mean is, when I reach this point,", 
    "you have a model that fits your training data very well.", 
    "So you have a model Z which is parameters by (mn,", 
    "bn) and it fits training data really, really well.", 
    "However, it's often the case that when you take data that", 
    "is not in the training sample, so not in the training sample,", 
    "the loss for not training becomes very high.", 
    "This doesn't help us,", 
    "because we want our models to be deployed at a later time when", 
    "data samples drawn from outside the training set should have", 
    "comparable performance as we had during the training phase.", 
    "This problem is very well known and is called overfitting.", 
    "In training model we need to be careful not to overfit."
  ]
}