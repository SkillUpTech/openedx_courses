0
00:00:01,820 --> 00:00:04,862
Now that you are quite familiar with the convolution operations,

1
00:00:04,862 --> 00:00:08,820
let's put together a pure convolutional network.

2
00:00:10,380 --> 00:00:15,530
We start with our MNIST image of digit 3.

3
00:00:15,530 --> 00:00:22,370
This is a greyscale image, so we specify the input

4
00:00:22,370 --> 00:00:27,340
with the dimensions of 1, 28, 28.

5
00:00:27,340 --> 00:00:33,270
And we're gonna use a filter size of 5 by 5,

6
00:00:33,270 --> 00:00:37,474
and we're gonna learn eight filters.

7
00:00:37,474 --> 00:00:39,860
We'll keep a stride of two.

8
00:00:41,460 --> 00:00:44,290
We'll set padding equals true.

9
00:00:44,290 --> 00:00:48,835
And we'll use activation equals value.

10
00:00:48,835 --> 00:00:52,940
At the end of this operation, when the image is processed,

11
00:00:52,940 --> 00:00:57,270
you will get eight 14 times 14 images.

12
00:00:59,400 --> 00:01:02,030
And then that becomes the input to the next layer.

13
00:01:03,950 --> 00:01:08,370
And in this layer, we're gonna keep the filter sizes the same,

14
00:01:08,370 --> 00:01:13,580
but the number of output filters we're gonna increase to 16.

15
00:01:13,580 --> 00:01:16,750
We'll keep rest of the parameters the same.

16
00:01:16,750 --> 00:01:19,015
This is the trend that you will notice, that,

17
00:01:19,015 --> 00:01:24,910
in convolutional network, that as you go build the layers,

18
00:01:24,910 --> 00:01:28,720
you tend to squeeze the image, the image sizes themselves will

19
00:01:28,720 --> 00:01:32,420
start getting smaller but the number of filters

20
00:01:32,420 --> 00:01:35,770
that you'll learn keeps getting deeper and deeper.

21
00:01:35,770 --> 00:01:40,130
So this is a key aspect of convolutional network

22
00:01:40,130 --> 00:01:42,330
where you keep squeezing the image.

23
00:01:42,330 --> 00:01:44,440
In this case we have only a 28 by 28 image,

24
00:01:44,440 --> 00:01:46,210
so you can only do it so much.

25
00:01:46,210 --> 00:01:49,882
But if you have a larger image, you can do a whole lot of things

26
00:01:49,882 --> 00:01:52,477
by playing around with these parameters.

27
00:01:52,477 --> 00:01:57,465
So the end of the second convolutional layer here,

28
00:01:57,465 --> 00:02:01,880
we have seven by seven and 16 of these here.

29
00:02:01,880 --> 00:02:08,120
So we would flatten that out at this stage into array, and

30
00:02:08,120 --> 00:02:13,390
map these values into output layer of 10 vectors.

31
00:02:13,390 --> 00:02:15,840
We need to get into this vector size of 10,

32
00:02:15,840 --> 00:02:20,760
because ultimate goal for us in this case is to characterize

33
00:02:20,760 --> 00:02:23,900
each of the images that are coming in as input

34
00:02:23,900 --> 00:02:27,946
to the corresponding digits from zero to nine.

35
00:02:27,946 --> 00:02:33,990
And, indeed once you ride this network and

36
00:02:33,990 --> 00:02:38,090
you run to a set of images you'll, for the m nest data,

37
00:02:38,090 --> 00:02:41,060
you will see that the output comes like a vector

38
00:02:41,060 --> 00:02:45,820
of 10 numbers with the largest number

39
00:02:45,820 --> 00:02:50,520
being the value corresponding to the digit three.

40
00:02:50,520 --> 00:02:52,280
And this is what the code for

41
00:02:52,280 --> 00:02:54,530
that particular network would look like.

42
00:02:54,530 --> 00:02:56,030
Here you can see it's a one-to-one

43
00:02:56,030 --> 00:02:58,230
correspondence between this diagram.

44
00:02:58,230 --> 00:03:02,070
Here the convolutional layer, this is the second convolutional

45
00:03:02,070 --> 00:03:04,737
layer, and then there's this dense layer.

46
00:03:08,829 --> 00:03:13,563
And with this type of network, when you run it through, you

47
00:03:13,563 --> 00:03:18,710
will see the error rate for the MNIST data goes down to 1.56.

48
00:03:18,710 --> 00:03:20,420
Significant jump from LR.

49
00:03:20,420 --> 00:03:27,340
If you've done the homework, the assignment for the feed

50
00:03:27,340 --> 00:03:31,050
forward network, you will see that this is an improvement

51
00:03:31,050 --> 00:03:35,930
over the multi-layer perceptron based network that we had.

52
00:03:35,930 --> 00:03:40,139
And we'll see how we can further reduce this error rate in

53
00:03:40,139 --> 00:03:42,076
the next several videos.

