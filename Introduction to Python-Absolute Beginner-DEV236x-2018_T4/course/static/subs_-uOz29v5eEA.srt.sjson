{
  "start": [
    800, 
    5630, 
    10576, 
    14943, 
    20826, 
    27170, 
    32360, 
    37970, 
    45490, 
    48932, 
    55970, 
    60434, 
    65274, 
    69250, 
    71851, 
    75549, 
    77337, 
    85861, 
    89760, 
    93230, 
    96900, 
    101240, 
    105950, 
    110470, 
    113530, 
    120412, 
    125120, 
    128750, 
    133790, 
    135820, 
    141868, 
    145305, 
    148752, 
    151130, 
    156620, 
    159190, 
    160380, 
    164580, 
    169497, 
    172402, 
    177113, 
    181163, 
    186310, 
    191129, 
    195390, 
    200607, 
    203376, 
    206997, 
    210200, 
    214470, 
    217407, 
    222371, 
    225836, 
    231519, 
    236981, 
    240752, 
    243215, 
    246373, 
    249093, 
    251041, 
    253977, 
    256770, 
    260576, 
    263438, 
    268435, 
    274380, 
    277760, 
    281970, 
    284370, 
    285940, 
    290335, 
    292732, 
    295025, 
    299546, 
    304620, 
    308738, 
    311664, 
    317144, 
    322040, 
    325637, 
    328104, 
    331080, 
    335673, 
    339702, 
    341990, 
    346469, 
    349776, 
    355220, 
    360151, 
    362229, 
    365399, 
    369116, 
    373056, 
    377515, 
    381339, 
    385612, 
    388200, 
    389985, 
    393470, 
    398347, 
    401703, 
    406314, 
    410936, 
    415349, 
    418500, 
    423298, 
    426975, 
    434480, 
    438112, 
    440617, 
    445533, 
    448757, 
    452830, 
    457000, 
    461210, 
    466765, 
    473845, 
    478306, 
    485699, 
    488720, 
    492280, 
    494840, 
    497810, 
    500800, 
    505550, 
    510213, 
    513464, 
    514682, 
    520781, 
    524452, 
    528163, 
    531010, 
    536764, 
    540919, 
    546261, 
    551040, 
    555858, 
    558739, 
    561930, 
    566304, 
    579726, 
    581505, 
    587005, 
    590645, 
    592200, 
    594950, 
    596440, 
    599000, 
    600430, 
    603730, 
    607799, 
    609502, 
    611710, 
    614350, 
    616340, 
    620040, 
    622278, 
    626669, 
    628877, 
    631923, 
    636210, 
    641240, 
    643990, 
    646600, 
    651790, 
    656950, 
    662810, 
    666140, 
    670420, 
    673770, 
    679590, 
    682300, 
    685691, 
    690567, 
    695830, 
    700630, 
    707860, 
    713210, 
    715380, 
    718340, 
    723610, 
    728325, 
    729900, 
    734100, 
    740025, 
    744717, 
    750610, 
    754242, 
    757957, 
    760497, 
    765670, 
    772380, 
    777190, 
    779172, 
    784055
  ], 
  "end": [
    5630, 
    10576, 
    14943, 
    20826, 
    25620, 
    30850, 
    37970, 
    45490, 
    48932, 
    54772, 
    60434, 
    65274, 
    69250, 
    71851, 
    75549, 
    77337, 
    85861, 
    89760, 
    93230, 
    96900, 
    100210, 
    105950, 
    110470, 
    113530, 
    120412, 
    125120, 
    128750, 
    133790, 
    135820, 
    141868, 
    145305, 
    148752, 
    151130, 
    156620, 
    159190, 
    160380, 
    164580, 
    169497, 
    172402, 
    177113, 
    181163, 
    186310, 
    191129, 
    195390, 
    200607, 
    203376, 
    206997, 
    208280, 
    214470, 
    217407, 
    222371, 
    225836, 
    231519, 
    236981, 
    240752, 
    243215, 
    246373, 
    249093, 
    251041, 
    253977, 
    255700, 
    260576, 
    263438, 
    268435, 
    274380, 
    277760, 
    281970, 
    284370, 
    285940, 
    290335, 
    292732, 
    295025, 
    299546, 
    303020, 
    308738, 
    311664, 
    317144, 
    322040, 
    325637, 
    328104, 
    331080, 
    335673, 
    339702, 
    341990, 
    346469, 
    349776, 
    353890, 
    360151, 
    362229, 
    365399, 
    369116, 
    373056, 
    377515, 
    381339, 
    385612, 
    388200, 
    389985, 
    391415, 
    398347, 
    401703, 
    406314, 
    410936, 
    415349, 
    418500, 
    423298, 
    425550, 
    434480, 
    438112, 
    440617, 
    445533, 
    448757, 
    451810, 
    457000, 
    461210, 
    466765, 
    473845, 
    478306, 
    482423, 
    488720, 
    492280, 
    494840, 
    497810, 
    500800, 
    505550, 
    510213, 
    513464, 
    514682, 
    520781, 
    524452, 
    528163, 
    531010, 
    536764, 
    540919, 
    546261, 
    549360, 
    555858, 
    558739, 
    561930, 
    566304, 
    571833, 
    581505, 
    587005, 
    590645, 
    592200, 
    594950, 
    596440, 
    599000, 
    600430, 
    603730, 
    607799, 
    609502, 
    611710, 
    614350, 
    616340, 
    617630, 
    622278, 
    626669, 
    628877, 
    631923, 
    636210, 
    641240, 
    643990, 
    646600, 
    651790, 
    656950, 
    662810, 
    666140, 
    670420, 
    673770, 
    679590, 
    682300, 
    685691, 
    690567, 
    695830, 
    700630, 
    707860, 
    712150, 
    715380, 
    718340, 
    722430, 
    728325, 
    729900, 
    734100, 
    740025, 
    744717, 
    750610, 
    754242, 
    757957, 
    760497, 
    765670, 
    772380, 
    777190, 
    779172, 
    784055, 
    787434
  ], 
  "text": [
    "Let's train the text classification model with", 
    "a recurrent network that you have just learned and", 
    "went through the code corresponding to it.", 
    "So in order to do the training, we need a loss function and", 
    "an error metric to track how well we are doing.", 
    "So let's see what we want to do here.", 
    "In a lot of places you would have the input", 
    "vocabulary size and the labeled size specified.", 
    "So in which case, you can directly use like x =", 
    "C.sequence.input(vocab_size), or similarly for label dimensions.", 
    "But in some cases, you may or may not have these inputs and", 
    "outputs ready at the time when you are defining your network.", 
    "In such situations, you use something called a placeholder.", 
    "So here you will see that I'm just showing for", 
    "illustrative purposes what that placeholder looks like here at", 
    "the bottom of the screen here.", 
    "You can create a container named placeholder.", 
    "You can create a placeholder container with the name labels.", 
    "The idea is that you would then be able to", 
    "use this container to define your loss and error.", 
    "Even though this labels container is not", 
    "properly shaped or formed, you can then replace at a later", 
    "time this container with the data that you will have.", 
    "So in this case, I am just showing you at the very bottom", 
    "of the screen that you can use the replace_placeholders", 
    "to put back what the data for that label would be.", 
    "In this case, this is a little bit contrived in the sense that", 
    "we already have the knowledge of our input label dimension,", 
    "so we don't need to do that.", 
    "So what we do here instead is a simple loss and", 
    "error metric function.", 
    "We call the criterion function, where both model and", 
    "labels are known ahead of time.", 
    "Which is, in our case, the input to the model is the vocabulary", 
    "or the words, converted into one-hot encoding.", 
    "So that would be x, and", 
    "the label, we have is the corresponding label, y.", 
    "So in the train function we take the model function,", 
    "and we pass the input encoded,", 
    "one-hot encoded vector through that function.", 
    "We get our model, we then can create our loss and", 
    "label errors for using this particular function here.", 
    "Note that I am already passing the model for this,", 
    "which was z in many of the tutorials before.", 
    "And y is the label in this case.", 
    "We define some of these training parameters,", 
    "that would be the number of samples in your minibatches, and", 
    "then the epoch size.", 
    "We define a learning rate here.", 
    "And if you see this notation here,", 
    "you will find that there is not a single learning rate which", 
    "we have been using in many cases before.", 
    "So what we do here is that we start with a larger learning", 
    "rate in the beginning part of the training and then.", 
    "So we don't use that same learning rate through all", 
    "the iterations of the training.", 
    "So we will use it for the first four epochs.", 
    "Then we will use the learning rate a bit and train it for", 
    "a certain number of more iterations.", 
    "And then, we will use the remaining number of iterations", 
    "with a different learning rate.", 
    "And that gives you a better control over how", 
    "the optimization progresses.", 
    "And is often useful to explore these kind of learning rate", 
    "based experimentation to achieve the best performance.", 
    "We are also gonna use the AdamOptimizer here,", 
    "because it works really well with text related datasets.", 
    "And you can play with the different parameters associated", 
    "with AdamOptimizer.", 
    "You can learn more about AdamOptimizers at this site,", 
    "with the link provided here.", 
    "And it has a parameter called momentum.", 
    "We will not go into describing what momentum means, but", 
    "it enables us to get a better convergence.", 
    "So in this tutorial, we are going to use the AdamOptimizer,", 
    "which works very well with text datasets.", 
    "And is a default optimizer for many of the tasks related", 
    "with training different kinds of classifiers.", 
    "And definitely, you should try this first,", 
    "when it comes to textual data.", 
    "You can play around with the different parameters associated.", 
    "I described you the learning rate schedule.", 
    "And you can also try out the momentum parameters", 
    "associated with it.", 
    "Now, the details of the momentum tech parameter can be learned", 
    "in the documentation for the AdamOptimizer.", 
    "We won't have time or to go into the details of it.", 
    "And here, we are using a built-in progress_printer that", 
    "comes with the tool kit.", 
    "You should leverage the different", 
    "facilities that the progress_printer", 
    "provides before you try to build your own.", 
    "We'll instantiate the trainer here, the model is passed", 
    "the label and the label_error, sorry the loss.", 
    "Here we instantiate the trainer where we pass the model,", 
    "the loss, and the label_error.", 
    "We pass the learner, and additionally,", 
    "we pass the progress_printer.", 
    "Here, we are logging the number of parameters in the model,", 
    "and then our training loop is right here.", 
    "We read some data, input x, the words, and", 
    "the y, which is the corresponding labels.", 
    "We run the trainer.train_minibatch,", 
    "which will update the various", 
    "parameters in our models using the learner.", 
    "And we will summarize the training progress.", 
    "Here, now let's run the model that we have created.", 
    "We will call the model function that gives us the function z,", 
    "which you are quite familiar with by now.", 
    "We will train this model z.", 
    "We pass in the reader that will read the data", 
    "from the corresponding training file.", 
    "And here you can see, that in a very short period,", 
    "we are able to reduce the loss quite significantly.", 
    "And the error metric also goes down quite a bit.", 
    "If you have a GPU based setup, I would recommend that you use it.", 
    "Because, A, your learning experience is much better, and", 
    "you can train on a lot more data in a fraction of the time.", 
    "Now that we have trained the model,", 
    "let's evaluate the learned model.", 
    "In this case, we create and", 
    "evaluate their function because we are gonna be", 
    "training different types of models, different models here.", 
    "So it helps to encapsulate all the evaluation components,", 
    "just like we did the training components in our function.", 
    "You take the input, you have the model function and", 
    "create the model.", 
    "And then you can, just similar to the trainer,", 
    "you read the training data.", 
    "In this case, this would be passed in through the reader,", 
    "and then you call the evaluator.", 
    "And here you know that only the last function is needed.", 
    "When you test the already trained model,", 
    "you can then iterate through all your test dataset and", 
    "summarize the test progress.", 
    "Here, let's see how it looks like when you test it", 
    "on at this data set test data.", 
    "So we pass in the test data to the reader.", 
    "And we pass the same model, z.", 
    "Remember, we have the z being the model we trained here.", 
    "And we run the test.", 
    "You can see that the metric that we are getting here is 0.48%.", 
    "Reasonably comparable with what we get here at the end", 
    "of training.", 
    "We will see how, as we introduce different components,", 
    "things change.", 
    "We will play around with the different kinds of model tweaks", 
    "you can make.", 
    "But before we do that, let's see the model for", 
    "the learning classify, we want to check the bias value.", 
    "In the beginning of the tutorial,", 
    "when we printed this value, it was all 0s.", 
    "Now you can see there are some numbers that", 
    "the system has automatically learned.", 
    "So, we are good.", 
    "We have trained the model and", 
    "it seems to have been producing pretty decent results.", 
    "And the corresponding weights and", 
    "parameter values you can print and visualize,", 
    "and convince yourself that the right things have been done.", 
    "As for the intent behind the model building and", 
    "training exercise that we are going through.", 
    "Now that we have tested a model,", 
    "let's try a query that we will put together.", 
    "Here, in this case, we illustrate how we can", 
    "interoperate the learned model with NumPy.", 
    "So in this case, I give a new sequence.", 
    "What I'm doing here is called beginning of sentence flight", 
    "from New York to Seattle, end of sentence.", 
    "And we look up the corresponding words in the dictionary,", 
    "and convert it to wording this says.", 
    "Then we create a one-hot representation using NumPy.", 
    "And that one-hot vector is then provided", 
    "as the input to the eval function here.", 
    "Where the z is the learnt model and", 
    "x is the input container, which has the one-hot representation", 
    "of the sentence that we want to learn, or classify, rather.", 
    "And here, if when I run it,", 
    "you can see that these are the indices that we", 
    "have corresponding to the words that are in the vocabulary.", 
    "There are 8 tokens, 1, 2, 3, 4, 5, 6, 7, 8.", 
    "So there are 8 tokens and", 
    "corresponding to those tokens there are 129 labels.", 
    "And you can see that 128 stands for other.", 
    "So that's the label for the first three tokens, and", 
    "then new is the beginning of the from location of the city_name.", 
    "I is the intermediate location of the city_name,", 
    "because New York, they are two separate words, but", 
    "they reflect the same city entity.", 
    "And then, to Seattle is the to location of the city_name.", 
    "So now we have trained, tested, and predicted on a sentence", 
    "that we'd never had seen before during our training process.", 
    "So we are pretty satisfied with that.", 
    "And this is your first classification of text data", 
    "using LSTM and recurrent models."
  ]
}