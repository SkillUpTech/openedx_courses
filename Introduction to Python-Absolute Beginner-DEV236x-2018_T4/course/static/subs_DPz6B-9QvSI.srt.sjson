{
  "start": [
    800, 
    4390, 
    8490, 
    12710, 
    17410, 
    23190, 
    25960, 
    30050, 
    33970, 
    36000, 
    40050, 
    43010, 
    47220, 
    51440, 
    55270, 
    61190, 
    65740, 
    70370, 
    73520, 
    75440, 
    78960, 
    84620, 
    90480, 
    97660, 
    99420, 
    101440, 
    106190, 
    110280, 
    112500, 
    115926, 
    118796, 
    125588, 
    131161, 
    135500, 
    143440, 
    148340, 
    149510, 
    153860, 
    157850, 
    160110, 
    166060, 
    171700, 
    176715, 
    181364, 
    186988, 
    190517, 
    195390, 
    200910, 
    206160, 
    212800, 
    218567, 
    225890, 
    232310, 
    236040, 
    241090, 
    245960, 
    251090, 
    256420, 
    259580, 
    264730, 
    266970, 
    272750, 
    278240, 
    281710, 
    283450, 
    287720, 
    290690, 
    293590
  ], 
  "end": [
    3130, 
    8490, 
    11680, 
    17410, 
    21740, 
    25960, 
    30050, 
    33970, 
    36000, 
    40050, 
    41340, 
    47220, 
    51440, 
    55270, 
    61190, 
    64420, 
    70370, 
    72230, 
    75440, 
    78960, 
    84620, 
    90480, 
    95670, 
    99420, 
    101440, 
    104720, 
    110280, 
    112500, 
    115926, 
    118796, 
    124268, 
    131161, 
    133560, 
    141710, 
    146590, 
    149510, 
    152330, 
    156610, 
    160110, 
    164530, 
    169370, 
    176715, 
    181364, 
    186988, 
    190517, 
    195390, 
    199880, 
    206160, 
    211660, 
    214622, 
    223090, 
    229980, 
    236040, 
    241090, 
    245960, 
    251090, 
    254160, 
    259580, 
    263240, 
    266970, 
    271210, 
    276140, 
    281710, 
    283450, 
    287720, 
    290690, 
    293590, 
    296280
  ], 
  "text": [
    "You are now quite familiar with the concept of recurrence.", 
    "How we can take multiple features as input and", 
    "emit the history that can be used in", 
    "future instances of the recurrent unit, which", 
    "help us make better predictions for the future time points.", 
    "Now before we get into this module,", 
    "which is gonna talk about a problem with long set of", 
    "recurrences, also known as the vanishing gradients.", 
    "Let me update you, and", 
    "bring to your focus one aspect, which will be very important in", 
    "understanding vanishing gradients.", 
    "So here in our previous module, we have seen that for", 
    "a given input and a history, we admit a new history.", 
    "And every time we do that we update the weight and", 
    "the bias parameter and the same parameters", 
    "are shared and updated across the time step.", 
    "So as we go from one time step to the other, this weight and", 
    "the bias parameters get updated.", 
    "What's the implication of that?", 
    "Let's take a step back from our time series example of the solar", 
    "panel output and for a moment take a pause and", 
    "switch ourselves into the world of text where each word", 
    "can be thought as a time point, as a sequence.", 
    "So here there's a long passage.", 
    "And I don't want you to read through it.", 
    "It's about a British Television program called Doctor Who.", 
    "And you can see that Doctor Who is a British science fiction", 
    "television program produced by the BBC.", 
    "And then it goes on to say, blah, blah, blah, blah, blah.", 
    "There is this sentence,", 
    "this television series produced by the blank.", 
    "Now, think of building a model where you take each word and", 
    "generate the next word.", 
    "So you start, say at time t with doctor with zero history", 
    "and output of the model is gonna be who, the next word.", 
    "You continue to do so.", 
    "Like you feed the next word as your input to the model.", 
    "And it emits the following word, is.", 
    "And you continue to do so", 
    "until you reach this particular dot, which is here.", 
    "And at this point, the right answer is BBC.", 
    "However, in order to remember that this answer is BBC.", 
    "You have to relate this state back to that", 
    "occurrence of BBC, which is 75 words behind.", 
    "So there are 75 recurrent blocks in between.", 
    "And remember every time you add a word,", 
    "the H vector gets updated", 
    "in the next iteration so if you have who it would have had", 
    "a role to play in the updated H vector now in the history, okay.", 
    "So what'd we do?", 
    "We have a history here and the input, the.", 
    "And we know this mechanism where we can update the history.", 
    "But increasingly, as you see, those many words that came in", 
    "between, each of them will be contributing to the history.", 
    "Thus the diluting the effect that this word BBC", 
    "would have had in this instance of the history", 
    "because it was so far back in time.", 
    "So the learning is that a single set of weights and", 
    "biases has a limited memory.", 
    "So when you have limited memory,", 
    "what are things you can do to overcome that situation?", 
    "Intuitively, you can do two things.", 
    "I'd like you to take a moment think about what those two", 
    "things might be.", 
    "And in the next video you'll see how we can incorporate", 
    "some of those intuitions you might already have.", 
    "And I'll go into some more of it and", 
    "introduce you to long short term memory cell."
  ]
}