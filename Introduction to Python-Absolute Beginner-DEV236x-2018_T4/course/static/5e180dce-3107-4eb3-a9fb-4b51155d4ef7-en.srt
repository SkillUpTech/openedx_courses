0
00:00:00,012 --> 00:00:05,930
One-hot representation can be considered as a kind of

1
00:00:05,930 --> 00:00:10,910
embedding which means you take an entity, in this case,

2
00:00:10,910 --> 00:00:14,250
it was the word token and you map it into a new micro vector.

3
00:00:16,800 --> 00:00:21,580
So, one-hot encoding is a Numerical representation of text

4
00:00:21,580 --> 00:00:23,150
in this case, as shown here.

5
00:00:24,350 --> 00:00:28,140
You have an array of 943 elements

6
00:00:28,140 --> 00:00:29,530
represented as the input.

7
00:00:32,168 --> 00:00:36,761
Word Embedding is another kind of encoding where specific

8
00:00:36,761 --> 00:00:39,948
techniques are developed to map words or

9
00:00:39,948 --> 00:00:42,778
phrases to vector of real numbers.

10
00:00:42,778 --> 00:00:46,970
Note in One-hot encoding, we had either zero or one.

11
00:00:46,970 --> 00:00:50,990
All the elements for zero except the one word that was present

12
00:00:50,990 --> 00:00:54,110
and it's corresponding index in the vocabulary.

13
00:00:54,110 --> 00:00:57,450
In the case of Word Embeddings, we will map words or

14
00:00:57,450 --> 00:00:59,620
phrases to vector of real numbers.

15
00:01:01,394 --> 00:01:05,414
This maps the one-hot encoded vector to a lower

16
00:01:05,414 --> 00:01:07,229
dimensional space.

17
00:01:09,803 --> 00:01:13,813
Instead of having a whole vector with a single number

18
00:01:13,813 --> 00:01:16,729
representing that particular word,

19
00:01:16,729 --> 00:01:19,290
it would compress the encoding.

20
00:01:19,290 --> 00:01:21,670
So instead of having 943 elements,

21
00:01:21,670 --> 00:01:25,610
it would have a much shorter representation and

22
00:01:25,610 --> 00:01:28,140
those representation would have real numbers.

23
00:01:31,260 --> 00:01:34,950
One example of word embedding is linear embedding.

24
00:01:37,280 --> 00:01:42,380
What we do here is, with some transformation that takes

25
00:01:42,380 --> 00:01:47,126
this one-hot encoding, we can compress it and represent it in

26
00:01:47,126 --> 00:01:52,450
let's say, 150 element vector instead of the 943 ones.

27
00:01:52,450 --> 00:01:55,150
And this could be a variable that you can tune for

28
00:01:55,150 --> 00:01:56,890
however long or short you may want.

29
00:01:58,010 --> 00:01:58,600
What does it do?

30
00:01:59,700 --> 00:02:05,080
We first take the vector and

31
00:02:05,080 --> 00:02:11,040
multiply a matrix with this one-hot encoded vector.

32
00:02:11,040 --> 00:02:15,550
The vector is of size 1 X 943 times 943 elements.

33
00:02:15,550 --> 00:02:20,961
And the matrix has a size of the number of elements that you want

34
00:02:20,961 --> 00:02:26,600
to compress it to, times the one-hot encoding vector length.

35
00:02:27,650 --> 00:02:33,800
And this matrix can be learned as a part of your network.

36
00:02:33,800 --> 00:02:37,270
You could initialize it with a random number and

37
00:02:37,270 --> 00:02:40,800
you can then learn this compact representation

38
00:02:41,890 --> 00:02:46,560
which would transfer your one-hot encoded input into

39
00:02:46,560 --> 00:02:49,180
a more dense embedding.

40
00:02:49,180 --> 00:02:52,620
Or, you can use some popular embedding techniques,

41
00:02:52,620 --> 00:02:56,790
such as Glove, or Word2Vec.

42
00:02:56,790 --> 00:03:00,670
These are more advanced techniques, that are widely

43
00:03:00,670 --> 00:03:05,710
available for use and people have code available of

44
00:03:05,710 --> 00:03:10,490
these locations where you can pre-generate these embeddings.

45
00:03:10,490 --> 00:03:14,690
Depending on you vocabulary and use them into your network

46
00:03:14,690 --> 00:03:20,420
either directly here, or you could initialize your We matrix,

47
00:03:21,690 --> 00:03:25,570
with the embedding from Glove or Word2vec, and

48
00:03:25,570 --> 00:03:28,580
then subsequently refine them as you train your network.

