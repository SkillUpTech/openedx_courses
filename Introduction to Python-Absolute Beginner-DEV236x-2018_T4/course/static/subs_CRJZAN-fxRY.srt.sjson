{
  "start": [
    140, 
    2276, 
    7880, 
    11300, 
    17807, 
    23940, 
    26300, 
    29490, 
    35351, 
    39605, 
    41861, 
    43880, 
    46166, 
    52160, 
    55213, 
    57690, 
    62430, 
    65040, 
    66070, 
    66676, 
    69899, 
    73570, 
    75110, 
    81030, 
    82710, 
    86150, 
    88290, 
    90430, 
    92270, 
    93433, 
    95660, 
    100415, 
    102536, 
    105680, 
    112530, 
    115936, 
    116779, 
    118553, 
    120582, 
    123986, 
    128390, 
    131120, 
    133940, 
    137024, 
    139910, 
    142236, 
    147323, 
    151471, 
    153910, 
    155220, 
    157785, 
    160443, 
    164390, 
    169123, 
    173689, 
    176313, 
    178840, 
    179970, 
    181918, 
    185270, 
    188980, 
    191835, 
    195462, 
    196620, 
    199400, 
    203450, 
    206410, 
    209430, 
    212920, 
    216145, 
    218580, 
    222751, 
    226080, 
    228657, 
    231586, 
    235817, 
    237440, 
    242090, 
    245224, 
    249462, 
    250380, 
    253001, 
    254990, 
    257210, 
    260400, 
    264670, 
    265263, 
    267260, 
    270727, 
    274693, 
    278750, 
    284117, 
    286464, 
    289241, 
    291230, 
    292600, 
    296161, 
    299576, 
    303760, 
    306295, 
    312225, 
    317911, 
    320390, 
    322750, 
    324799, 
    326930, 
    332865, 
    338280, 
    340902, 
    344945, 
    346602, 
    348626, 
    352188, 
    355389, 
    358100, 
    364291, 
    366290, 
    370905, 
    373904, 
    375238, 
    378108, 
    380440, 
    383760, 
    389040, 
    391656, 
    399400, 
    402228, 
    406470, 
    408510, 
    411600, 
    414845, 
    416630, 
    420907, 
    425248, 
    428710, 
    431694, 
    438480, 
    440543, 
    443386, 
    446838, 
    448989, 
    454320, 
    458986, 
    459860, 
    462400, 
    466620, 
    467785, 
    470340, 
    473380, 
    477510, 
    479920, 
    482080, 
    483870, 
    487064, 
    490180, 
    493028, 
    497510, 
    501150, 
    503900
  ], 
  "end": [
    2276, 
    6560, 
    11300, 
    12526, 
    22630, 
    26300, 
    29490, 
    33158, 
    39605, 
    41861, 
    43880, 
    46166, 
    48433, 
    55213, 
    56420, 
    62430, 
    65040, 
    66070, 
    66676, 
    69899, 
    73570, 
    75110, 
    81030, 
    82710, 
    86150, 
    88290, 
    90430, 
    92270, 
    93433, 
    95660, 
    100415, 
    102536, 
    104020, 
    112530, 
    115936, 
    116779, 
    118553, 
    120582, 
    123986, 
    127059, 
    131120, 
    133940, 
    137024, 
    139910, 
    142236, 
    147323, 
    149235, 
    153910, 
    155220, 
    157785, 
    160443, 
    162909, 
    169123, 
    173689, 
    176313, 
    177470, 
    179970, 
    181918, 
    185270, 
    187520, 
    191835, 
    195462, 
    196620, 
    199400, 
    201370, 
    206410, 
    209430, 
    212920, 
    216145, 
    218580, 
    222751, 
    226080, 
    228657, 
    231586, 
    235817, 
    237440, 
    242090, 
    245224, 
    249462, 
    250380, 
    253001, 
    254990, 
    257210, 
    260400, 
    264670, 
    265263, 
    267260, 
    268208, 
    274693, 
    278750, 
    284117, 
    286464, 
    289241, 
    291230, 
    292600, 
    296161, 
    299576, 
    303760, 
    306295, 
    312225, 
    317911, 
    320390, 
    321560, 
    324799, 
    325860, 
    332865, 
    336770, 
    340902, 
    344945, 
    346602, 
    348626, 
    350330, 
    355389, 
    358100, 
    364291, 
    366290, 
    370905, 
    373904, 
    375238, 
    378108, 
    380440, 
    383760, 
    388000, 
    391656, 
    396562, 
    402228, 
    405290, 
    408510, 
    410471, 
    414845, 
    415490, 
    420907, 
    425248, 
    428710, 
    431694, 
    437340, 
    440543, 
    443386, 
    446838, 
    448989, 
    454320, 
    458986, 
    459860, 
    462400, 
    466620, 
    467785, 
    470340, 
    473380, 
    477510, 
    479920, 
    482080, 
    483870, 
    487064, 
    489140, 
    493028, 
    497510, 
    501150, 
    502390, 
    505070
  ], 
  "text": [
    "Welcome to the lab for module 3,", 
    "Recognizing MNIST Data with the Multi-layer Perceptron Model.", 
    "This lab is stored as CNTK_103C.", 
    "And I'm gonna open that now.", 
    "And let's walk through the code here.", 
    "And just to remind you of what our digits look like.", 
    "These are 28x28 greyscale values.", 
    "You can see different examples of the digit 5 here.", 
    "All right, we're gonna start with importing our libraries,", 
    "matplotlib, numpy, sys, os.", 
    "And then here is cntk.", 
    "We're gonna refer to that with capital C.", 
    "And there's our matplotlib inline again.", 
    "Here's the test code for the notebook itself, and", 
    "we can ignore that.", 
    "And like before, I'm gonna start skipping over some of these code", 
    "sections that are the same as the previous labs.", 
    "So this is one of them.", 
    "But again,", 
    "it's good to know that our input dimensions are still 784.", 
    "And our number of output classes are 10.", 
    "The data reading is the same.", 
    "We create a reader that can read the CTF format for", 
    "our labels and features.", 
    "And then we find the directory that our test and", 
    "training files are in.", 
    "And we print that out here.", 
    "Then we create our model.", 
    "And this part is different.", 
    "So let's take a look at this.", 
    "We're still taking our 28x28 image data as a flat", 
    "array of pixel values.", 
    "And that's our input to our model.", 
    "We're feeding that into a dense layer that is 400 wide.", 
    "And the output of that dense layer goes into another hidden", 
    "dense layer.", 
    "So that's our second hidden layer.", 
    "It's also 400 wide.", 
    "And then our output layer is 10 nodes of a dense", 
    "layer to predict each of our 10 classes.", 
    "So this graph, it is slightly outdated.", 
    "It says the middle layer is 200, but", 
    "it's really gonna be 400 for our case.", 
    "And then we'll do the same as before.", 
    "We won't have a sigmoid or", 
    "any other activation function at the last output layer.", 
    "We'll take care of that with our loss function.", 
    "So let's go down to the code.", 
    "We're gonna set some variables.", 
    "Number of hidden layers is gonna be 2.", 
    "And the hidden dimension of each hidden layer,", 
    "the number of nodes in it is gonna be 400.", 
    "And again, we set up two input variables, one for our features", 
    "of size input_dim, which is 784, store that in input.", 
    "And num classes, which would be our labels, and", 
    "store that in label.", 
    "Here's where we build our model.", 
    "So this is actually the piece of code.", 
    "I think it may be the only piece of code in the notebook that's", 
    "different from the previous notebook.", 
    "So again, we use this default_options and", 
    "the glorot_uniform to initialize the random values of", 
    "the weights.", 
    "And in this case, we're also saying the activation for", 
    "each layer by default will be relu.", 
    "So we start with our features and we store that in h.", 
    "And then we go through this loop for", 
    "a number of times directed by num_hidden_layers.", 
    "And we create a dense layer whose dimension is", 
    "hidden_layers_dim, which would be 400.", 
    "And we pass h in as the input, and we get it as the output.", 
    "So with these, they're gonna change layers together.", 
    "And then finally after that, all the hidden layers,", 
    "we're gonna put one final dense layer for the output layer.", 
    "And on it we're gonna say, activation is none and", 
    "the width is 10.", 
    "And finally, we'll return the value of the model we created.", 
    "And then actually, this line is a typo.", 
    "I think I may have created that by mistake at one point and", 
    "left it in.", 
    "Here's the real line that happens.", 
    "So it overrides what was done here.", 
    "It creates the model passing input,", 
    "which is our binding input variable.", 
    "And we divide by 255 again to normalize the data between 0", 
    "and 1.", 
    "That gives us our model.", 
    "We store that in z.", 
    "And then again, we do same as before.", 
    "We'll do a cross_entropy_with_softmax loss.", 
    "We'll do a classification_error metric or label error.", 
    "And we're getting ready for our minibatch loop here.", 
    "So we set our parameters, same as before, 0.2 for", 
    "the learning rate.", 
    "And we create our trainer.", 
    "We have our helper function for the moving average and", 
    "our helper function for print training progress.", 
    "And then here's our minibatch parameters, 64 size,", 
    "60,000 training samples.", 
    "The number of epochs or time to sweep the whole data is 10.", 
    "So we'll do 600,000 samples in our training.", 
    "And as before, we create a reader.", 
    "We create an input map.", 
    "We set up these variables.", 
    "We go through the loop.", 
    "We grab our next set of data in the minibatch_size, which is 64.", 
    "So our next 64 samples of data, and we train with those.", 
    "And then we do our print progress.", 
    "And then we take the three variables, minibatch number,", 
    "the loss and the error.", 
    "And we store them in our plotdata variable so", 
    "we can have them for plotting later.", 
    "And here's the actual run of our print progress that shows", 
    "minibatches again going from 0 to 9,000.", 
    "And our loss, starting at 2.3 and going pretty steadily down.", 
    "It doesn't seem too much variation.", 
    "It seems more, Closing in than", 
    "previously on our logistic regression model.", 
    "And the same goes true for the error.", 
    "In fact, there's a bunch of zeros here for", 
    "our error after we get up to there.", 
    "So, again, we plot these using matplotlib.", 
    "We create two plots, one for the loss and one for the error.", 
    "And yeah, we can see that this goes pretty close to 0.", 
    "And then this looks like it goes even more at 0 also.", 
    "Let's evaluate it and see what our error is with", 
    "our holdout data, which in this case is our test data.", 
    "So to do that, again, we create a reader,", 
    "we create a map, seeing the pattern now.", 
    "We say, test_minibatch_size is gonna be 512 again because we're", 
    "evaluating.", 
    "And we go through the loop and trainer.test_minibatch.", 
    "And we get out average test error is 1.74.", 
    "So I think it was 7.8 in our logistic regression.", 
    "So we've done notably better,", 
    "dramatically better using this MLP with two hidden layers.", 
    "Then again, we're going to do the prediction.", 
    "So we extend it with the softmax.", 
    "And we come through here, the same code as before.", 
    "And now, if we look at our predicted label and", 
    "our actual labels, They look much better.", 
    "In fact, just eyeing these, I don't think I see a mismatch.", 
    "So that's pretty good.", 
    "Then we're gonna sample one of the labels again,", 
    "image number 5, and we'll plot it out.", 
    "And then we'll print the label.", 
    "And this time, it's the same 9.", 
    "And it looks like we got the label correct this time for 9.", 
    "So that's it for our tour of the notebook.", 
    "Now, it's time for you to go back and", 
    "open the notebook on your own machine.", 
    "Go through, run it once.", 
    "Try changing some values.", 
    "Try printing out intermediate values.", 
    "Really try and understand everything that happens.", 
    "And get a feel for the code.", 
    "And we also suggest you try the exploration", 
    "exercises at the bottom.", 
    "Okay, that's it for this lab."
  ]
}