{
  "start": [
    1000, 
    5150, 
    8890, 
    13570, 
    15920, 
    17420, 
    20752, 
    22720, 
    25654, 
    30200, 
    33720, 
    39390, 
    45010, 
    49950, 
    52390, 
    58380, 
    62960, 
    67650, 
    74090, 
    77970, 
    81165, 
    81920, 
    86770, 
    89270, 
    92220, 
    95490, 
    100930, 
    103820, 
    111060, 
    116126, 
    126315, 
    132250, 
    138500, 
    147027, 
    151695, 
    155030, 
    159699, 
    169521, 
    173737, 
    181507, 
    184268, 
    188410, 
    189480, 
    192520, 
    196248, 
    201966, 
    207435, 
    215830, 
    221022, 
    228329, 
    233080, 
    238196, 
    245864, 
    251826, 
    254004, 
    259812, 
    265257, 
    271791, 
    276640, 
    280040, 
    284680, 
    291780, 
    295330, 
    299140, 
    303740, 
    307451, 
    313956
  ], 
  "end": [
    5150, 
    8890, 
    13570, 
    15920, 
    17420, 
    20752, 
    22720, 
    25654, 
    30200, 
    33720, 
    36620, 
    45010, 
    48850, 
    52390, 
    57120, 
    62960, 
    67650, 
    74090, 
    77970, 
    81165, 
    81920, 
    86770, 
    87450, 
    92220, 
    92960, 
    100930, 
    102730, 
    108649, 
    116126, 
    121046, 
    132250, 
    138500, 
    143986, 
    151695, 
    155030, 
    159699, 
    165389, 
    173737, 
    178390, 
    184268, 
    188410, 
    189480, 
    192520, 
    196248, 
    201966, 
    207435, 
    213630, 
    221022, 
    228329, 
    233080, 
    238196, 
    242593, 
    251826, 
    254004, 
    259812, 
    265257, 
    271791, 
    276640, 
    280040, 
    284680, 
    288470, 
    295330, 
    299140, 
    303740, 
    307451, 
    313956, 
    318410
  ], 
  "text": [
    "You've heard me talk about activation functions many times,", 
    "and every time I bring up sigmoid, so you might be", 
    "wondering like beyond sigmoid is there something?", 
    "What are these activation functions?", 
    "When do I use it?", 
    "What are the traits, tradeoffs maybe associated with these", 
    "different activation functions?", 
    "So, let's take a moment and", 
    "learn the concept of activation function.", 
    "In the next several slides you will see", 
    "the different options you may have when building your models.", 
    "So, activation functions, also known as non-linearities,", 
    "take a single number and map it to a different value.", 
    "So for example, your friend and", 
    "much too familiar sigmoid activation function maps values,", 
    "any numerical value, to a 0,1 range.", 
    "Some of the popular activation functions yes, sigmoid again,", 
    "then we have hyperbolic tangent also known as tan.h value,", 
    "this is a very popular one it's called the rectified liner unit", 
    "and then there's an advancement over value leaky value and", 
    "parametric value.", 
    "There are few others that I'll allude to you as we go along", 
    "in the video.", 
    "Let's take a closer look at the different activation", 
    "functions you have.", 
    "Let's start with our friend sigmoid and", 
    "understand what its characteristics are.", 
    "So we have Our function is very simple", 
    "And the output of this function becomes", 
    "the input to my activation function.", 
    "So say we have a value of 0 here at the output,", 
    "which is the input to the activation function,", 
    "the value of the sigmoid would be 0.5.", 
    "Now if we have a different value,", 
    "say something like 3 or", 
    "something, so it'll go here and", 
    "it would give me a value of, say 0.9.", 
    "Historically, this activation function has been very,", 
    "very popular and because it mimics the brain-firing pattern.", 
    "Recently, it's popularity is dipping, and", 
    "largely due to saturation and vanishing gradients.", 
    "What does this mean?", 
    "Let's take a deeper look.", 
    "So the either end of the tails here,", 
    "like once your input crosses a certain value, say 4 or", 
    "5, the output of your activation function is gonna", 
    "be close to 1, As shown here.", 
    "And say you had initial model wx + b where for", 
    "certain choice of weights and biases, your value was say 8.", 
    "So the output would be, say very close to 1.", 
    "And now with the different set of weights which got updated", 
    "during training, you would get say a value of 5.", 
    "And the output of the activation unit would be close to 1.", 
    "So which means that,", 
    "if you were gonna take the difference between these two,", 
    "the change between the updates of w to w' between that", 
    "iterations of your training set, you would see that the slope or", 
    "the change is gonna be very, very negligible.", 
    "Which means that for that particular", 
    "update during the training step, the net new learning for", 
    "the network is very, very low if anything else.", 
    "Besides this phenomenon of vanishing gradient or", 
    "saturation, the output", 
    "of sigmoid function is non-zero centered.", 
    "What it means is that for", 
    "input value of 0, the output is 0.5.", 
    "It's more of an inconvenience and less severe than saturation."
  ]
}