{
  "start": [
    180, 
    5090, 
    8360, 
    12650, 
    15480, 
    18720, 
    22760, 
    24920, 
    28466, 
    34493, 
    36666, 
    40053, 
    42026, 
    45200, 
    48706, 
    52470, 
    56410, 
    62460, 
    64990, 
    70630, 
    74350, 
    77950, 
    82170, 
    86650, 
    88630, 
    91870, 
    94150, 
    98680, 
    101110, 
    104350, 
    107590, 
    113280, 
    115290, 
    119320, 
    123950, 
    129050, 
    133020, 
    136560, 
    139610, 
    143980, 
    145250, 
    148820, 
    152550, 
    162650, 
    167920, 
    170470, 
    175850, 
    178410, 
    180880, 
    183240, 
    186810, 
    191160, 
    193590, 
    199230, 
    203620, 
    208380, 
    215220, 
    218320, 
    221100, 
    224370, 
    227550, 
    230950, 
    235320, 
    239190, 
    241500, 
    245220, 
    246980, 
    250880, 
    253250, 
    256470, 
    259520, 
    263400, 
    265920, 
    269450, 
    272980, 
    277180
  ], 
  "end": [
    5040, 
    8013, 
    12586, 
    15160, 
    18666, 
    22706, 
    24813, 
    28160, 
    34040, 
    36600, 
    39986, 
    41906, 
    45120, 
    48620, 
    52440, 
    56390, 
    62449, 
    64979, 
    70619, 
    74200, 
    77930, 
    82150, 
    86630, 
    88610, 
    91850, 
    94130, 
    98660, 
    101093, 
    104330, 
    107570, 
    112200, 
    115270, 
    118546, 
    123930, 
    129039, 
    133000, 
    136540, 
    139590, 
    143960, 
    145239, 
    148800, 
    152530, 
    162630, 
    167900, 
    170459, 
    175830, 
    178390, 
    180860, 
    183220, 
    186790, 
    191140, 
    193570, 
    199210, 
    203600, 
    208360, 
    215200, 
    218300, 
    221080, 
    224350, 
    227530, 
    230666, 
    235300, 
    239170, 
    241480, 
    245200, 
    246960, 
    250860, 
    253230, 
    256450, 
    259500, 
    263380, 
    265900, 
    269430, 
    272960, 
    277160, 
    282160
  ], 
  "text": [
    "We have all the components to train", 
    "our Multi-layer Perceptron Network.", 
    "We are going to us the same train validate", 
    "test predict workflow that you have seen before.", 
    "The only difference here is the", 
    "model itself. In the previous module the", 
    "model was logistic regression.", 
    "Here it's a Multi-layer Perceptron model.", 
    "So let's see the Train workflow remains exactly the same", 
    "except we are going to replace", 
    "this part with the Multi-layer Perceptron model.", 
    "So I want to spend time", 
    "explaining the loss function how it", 
    "evolves over the iterations. The suffice to", 
    "say that during this stage we are", 
    "finding new sets of parameters which", 
    "yield lower and lower loss value.", 
    "Subsequently in the validation workflow", 
    "we select the final model based on a", 
    "different data set called the validation data set", 
    "such that the loss is the least", 
    "on the validation data set.", 
    "So let's start with the same database", 
    "the MNIST database. Here we are going to", 
    "use the training data set again. Pretty", 
    "much nothing changes here we sample a", 
    "mini batch of size 128 these are the", 
    "number of images that we are drawing from this", 
    "MNIST database and the corresponding", 
    "labels which are One-hot encoded. Now this", 
    "is where things are different right. The model here", 
    "is the Multi-layer Perceptron model that", 
    "we have seen during the lectures.", 
    "The first dense layer produces 400 outputs,", 
    "the second one has 200 outputs", 
    "and the final one emits an array of 10 numbers", 
    "representing the ten digits that", 
    "we are trying to classify. We are using", 
    "activation function relu here in the hidden layers.", 
    "Since we are going to use soft max", 
    "we do not use an activation function,", 
    "we just have it as a pass through in the final output layer.", 
    "So there are six sets of parameters, three weights and three biases", 
    "our loss function is cross_entropy_with_softmax", 
    "which takes the output of the model", 
    "and the labels from our training set.", 
    "Optionally we also use the", 
    "classification error() function to figure out", 
    "how well our classifier is doing", 
    "against the ground truth or the label", 
    "set and then using the trainer object", 
    "which takes the model, the loss, the error", 
    "and learner functions we train our model", 
    "like repeatedly calling Trainer.train_minibatch", 
    "with the data and the labels provided to the function as an input.", 
    "The learners can be any one of them such as sgd, adagrad et cetera that", 
    "we have introduced to you in the last lecture.", 
    "Following the selection of the", 
    "final model we want to test what its", 
    "performance is going to be on data set", 
    "that hasn't been used for training.", 
    "So using the final model, we sample data sets", 
    "from the test database and using", 
    "the final model we generate predictions", 
    "and calculate the error. Let's see how", 
    "the test workflow loads out here.", 
    "We do not want to sample from the training data set.", 
    "We sample from the test data set,", 
    "we sample the input data or the", 
    "features in this case what we call and the corresponding labels .", 
    "Note in this case the model is frozen", 
    "that means the weights and the bias", 
    "values are fixed, they are not changing.", 
    "We called Trainer.test_mini-batch", 
    "repeatedly and evaluate the average error that", 
    "this model would produce."
  ]
}