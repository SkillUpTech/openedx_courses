{
  "start": [
    80, 
    4070, 
    7640, 
    11920, 
    15040, 
    20190, 
    25570, 
    28690, 
    34440, 
    39170, 
    46070, 
    50890, 
    54990, 
    57320, 
    59320, 
    64290, 
    68170, 
    74990, 
    82390, 
    86270, 
    89590, 
    95770, 
    99940, 
    104190, 
    107090, 
    109090, 
    112570, 
    118040, 
    123990, 
    129670, 
    134540, 
    139090, 
    142520, 
    145520, 
    153470, 
    161690, 
    168470, 
    175340, 
    180070, 
    184150, 
    189380, 
    194330, 
    198400, 
    204000, 
    208370, 
    216200, 
    221330, 
    226100, 
    229280, 
    233000, 
    241360, 
    247050, 
    250140, 
    252660, 
    255680, 
    259600, 
    265390, 
    268060, 
    271310, 
    277620, 
    281280, 
    288560, 
    294020, 
    297830, 
    306330, 
    309960, 
    315560, 
    319610, 
    325870, 
    332210, 
    335060
  ], 
  "end": [
    4050, 
    7625, 
    11900, 
    15025, 
    20175, 
    25550, 
    28675, 
    34400, 
    39150, 
    46050, 
    50875, 
    54950, 
    57300, 
    59300, 
    64275, 
    68150, 
    74975, 
    82375, 
    86250, 
    89575, 
    95750, 
    99925, 
    104150, 
    107070, 
    109070, 
    112550, 
    118025, 
    123975, 
    129650, 
    134525, 
    139075, 
    142500, 
    144500, 
    153450, 
    161675, 
    168450, 
    175325, 
    180050, 
    183610, 
    188290, 
    193790, 
    197940, 
    203950, 
    207490, 
    214390, 
    220230, 
    225680, 
    229220, 
    232510, 
    240660, 
    246480, 
    250120, 
    252120, 
    254820, 
    259120, 
    264760, 
    268040, 
    270870, 
    277600, 
    280620, 
    287870, 
    293270, 
    297730, 
    305460, 
    309670, 
    314460, 
    318650, 
    325170, 
    331830, 
    334920, 
    338960
  ], 
  "text": [
    "So now we are at the last slide, we have all the pieces", 
    "that we need to put together our end to end", 
    "Training Validation Test Predict Workflow.", 
    "So let's see what we have here that is different,", 
    "the only difference that we have here is this model.", 
    "We are going to be using a convolution Network here", 
    "instead of using logistic regression", 
    "or a multi-layer perceptron model in the previous lectures.", 
    "So. again we start with the training database here", 
    "we sample a mini batch of handwritten digits.", 
    "So in this case you will note that we have 128 samples", 
    "for the mini batch, but then we have the handwritten digits,", 
    "which are now not flattened", 
    "out instead they are being red", 
    "as one channel for the color in this case it's gray scales,", 
    "so that's one if it was a natural scene this would be three", 
    "and the image pixel width and the image pixel height both being 28.", 
    "The labels are the same they don't change and each of the input image", 
    "has a corresponding label, our model is", 
    "convolution model here with max pooling.", 
    "This is same as the one that we described in the lectures before.", 
    "The parameters here are somewhat different", 
    "they are the parameters coming in from the convolution", 
    "and the dense outputs,", 
    "note max pooling does not", 
    "change the number of parameters in itself.", 
    "Then we have used the same cross entropy with softmax loss function,", 
    "the classification error function create the trainer object here", 
    "and using the trainer dot train underscore minibatch and providing", 
    "the trainer with different instances of randomly sampled", 
    "minibatches of handwritten digits and their corresponding labels.", 
    "The learners are pretty much the same", 
    "options that you have you can explore that", 
    "sgd, adagrad and many others that are available with most of the tool kits.", 
    "Having found the final model that we want to see how well it performs on", 
    "previously unforeseen data during training and use the test data.", 
    "We measure the performance of the model by calculating the error", 
    "that is how many images of the MS digits are misclassified.", 
    "and we repeat this a few times", 
    "to get the average performance of these trained model.", 
    "Now that we have the trained model we are going to test its performance,", 
    "on the test database we will start with the MNIST", 
    "test database here, we'll draw many batches of handwritten", 
    "images from this test database.", 
    "The image data and the corresponding one hot encoded labels,", 
    "this would be the input to the model.", 
    "We are illustrating the use of convolution with max pooling model", 
    "here, these are weights that have been", 
    "identified during the training process, we are not going to change that", 
    "in testing. We simply iterate through the samples in the test database", 
    "and each time we measure how well the classifier does", 
    "with this test samples", 
    "compared against the labels", 
    "that are provided in the test database as well", 
    "and then finally return the classification error as percent", 
    "incorrectly labeled and missed image. And you have seen that", 
    "as we went from logistic regression", 
    "to multi-layer perceptron to the convolutions", 
    "the error increasingly reduced that means the accuracy of our classifier", 
    "is becoming better and better.", 
    "Now with the tested model we can deploy this model,", 
    "in an application or web service where we take an arbitrary image", 
    "of handwritten digit and predict", 
    "it's corresponding optically recognized value in this case it's nine", 
    "which is the index with the highest value", 
    "in the predicted softmax probability array", 
    "can use numpy, argmax functions", 
    "to identify the index of the array where this has the highest value.", 
    "In general you can use numpy operations to craft different ways", 
    "to enhance your modeling capabilities", 
    "and either pre-process or post-process your data as well."
  ]
}