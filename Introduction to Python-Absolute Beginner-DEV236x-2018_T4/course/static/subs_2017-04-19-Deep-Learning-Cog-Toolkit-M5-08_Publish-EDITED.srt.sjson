{
  "start": [
    570, 
    5421, 
    9610, 
    13703, 
    16644, 
    20410, 
    25620, 
    33472, 
    38720, 
    42790, 
    46470, 
    50950, 
    56310, 
    62380, 
    68220, 
    72527, 
    78340, 
    82626, 
    88422, 
    94970, 
    99740, 
    101540, 
    104880, 
    109270, 
    116584, 
    118630, 
    122740, 
    125465, 
    133042, 
    141500, 
    146244, 
    153613, 
    156339, 
    161250, 
    164960, 
    170628, 
    175823, 
    184030, 
    188440, 
    195990, 
    199400, 
    203430, 
    211890, 
    215790, 
    222655, 
    227540, 
    235450, 
    238590, 
    243150, 
    245500, 
    251390, 
    254770, 
    259390, 
    262720, 
    267330, 
    271870, 
    277550, 
    282330, 
    288860, 
    295380, 
    300970, 
    305530, 
    310740, 
    314257, 
    318903, 
    324037, 
    329064, 
    335860, 
    340340, 
    347610, 
    354044, 
    358818, 
    363438, 
    367904, 
    374064, 
    378838, 
    386350, 
    388540, 
    394100, 
    399668, 
    404933, 
    410800, 
    414877, 
    420721, 
    427650, 
    432771, 
    437180, 
    442585, 
    448835, 
    451720, 
    457210, 
    459843, 
    462407, 
    466651, 
    468800, 
    470560, 
    473690, 
    477210, 
    478170, 
    484010, 
    488010, 
    489720, 
    493857, 
    496621, 
    501065, 
    505610, 
    509257, 
    513840, 
    519189, 
    524770, 
    528960, 
    533430, 
    540470, 
    544680, 
    547770, 
    551400
  ], 
  "end": [
    5421, 
    9610, 
    13703, 
    16644, 
    18319, 
    25620, 
    30611, 
    36380, 
    41460, 
    46470, 
    50950, 
    55130, 
    60850, 
    66790, 
    72527, 
    76460, 
    82626, 
    88422, 
    93330, 
    99740, 
    101540, 
    104880, 
    109270, 
    116584, 
    118630, 
    120220, 
    125465, 
    131280, 
    141500, 
    146244, 
    150362, 
    156339, 
    159460, 
    163850, 
    170628, 
    175823, 
    181040, 
    188440, 
    193490, 
    199400, 
    202080, 
    210540, 
    215790, 
    222655, 
    227540, 
    235450, 
    238590, 
    243150, 
    245500, 
    249130, 
    254770, 
    259390, 
    261230, 
    267330, 
    271870, 
    277550, 
    282330, 
    286690, 
    293490, 
    299940, 
    305530, 
    309730, 
    314257, 
    318903, 
    324037, 
    329064, 
    334000, 
    340340, 
    347610, 
    354044, 
    358818, 
    363438, 
    367904, 
    374064, 
    378838, 
    383780, 
    388540, 
    392560, 
    399668, 
    404933, 
    410800, 
    414877, 
    420721, 
    426327, 
    432771, 
    437180, 
    442585, 
    445165, 
    451720, 
    454890, 
    459843, 
    462407, 
    466651, 
    467430, 
    470560, 
    472390, 
    477210, 
    478170, 
    484010, 
    488010, 
    488670, 
    493857, 
    496621, 
    501065, 
    505610, 
    509257, 
    513840, 
    519189, 
    524770, 
    528960, 
    533430, 
    540470, 
    544680, 
    547770, 
    551400, 
    553410
  ], 
  "text": [
    "Hopefully, you have had a moment to think through,", 
    "from the previous video, how we can overcome", 
    "the limitation of having limited history.", 
    "So let's come back to that question.", 
    "What can we do about limited memory?", 
    "First thing, you can add memory, you can grow memory, right?", 
    "We have a computer system, we can add more memory, what else?", 
    "We can become more efficient in using the memory.", 
    "It would mean that you would maybe", 
    "try to forget some things that are not as relevant.", 
    "Maybe the words like is, the, and, some of", 
    "these repeated words that keep coming over and over, you can", 
    "not give as much importance to them or even forget it.", 
    "So we will see how we can add more memory to our system.", 
    "And also become more efficient in forgetting", 
    "those somewhat not so important things.", 
    "And that's what the LSTM cell will enable you to do.", 
    "Long-Short Term Memory cell is one of the possible solutions", 
    "that are very, very influential in deep networks.", 
    "Because it enables you to remember context for", 
    "a much longer period.", 
    "And is much more effective in", 
    "solving many practical real world problems that you'll see.", 
    "So let's start with our history at time t-1.", 
    "Remember the dimensions.", 
    "This is M dimensional vector.", 
    "And we have our current input,", 
    "x(t), which is N dimensional.", 
    "And the capital X bar is our new Input, which combines x and h.", 
    "You could also think of it as x star,", 
    "which we have used last video.", 
    "Based on this history, and the new input,", 
    "you want to generate the new, updated history.", 
    "And now we talked about adding new memory.", 
    "So let's say that c stands for our new memory concept that", 
    "has the ability to add memory to our existing set up,", 
    "which encapsulates history through this h vector.", 
    "And based on the new input past history and", 
    "the past memory, we want to update our memory.", 
    "First let's try to forget things", 
    "that are not gonna be of much use.", 
    "We project x through a sigmoid and a linear function here.", 
    "And the output of this one, a value between zero and one.", 
    "And we can multiply the output of this function and", 
    "where element wise multiplication with my memory,", 
    "which has the same dimensions as the output of this forget unit.", 
    "We would be able to wipe out things that we do not want to", 
    "remember or keep the things we want to remember and", 
    "all the intermediate stages.", 
    "Depending on the value of this sigmoid function.", 
    "Because we control the what to forget and", 
    "what not to forget, this concept is called a gate.", 
    "And this is called the forget gate.", 
    "Now that we have pruned our memory with things we want to", 
    "forget, let's figure out what we want to remember.", 
    "So we take the input X here, the capital X, and", 
    "we project it just like we were doing earlier into an M", 
    "dimensional output, this things is n plus m dimension, n plus m.", 
    "And the output of this unit is gonna be m dimensional.", 
    "And we want to keep the things that we want to update.", 
    "That means these are the things in the processed", 
    "input channel that we want to retain and update.", 
    "So this is called an update gate.", 
    "Again notice that this is a sigmoid", 
    "function with output being 0 and 1.", 
    "This is an essential function to be used for the gates,", 
    "because we want to have states only between 0 and 1.", 
    "Now that we know what to update our memory with,", 
    "let's go ahead and add it to our pruned memory here.", 
    "And we will update the next date of my memory which is here.", 
    "Which is c of t-1 times the output of", 
    "the forget date plus the input which", 
    "combines both x and h at t-1 xt and", 
    "multiplied with the output of the update gate so", 
    "it will be the thing we can remember,", 
    "and that becomes the new cell memory.", 
    "Now that we have the new cell memory updated,", 
    "let's figure out what we want to do with my new history.", 
    "So here, what we do is we create a result gate", 
    "which takes the input capital expector,", 
    "here, and we map parts of the updated memory,", 
    "blend it with the result gates.", 
    "So that we know how we want to take things from my updated", 
    "memory, and pass it as the new state of the history.", 
    "Now that we have updated the history here,", 
    "we can also pass it through a dense and", 
    "a soft max layer to emit the new output state,", 
    "should you need one.", 
    "So you can see that mathematically,", 
    "we are introducing a lot of parameters.", 
    "And whenever you do a lot of parameters,", 
    "you increase the model capacity here.", 
    "So you can see that we have added many more weights and", 
    "biases.", 
    "Whenever you add these parameters.", 
    "So in LSTM cells,", 
    "you can see that we have added a lot more parameters.", 
    "And by doing so,", 
    "we have increased the capacity of the LSTM unit.", 
    "Which is one of the popular building blocks for", 
    "recount units.", 
    "So instead of just having one set of weights and", 
    "bias, you now have many more.", 
    "This is a key concept, where you are able to increase", 
    "the memory as well as by including the forget gates and", 
    "updating the ones that you want to update.", 
    "You are becoming more and more efficient in", 
    "utilizing these existing parameters of the model", 
    "to better capture the different time steps.", 
    "And as you build longer recurrent units, as you saw in", 
    "that passage generation part where we had to refer to", 
    "the BBC, that 75 blocks away, it becomes increasingly feasible.", 
    "There are other recurrent units like GRU or", 
    "Gated Recurrent Units, we haven't covered that here.", 
    "For this course, we are gonna focus on LSTM", 
    "as our building block of recurrent units."
  ]
}