0
00:00:00,011 --> 00:00:07,264
All right, so we will start with dissecting into the data first.

1
00:00:07,264 --> 00:00:12,415
And formatting in a way that we can then build our first model

2
00:00:12,415 --> 00:00:17,480
of this course, which is the logistic regression model.

3
00:00:18,690 --> 00:00:23,780
What we do here is we take the image, as shown here,

4
00:00:23,780 --> 00:00:26,680
it's made out of integer values,

5
00:00:26,680 --> 00:00:32,320
28 of them in rows, and then there are 28 columns.

6
00:00:32,320 --> 00:00:36,650
We stretch it out and put it in the form of a vector, okay?

7
00:00:36,650 --> 00:00:40,190
And the first one becomes the first element of the vector,

8
00:00:40,190 --> 00:00:44,620
the second pixel becomes the second element of the vector,

9
00:00:44,620 --> 00:00:48,830
and so on and so forth, and the last one goes at the very end.

10
00:00:48,830 --> 00:00:53,600
Now, for a 28 by 28 image, when you stretch it out,

11
00:00:53,600 --> 00:00:59,650
if you do the arithmetic, you'll find that there are 784 pixels.

12
00:00:59,650 --> 00:01:06,190
We take this array of pixels and treat that as our input.

13
00:01:07,500 --> 00:01:10,354
And it's represented by this input variable,

14
00:01:10,354 --> 00:01:13,629
which we call it as x, and there is an arrow on top of it,

15
00:01:13,629 --> 00:01:15,700
which indicates it's a vector.

16
00:01:15,700 --> 00:01:20,100
Now, it's no longer a simple number like it was in

17
00:01:20,100 --> 00:01:22,990
representing the average day of the temperature.

18
00:01:22,990 --> 00:01:25,430
But instead, there's a set of values,

19
00:01:25,430 --> 00:01:30,280
784 of those, representing each pixel in this array of

20
00:01:30,280 --> 00:01:35,130
28 by 28 numbers representing the digit 3.

21
00:01:35,130 --> 00:01:37,220
The 28 by 28, by the way,

22
00:01:37,220 --> 00:01:41,410
just happens to be the size of the images from this data set.

23
00:01:41,410 --> 00:01:43,113
It doesn't have to be 28 by 28.

24
00:01:43,113 --> 00:01:44,193
It could be anything.

25
00:01:46,677 --> 00:01:51,733
And what we want to do is, instead of directly

26
00:01:51,733 --> 00:01:55,858
classifying this to be a value of 3,

27
00:01:55,858 --> 00:01:59,862
we would classify that as digit 3.

28
00:01:59,862 --> 00:02:04,902
What we do instead is predict what is

29
00:02:04,902 --> 00:02:11,626
the chance this image represents the digit 0.

30
00:02:13,461 --> 00:02:18,470
1, 2, 3, 4, to all the way till 9, so on, so forth.

31
00:02:18,470 --> 00:02:20,630
Now, you can see that, in this case,

32
00:02:20,630 --> 00:02:23,300
this digit is hand-written digit of 3.

33
00:02:23,300 --> 00:02:27,160
So we expect that the value corresponding to that digit

34
00:02:27,160 --> 00:02:28,310
should be relatively high.

35
00:02:29,870 --> 00:02:35,560
And that's what we get in logistic regression modeling.

36
00:02:35,560 --> 00:02:39,250
So the model is going to output ten different values,

37
00:02:39,250 --> 00:02:42,600
and the highest value should

38
00:02:42,600 --> 00:02:44,750
be the one that corresponds to that digit.

39
00:02:46,110 --> 00:02:48,200
So this is our model here.

40
00:02:48,200 --> 00:02:52,557
In this case, instead of m and b being the parameters,

41
00:02:52,557 --> 00:02:57,411
we have W and b, which you will learn shortly what they mean.

42
00:02:57,411 --> 00:03:02,931
But the thing to remember here is that this model

43
00:03:02,931 --> 00:03:09,560
maps input features to discrete output classes, okay?

44
00:03:10,700 --> 00:03:12,460
As opposed to linear regression,

45
00:03:12,460 --> 00:03:14,200
which maps it into a continuous value.

46
00:03:15,220 --> 00:03:18,380
So, in the solar panel example,

47
00:03:18,380 --> 00:03:21,820
the output of the panel could be any number.

48
00:03:21,820 --> 00:03:25,834
In this case, that is not gonna be the situation,

49
00:03:25,834 --> 00:03:30,912
because output of this model has to be a digit between 0 and 9.

50
00:03:30,912 --> 00:03:36,952
And our model is parameterized by a function z.

51
00:03:36,952 --> 00:03:40,385
And if you're wondering what these W and bs are,

52
00:03:40,385 --> 00:03:43,317
these are often referred to as weights and

53
00:03:43,317 --> 00:03:47,440
b is called bias, similar to the slope and the intercept.

54
00:03:47,440 --> 00:03:51,253
And you'll notice that there are gonna be weights corresponding

55
00:03:51,253 --> 00:03:54,103
to 1 for each digit, so there are 10 of these.

56
00:03:54,103 --> 00:03:58,767
And we'll learn what the anatomy of this weight matrix is

57
00:03:58,767 --> 00:04:00,230
shortly.

58
00:04:00,230 --> 00:04:03,620
And because there are ten digits and we're trying to predict

59
00:04:03,620 --> 00:04:08,018
the likelihood of each digit, we have ten such

60
00:04:08,018 --> 00:04:13,130
bias units, or values corresponding to one,

61
00:04:13,130 --> 00:04:15,320
corresponding to each digit, as shown here.

