{
  "start": [
    480, 
    7900, 
    12840, 
    15900, 
    23040, 
    28160, 
    31870, 
    34420, 
    37300, 
    42670, 
    49070, 
    52560, 
    57360, 
    60950, 
    63220, 
    66400, 
    71780, 
    75840, 
    79810, 
    81790, 
    86220, 
    87870, 
    94930, 
    99040, 
    107100, 
    111990, 
    115500, 
    120300, 
    123060, 
    126940, 
    130780, 
    133660, 
    138300, 
    139900, 
    144650, 
    148620, 
    153550, 
    156280, 
    158220, 
    162720, 
    164920, 
    168620, 
    170350, 
    173880, 
    179650, 
    184850, 
    187720, 
    190950, 
    197380, 
    201480, 
    205420, 
    207950, 
    211280, 
    217220, 
    221920, 
    226650, 
    230020, 
    233980, 
    240080, 
    247060, 
    251310, 
    255420, 
    258280, 
    262830, 
    267590, 
    275520, 
    280290, 
    284220, 
    288100, 
    293920, 
    297650, 
    301070, 
    305270, 
    307600, 
    315010, 
    320390, 
    327490, 
    333470, 
    336620, 
    344320, 
    350470, 
    353990, 
    357520, 
    365590, 
    371470, 
    375520, 
    381830, 
    384840, 
    390100, 
    395040, 
    403100, 
    405660, 
    410200, 
    411920, 
    414100, 
    417220, 
    422450, 
    424720, 
    428850, 
    433030, 
    436510, 
    439010, 
    441140, 
    443770, 
    445190, 
    446870, 
    452330, 
    456980, 
    462130, 
    465210, 
    470250, 
    475610, 
    479390, 
    484410, 
    487640, 
    490810, 
    494720, 
    502580, 
    505670, 
    510790, 
    517370, 
    521670, 
    526500, 
    529550, 
    535490, 
    539350, 
    543260, 
    549020, 
    552340, 
    555740, 
    558980, 
    566550, 
    569120, 
    572090, 
    575130, 
    581970, 
    588180, 
    595360, 
    601210, 
    608030, 
    612990, 
    616140, 
    617940, 
    623340, 
    628300, 
    635440, 
    640030, 
    645420, 
    649140, 
    654580, 
    658770, 
    662210, 
    666940, 
    671080, 
    676190, 
    681120, 
    686220, 
    688690, 
    692760, 
    695040, 
    697880, 
    700820, 
    705380, 
    708260, 
    715600, 
    721120, 
    730390, 
    736260, 
    740440, 
    746840, 
    750100, 
    752590, 
    756100, 
    760160, 
    765360, 
    770200, 
    774600, 
    778190, 
    781540, 
    785700, 
    789560, 
    793180, 
    795660, 
    797840, 
    801160, 
    803980, 
    806640, 
    811800, 
    815270, 
    818340, 
    823720, 
    832760, 
    838270, 
    841020, 
    848880, 
    853020, 
    858590, 
    863820, 
    867940, 
    872080, 
    876820, 
    888060, 
    890220, 
    892870, 
    896220, 
    900820, 
    903370, 
    906370, 
    909710, 
    913150, 
    917080, 
    921400, 
    923760, 
    927100, 
    929680, 
    934300, 
    942160, 
    949660, 
    954350, 
    957530, 
    959820, 
    964170, 
    966680, 
    969420, 
    973060, 
    976960, 
    980480, 
    987280, 
    992560, 
    996570, 
    1002760, 
    1007800, 
    1014000, 
    1016980, 
    1020700, 
    1025540, 
    1031100, 
    1034300, 
    1040350, 
    1043530, 
    1044730, 
    1047130, 
    1050900
  ], 
  "end": [
    7800, 
    12540, 
    15470, 
    22220, 
    28010, 
    31760, 
    34240, 
    37050, 
    42510, 
    48940, 
    52440, 
    57230, 
    60830, 
    63033, 
    66166, 
    71500, 
    75710, 
    79790, 
    81632, 
    86080, 
    87710, 
    94780, 
    98890, 
    106960, 
    111840, 
    115340, 
    120171, 
    122910, 
    126750, 
    130620, 
    133470, 
    138090, 
    139820, 
    144440, 
    148430, 
    153390, 
    156266, 
    158100, 
    162333, 
    164720, 
    168440, 
    170010, 
    173710, 
    179470, 
    184700, 
    187450, 
    190766, 
    197200, 
    201310, 
    205310, 
    207790, 
    210920, 
    217050, 
    221720, 
    226166, 
    229900, 
    233133, 
    239680, 
    247040, 
    251290, 
    255400, 
    258170, 
    262810, 
    267560, 
    275500, 
    280270, 
    284200, 
    288080, 
    293900, 
    297630, 
    301050, 
    305250, 
    307570, 
    314800, 
    319850, 
    327220, 
    333380, 
    336600, 
    344300, 
    349670, 
    353970, 
    356970, 
    365470, 
    370850, 
    375240, 
    381800, 
    384820, 
    389790, 
    394940, 
    401280, 
    405630, 
    410170, 
    411890, 
    413940, 
    417180, 
    422430, 
    424240, 
    428540, 
    432620, 
    436490, 
    438990, 
    441120, 
    443750, 
    445170, 
    446850, 
    449760, 
    456610, 
    462100, 
    464770, 
    470180, 
    475580, 
    479110, 
    484290, 
    487620, 
    490320, 
    494700, 
    501010, 
    505510, 
    509120, 
    517320, 
    521570, 
    526130, 
    529530, 
    535250, 
    538850, 
    543170, 
    548840, 
    552000, 
    555450, 
    558620, 
    561750, 
    569010, 
    571710, 
    574960, 
    579440, 
    587840, 
    595290, 
    601090, 
    607950, 
    612880, 
    616050, 
    617850, 
    622970, 
    628220, 
    635360, 
    639950, 
    645310, 
    649050, 
    654510, 
    658680, 
    662190, 
    666920, 
    671060, 
    676170, 
    681100, 
    686200, 
    688670, 
    692740, 
    695020, 
    697860, 
    700800, 
    705340, 
    708240, 
    715580, 
    721100, 
    726620, 
    736240, 
    740420, 
    746820, 
    750080, 
    752570, 
    756080, 
    760140, 
    765340, 
    770180, 
    774580, 
    778170, 
    781520, 
    785680, 
    789540, 
    793160, 
    795640, 
    797820, 
    801140, 
    803960, 
    806620, 
    811780, 
    815250, 
    818320, 
    823700, 
    832740, 
    838250, 
    841000, 
    848860, 
    853000, 
    858450, 
    863800, 
    867900, 
    872060, 
    876800, 
    888040, 
    890200, 
    892850, 
    896200, 
    900800, 
    903310, 
    906350, 
    909690, 
    913130, 
    917060, 
    921380, 
    923740, 
    927080, 
    929660, 
    934280, 
    942130, 
    949640, 
    954330, 
    957510, 
    959750, 
    964080, 
    966660, 
    969400, 
    973040, 
    976930, 
    980460, 
    987260, 
    992530, 
    996550, 
    1002730, 
    1007770, 
    1013970, 
    1016950, 
    1020680, 
    1025510, 
    1031080, 
    1034280, 
    1040330, 
    1043510, 
    1044710, 
    1047040, 
    1050730, 
    1055710
  ], 
  "text": [
    "Welcome to lab 5 which is corresponding to the module number 6", 
    "this is going to be the last lab for this entire curriculum.", 
    "And here in this lab we are going to", 
    "do a language understanding tasks. Which is classification of text.", 
    "The words in the text that come through a lot of applications,", 
    "that you might have, say want to have a set of queries", 
    "in which you want to recognize different entities", 
    "or you want to do part of speech tagging", 
    "or you may want to simply have some specific task in your hand.", 
    "In this case what we will use is a data set from Air Traffic Information Systems.", 
    "And in where we want to tag different entities such as", 
    "the location from where you are going to take a flight and the destination", 
    "the day at which you are going to take it and we want to be able", 
    "to automatically tag that information.", 
    "So that we can build interesting applications on top of it.", 
    "Now before I can go and walk you through this tutorial which I will do shortly.", 
    "What I would do also is to give you a brief recap", 
    "of the ATIS data set we are going to use it's going to be a recap of what you have", 
    "seen in the lecture set.", 
    "So that you become familiar in the context of the lab notebook", 
    "that we will be following.", 
    "So let's crunch forward. So just to recap from the last module where we", 
    "dealt with sequences. And in this case we are going to look into", 
    "tagging the different entities in Air Traffic Data here.", 
    "And we talked about in the lecture how we can", 
    "take a textual token convert it into a numerical embedding", 
    "pass it through a set of more modular components", 
    "which defines this recurrence block.", 
    "And then we will be able to get a classification out of it.", 
    "And the recurrence notion is captured here,", 
    "which you will see how it can be materialized in code.", 
    "And then we will once you unroll this recurrence", 
    "this is what it means.", 
    "So your input text would be say show Burbank to Seattle's flights tomorrow.", 
    "and the corresponding labels are going to be other from city,", 
    "other to city note that even though Burbank and Seattle are the city names", 
    "the fact that they are their position", 
    "in the sentence is somewhat different", 
    "defines the tags being From city and To city.", 
    "And then this being a date.", 
    "So keep this in mind that whenever we build this model which", 
    "you'll see in this shape.", 
    "Internally there is an unrolling of the loop that's going on here", 
    "which enables you to take positional information of these words", 
    "in the sentence and help us do the right classification", 
    "of these tags into their corresponding entities.", 
    "So the ATIS data again", 
    "has 943 unique words, we'll call this the vocabulary", 
    "corresponding to those words there are 129 unique tags,", 
    "these are called the labels. And there are in the data set if you look closely", 
    "there are also 26 intent tags,", 
    "which we are not using in this tutorial.", 
    "Let's take a quick look into the data example and in this data set", 
    "data snippet and related to components in the tutorial.", 
    "So this data snippet have a sentence", 
    "please give me the flight from Boston to Pittsburgh", 
    "on thursday of next week end of the sentence.", 
    "178 token of my vocabulary is beginning of sentence", 
    "and so and so forth in this whole segment here for instance the word from", 
    "is the 444 token in my vocabulary set", 
    "corresponding to each of these tokens there are world labels here", 
    "most of them are see here others but", 
    "some of them have entities associated with it and these are the", 
    "classification tags include that we this classifier is going to learn", 
    "and there are 129 of these tags, each of these tags as well as the words", 
    "are encoded in a one hot representation which you have already", 
    "familiar with if you have gone through the tutorial the", 
    "if you have gone through the video recordings.", 
    "So let's see what we want to do here we just like the MNIST data set", 
    "where the digit 3 was turned hot here", 
    "this is the fourth index so this is 0 1 2 3", 
    "in the case of words, what we do here", 
    "is we say the word is Boston and that's", 
    "the 266 element in my vocabulary I'll have a vector of 943 elements", 
    "and the 266th element here would be turned on.", 
    "And similarly for each label we'll have one-hot presentation of vector", 
    "with 129 elements in it. Just a quick recap of the Embedding", 
    "because it's a new concept that you will see appear", 
    "as a line of code in the CNT cape lectures in the lecture set", 
    "as well as in the tutorial here what we take is we take the token", 
    "and we get a numerical representation of", 
    "the text here which is the one-hot encoding which you are familiar with", 
    "and then we map these one-hot encoding into a more", 
    "compact lower dimensional space and this process is called word Embedding.", 
    "In this tutorial we are using a simple linear Embedding,", 
    "which projects this 943 dimensional vector into a", 
    "vector of length 150 this would be a parameter", 
    "that you will see appear in the tutorial and we will be using", 
    "this formulation for our embedding. In other words,", 
    "We will learn this W-e matrix in during the training process", 
    "Glove and word2Vec embedding are also very popular", 
    "we are not going to use it in this tutorial but you can surely play around", 
    "by substituting the embeddings", 
    "for the words in your vocabulary", 
    "with the Glove or word2Vec embeddings", 
    "and substitute it in the this W-e matrix", 
    "during the initialization period", 
    "or if you want to not learn this embedding,", 
    "you can directly take the Text token embed", 
    "get the embeddings from Glove and feed that directly", 
    "here into the Recurrent block. But we are not going to do that,", 
    "what we are going to do in this tutorial", 
    "to keep things simple is this whole process", 
    "where we take one-hot encoding will", 
    "learn the embedding's using a simple", 
    "linear embedding and then feed it into current block.", 
    "The model that we will have is very familiar to you", 
    "and it is taking the Text token converting into a compact", 
    "representation then we will pass it into an LSTM block", 
    "and output of the LSTM block will be processed project", 
    "using a dense layer and we will project it into the class labels.", 
    "So this would be a 129 size vector", 
    "and our model is going to be looking like this", 
    "where you will have the input being 943", 
    "dimensional one-hot encoding, output is 150.", 
    "The LSTM will take in this 150", 
    "encodings and the hidden state is a 300 dimensional vector", 
    "that will be processed in the dense layer", 
    "and we will emit the 129 classes that belongs to it.", 
    "For the 129 vectors corresponding to the what entities", 
    "in our label set that we want to classify.", 
    "Here is a brief summary of what the model is going to do, you can see that", 
    "we'll take the sentence and we will", 
    "unroll it internally this you will not see in the code,", 
    "because this unrolling is done internally", 
    "within the CNTk engine making this really really convenient, because", 
    "as you can see that if the length of that sentence changes", 
    "the amount of unrolling that you need to do will be also", 
    "changing. And you don't have to worry about it", 
    "when using this tool kit to be able to tag", 
    "the words to their individual Class Labels.", 
    "Now that you are quite familiar with the dataset,", 
    "how it's going to look like the model structure", 
    "it will be a piece of cake for us to walk through the tutorial", 
    "and get familiar with how to translate those pictorial views into code.", 
    "So let's start with first thing first we download the data here", 
    "and you can see that, we look for the train test", 
    "the vocabulary and the slots, these are the vocabulary words", 
    "and these are the label words. We will import", 
    "a few things NumPy, math and the CNTK library", 
    "this block is corresponding to some", 
    "internal testing that we do", 
    "and should not-- is not you don't need to fiddle around with it.", 
    "This is the structure that we have alluded to in the", 
    "slides before here you can see that S1 is the intent part", 
    "above which had 26 different intents and we are not using it we're just", 
    "going to use the S0 labels and the S2 labels.", 
    "So there the tokens, this one shows show flights from", 
    "burbank to st.Louis on Monday end of sentence and corresponding to", 
    "that there are labels, if you see st.Louis is one place", 
    "but there are split into two tokens. So you have", 
    "a label that signifies that when this beginning of that", 
    "to location city name and there is intermediate to location", 
    "city name. So this is  an additional tag which means", 
    "that with this engine that you are going to learn you can put any sentence", 
    "and as long as you have these tokens that are labeled with", 
    "whatever maybe the label you want you", 
    "would be able to classify them. I Don't believe that", 
    "this tutorial by lead reading all this", 
    "material, because I've already walked you through the", 
    "corresponding components in the slides.", 
    "But please take a look at it closely and get familiar with the", 
    "input data sequence syntax", 
    "and again here is the unrolled loop, that I was talking to you during the slides", 
    "preceding this video okay.", 
    "This is what our code is going to look like, it looks off fully simple", 
    "makes it really simple, we make it really simple how to interpret and create models.", 
    "So here the vocabulary size is 943 number of labels", 
    "was 129 this is unused again and a model dimensions here,", 
    "the input dimension going to the vocabulary size,the label dimensions", 
    "number of labels the embedding dim", 
    "dimension is 150 you can change it, You can play around with it", 
    "and see what impact it has in your classification results,", 
    "from the hidden_dim that,we use for the LSTM block is set to 300.", 
    "We create two containers, x standing for the input,Y stands for the labels", 
    "and create the model, this one we have", 
    "initialized with a default initial state", 
    "and then we create our model.", 
    "This is a sequential construct that you might be familiar", 
    "if those of you have used libraries like Kara's.", 
    "What it doing is taking the input", 
    "whatever you pass it through would you", 
    "at some point you would pass it to this model,", 
    "it will process it to the embedding", 
    "the output of the embedding would be concatenate would be", 
    "passed into the recurrence model.", 
    "And here you can see that we have recurrent blocks", 
    "with the each of the blocks being an LSTM", 
    "of the hidden dim and then you have", 
    "the final output layer,which is the dense layer, which projects the", 
    "internal states of the LSTM and into  number of labels", 
    "that you want to classify. So let's go through this and see", 
    "how we can take a peak into the model.", 
    "Here if you can see here, you create the  model z and in case note", 
    "that we haven't yet passed any input parameters", 
    "yet input shape or data it has no notion of this x ok!.", 
    "So we just call create model as shown here, and it creates that", 
    "function at this point it's a simple function and when we try to", 
    "look at the shape of the embedding.", 
    "It gives me -1 to 150  with this** all of these being initialized to 0", 
    "right, so sorry. Let's print the value of z component", 
    "the embedding component of z.", 
    "Let's print the embedding component of the z,", 
    "and here we print the shape of the embedding matrix.", 
    "You can see it's -1 and 150, -1 indicates that", 
    "this model doesn't know", 
    "what kind of input is going to get.", 
    "Because there is no notion of x here", 
    "in that and then we can also print", 
    "the classify layer which is the Dense layer here,", 
    "and print the bias values.", 
    "So in this case you can see that", 
    "there are going to be 129 of these", 
    "and you would be able to see that", 
    "all the bias cells have been initialized to 0.", 
    "Now, if I pass the input parameter x", 
    "which is defined here which is a container of length", 
    "or 943 defined by the vocabulary size", 
    "and then print the", 
    "shape of the embedding matrix", 
    "you can see that it becomes 943,150.", 
    "So what I'm showing here is there are different ways you can", 
    "inspect the model at different layers", 
    "and different stages so feel free to explore that.", 
    "Let's take a brief look at the data", 
    "and the data reading component of it so here", 
    "you will provide the data that the CNTK toolkit", 
    "accepts and you can read more about the CNTKTextFormatReader", 
    "there's a link here and it converts", 
    "a sentence like this into a format", 
    "that I talked about  here yep.", 
    "And to facilitate some of these processing you there's some built-in", 
    "convenience function that you can use", 
    "to convert a text to a CTF format.", 
    "And then once you have that let's define the reader", 
    "which will read the different streams in this case query", 
    "the unused intent and the labels", 
    "and with that we are ready to trainer model.", 
    "Without being equipped with the data", 
    "in the right format", 
    "and the model structure that you have,", 
    "learned in the video and seen it in the code", 
    "let's do the Train test and the predict workflow."
  ]
}