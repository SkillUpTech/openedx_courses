0
00:00:00,520 --> 00:00:03,660
Now that we have put together a pretty decent

1
00:00:04,660 --> 00:00:08,960
set of layered components together.

2
00:00:09,900 --> 00:00:14,130
We have a Convolution based deep network.

3
00:00:14,360 --> 00:00:16,660
Let's figure out how we can

4
00:00:16,760 --> 00:00:19,330
put these together into a training workflow.

5
00:00:19,440 --> 00:00:22,000
Much of these going to be very familiar to you.

6
00:00:22,160 --> 00:00:25,640
But it will be in the context of the convolution operations

7
00:00:25,680 --> 00:00:28,120
and the layered elements that

8
00:00:28,520 --> 00:00:30,400
you just learnt in this module.

9
00:00:30,480 --> 00:00:32,980
We will be incorporating those into the workflow.

10
00:00:34,100 --> 00:00:37,780
So we start with the digit 3

11
00:00:38,120 --> 00:00:43,660
from MNIST. And we will give that as an input to the

12
00:00:44,460 --> 00:00:48,200
convolution model without flattening that.

13
00:00:48,500 --> 00:00:50,200
So here we are going to learn

14
00:00:50,600 --> 00:00:56,040
8(5x5) weight matrix + 8 biases.

15
00:00:56,780 --> 00:00:59,160
Then we have a pooling layer

16
00:00:59,880 --> 00:01:04,000
then we have a another convolution operation here.

17
00:01:04,260 --> 00:01:05,400
And here we are learning

18
00:01:05,460 --> 00:01:09,620
16(5x5)weight matrices and the corresponding 16 biases.

19
00:01:10,300 --> 00:01:12,560
And then you have the another pooling layer

20
00:01:12,780 --> 00:01:14,260
followed by a dense layer.

21
00:01:14,320 --> 00:01:18,060
So the parameters we are going to learn here

22
00:01:18,140 --> 00:01:23,920
are this layer, this layer and the dense layer. Ok!

23
00:01:24,520 --> 00:01:27,520
And here is the output of the dense layer which

24
00:01:27,930 --> 00:01:30,440
is same as before in the

25
00:01:30,520 --> 00:01:32,320
output numbers are going to be different.

26
00:01:32,400 --> 00:01:37,560
But the shape of the output is going to be vector of 10 numbers.

27
00:01:39,466 --> 00:01:43,066
It will compute the error of the loss function

28
00:01:43,260 --> 00:01:46,060
where we will pass the digit 3

29
00:01:46,180 --> 00:01:49,120
through the model generated, it's predicted probabilities

30
00:01:49,340 --> 00:01:53,200
and then compare it with the one-hot encoded label

31
00:01:53,380 --> 00:01:55,700
that comes along with the training set

32
00:01:55,900 --> 00:01:58,000
and we will use our friend

33
00:01:58,140 --> 00:02:01,980
cross entropy to compute the loss function.

