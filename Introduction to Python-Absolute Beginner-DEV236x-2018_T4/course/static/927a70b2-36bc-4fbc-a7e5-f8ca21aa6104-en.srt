0
00:00:00,480 --> 00:00:07,800
Welcome to lab 5 which is corresponding to the module number 6

1
00:00:07,900 --> 00:00:12,540
this is going to be the last lab for this entire curriculum.

2
00:00:12,840 --> 00:00:15,470
And here in this lab we are going to

3
00:00:15,900 --> 00:00:22,220
do a language understanding tasks. Which is classification of text.

4
00:00:23,040 --> 00:00:28,010
The words in the text that come through a lot of applications,

5
00:00:28,160 --> 00:00:31,760
that you might have, say want to have a set of queries

6
00:00:31,870 --> 00:00:34,240
in which you want to recognize different entities

7
00:00:34,420 --> 00:00:37,050
or you want to do part of speech tagging

8
00:00:37,300 --> 00:00:42,510
or you may want to simply have some specific task in your hand.

9
00:00:42,670 --> 00:00:48,940
In this case what we will use is a data set from Air Traffic Information Systems.

10
00:00:49,070 --> 00:00:52,440
And in where we want to tag different entities such as

11
00:00:52,560 --> 00:00:57,230
the location from where you are going to take a flight and the destination

12
00:00:57,360 --> 00:01:00,830
the day at which you are going to take it and we want to be able

13
00:01:00,950 --> 00:01:03,033
to automatically tag that information.

14
00:01:03,220 --> 00:01:06,166
So that we can build interesting applications on top of it.

15
00:01:06,400 --> 00:01:11,500
Now before I can go and walk you through this tutorial which I will do shortly.

16
00:01:11,780 --> 00:01:15,710
What I would do also is to give you a brief recap

17
00:01:15,840 --> 00:01:19,790
of the ATIS data set we are going to use it's going to be a recap of what you have

18
00:01:19,810 --> 00:01:21,632
seen in the lecture set.

19
00:01:21,790 --> 00:01:26,080
So that you become familiar in the context of the lab notebook

20
00:01:26,220 --> 00:01:27,710
that we will be following.

21
00:01:27,870 --> 00:01:34,780
So let's crunch forward. So just to recap from the last module where we

22
00:01:34,930 --> 00:01:38,890
dealt with sequences. And in this case we are going to look into

23
00:01:39,040 --> 00:01:46,960
tagging the different entities in Air Traffic Data here.

24
00:01:47,100 --> 00:01:51,840
And we talked about in the lecture how we can

25
00:01:51,990 --> 00:01:55,340
take a textual token convert it into a numerical embedding

26
00:01:55,500 --> 00:02:00,171
pass it through a set of more modular components

27
00:02:00,300 --> 00:02:02,910
which defines this recurrence block.

28
00:02:03,060 --> 00:02:06,750
And then we will be able to get a classification out of it.

29
00:02:06,940 --> 00:02:10,620
And the recurrence notion is captured here,

30
00:02:10,780 --> 00:02:13,470
which you will see how it can be materialized in code.

31
00:02:13,660 --> 00:02:18,090
And then we will once you unroll this recurrence

32
00:02:18,300 --> 00:02:19,820
this is what it means.

33
00:02:19,900 --> 00:02:24,440
So your input text would be say show Burbank to Seattle's flights tomorrow.

34
00:02:24,650 --> 00:02:28,430
and the corresponding labels are going to be other from city,

35
00:02:28,620 --> 00:02:33,390
other to city note that even though Burbank and Seattle are the city names

36
00:02:33,550 --> 00:02:36,266
the fact that they are their position

37
00:02:36,280 --> 00:02:38,100
in the sentence is somewhat different

38
00:02:38,220 --> 00:02:42,333
defines the tags being From city and To city.

39
00:02:42,720 --> 00:02:44,720
And then this being a date.

40
00:02:44,920 --> 00:02:48,440
So keep this in mind that whenever we build this model which

41
00:02:48,620 --> 00:02:50,010
you'll see in this shape.

42
00:02:50,350 --> 00:02:53,710
Internally there is an unrolling of the loop that's going on here

43
00:02:53,880 --> 00:02:59,470
which enables you to take positional information of these words

44
00:02:59,650 --> 00:03:04,700
in the sentence and help us do the right classification

45
00:03:04,850 --> 00:03:07,450
of these tags into their corresponding entities.

46
00:03:07,720 --> 00:03:10,766
So the ATIS data again

47
00:03:10,950 --> 00:03:17,200
has 943 unique words, we'll call this the vocabulary

48
00:03:17,380 --> 00:03:21,310
corresponding to those words there are 129 unique tags,

49
00:03:21,480 --> 00:03:25,310
these are called the labels. And there are in the data set if you look closely

50
00:03:25,420 --> 00:03:27,790
there are also 26 intent tags,

51
00:03:27,950 --> 00:03:30,920
which we are not using in this tutorial.

52
00:03:31,280 --> 00:03:37,050
Let's take a quick look into the data example and in this data set

53
00:03:37,220 --> 00:03:41,720
data snippet and related to components in the tutorial.

54
00:03:41,920 --> 00:03:46,166
So this data snippet have a sentence

55
00:03:46,650 --> 00:03:49,900
please give me the flight from Boston to Pittsburgh

56
00:03:50,020 --> 00:03:53,133
on thursday of next week end of the sentence.

57
00:03:53,980 --> 00:03:59,680
178 token of my vocabulary is beginning of sentence

58
00:04:00,080 --> 00:04:07,040
and so and so forth in this whole segment here for instance the word from

59
00:04:07,060 --> 00:04:11,290
is the 444 token in my vocabulary set

60
00:04:11,310 --> 00:04:15,400
corresponding to each of these tokens there are world labels here

61
00:04:15,420 --> 00:04:18,170
most of them are see here others but

62
00:04:18,280 --> 00:04:22,810
some of them have entities associated with it and these are the

63
00:04:22,830 --> 00:04:27,560
classification tags include that we this classifier is going to learn

64
00:04:27,590 --> 00:04:35,500
and there are 129 of these tags, each of these tags as well as the words

65
00:04:35,520 --> 00:04:40,270
are encoded in a one hot representation which you have already

66
00:04:40,290 --> 00:04:44,200
familiar with if you have gone through the tutorial the

67
00:04:44,220 --> 00:04:48,080
if you have gone through the video recordings.

68
00:04:48,100 --> 00:04:53,900
So let's see what we want to do here we just like the MNIST data set

69
00:04:53,920 --> 00:04:57,630
where the digit 3 was turned hot here

70
00:04:57,650 --> 00:05:01,050
this is the fourth index so this is 0 1 2 3

71
00:05:01,070 --> 00:05:05,250
in the case of words, what we do here

72
00:05:05,270 --> 00:05:07,570
is we say the word is Boston and that's

73
00:05:07,600 --> 00:05:14,800
the 266 element in my vocabulary I'll have a vector of 943 elements

74
00:05:15,010 --> 00:05:19,850
and the 266th element here would be turned on.

75
00:05:20,390 --> 00:05:27,220
And similarly for each label we'll have one-hot presentation of vector

76
00:05:27,490 --> 00:05:33,380
with 129 elements in it. Just a quick recap of the Embedding

77
00:05:33,470 --> 00:05:36,600
because it's a new concept that you will see appear

78
00:05:36,620 --> 00:05:44,300
as a line of code in the CNT cape lectures in the lecture set

79
00:05:44,320 --> 00:05:49,670
as well as in the tutorial here what we take is we take the token

80
00:05:50,470 --> 00:05:53,970
and we get a numerical representation of

81
00:05:53,990 --> 00:05:56,970
the text here which is the one-hot encoding which you are familiar with

82
00:05:57,520 --> 00:06:05,470
and then we map these one-hot encoding into a more

83
00:06:05,590 --> 00:06:10,850
compact lower dimensional space and this process is called word Embedding.

84
00:06:11,470 --> 00:06:15,240
In this tutorial we are using a simple linear Embedding,

85
00:06:15,520 --> 00:06:21,800
which projects this 943 dimensional vector into a

86
00:06:21,830 --> 00:06:24,820
vector of length 150 this would be a parameter

87
00:06:24,840 --> 00:06:29,790
that you will see appear in the tutorial and we will be using

88
00:06:30,100 --> 00:06:34,940
this formulation for our embedding. In other words,

89
00:06:35,040 --> 00:06:41,280
We will learn this W-e matrix in during the training process

90
00:06:43,100 --> 00:06:45,630
Glove and word2Vec embedding are also very popular

91
00:06:45,660 --> 00:06:50,170
we are not going to use it in this tutorial but you can surely play around

92
00:06:50,200 --> 00:06:51,890
by substituting the embeddings

93
00:06:51,920 --> 00:06:53,940
for the words in your vocabulary

94
00:06:54,100 --> 00:06:57,180
with the Glove or word2Vec embeddings

95
00:06:57,220 --> 00:07:02,430
and substitute it in the this W-e matrix

96
00:07:02,450 --> 00:07:04,240
during the initialization period

97
00:07:04,720 --> 00:07:08,540
or if you want to not learn this embedding,

98
00:07:08,850 --> 00:07:12,620
you can directly take the Text token embed

99
00:07:13,030 --> 00:07:16,490
get the embeddings from Glove and feed that directly

100
00:07:16,510 --> 00:07:18,990
here into the Recurrent block. But we are not going to do that,

101
00:07:19,010 --> 00:07:21,120
what we are going to do in this tutorial

102
00:07:21,140 --> 00:07:23,750
to keep things simple is this whole process

103
00:07:23,770 --> 00:07:25,170
where we take one-hot encoding will

104
00:07:25,190 --> 00:07:26,850
learn the embedding's using a simple

105
00:07:26,870 --> 00:07:29,760
linear embedding and then feed it into current block.

106
00:07:32,330 --> 00:07:36,610
The model that we will have is very familiar to you

107
00:07:36,980 --> 00:07:42,100
and it is taking the Text token converting into a compact

108
00:07:42,130 --> 00:07:44,770
representation then we will pass it into an LSTM block

109
00:07:45,210 --> 00:07:50,180
and output of the LSTM block will be processed project

110
00:07:50,250 --> 00:07:55,580
using a dense layer and we will project it into the class labels.

111
00:07:55,610 --> 00:07:59,110
So this would be a 129 size vector

112
00:07:59,390 --> 00:08:04,290
and our model is going to be looking like this

113
00:08:04,410 --> 00:08:07,620
where you will have the input being 943

114
00:08:07,640 --> 00:08:10,320
dimensional one-hot encoding, output is 150.

115
00:08:10,810 --> 00:08:14,700
The LSTM will take in this 150

116
00:08:14,720 --> 00:08:21,010
encodings and the hidden state is a 300 dimensional vector

117
00:08:22,580 --> 00:08:25,510
that will be processed in the dense layer

118
00:08:25,670 --> 00:08:29,120
and we will emit the 129 classes that belongs to it.

119
00:08:30,790 --> 00:08:37,320
For the 129 vectors corresponding to the what entities

120
00:08:37,370 --> 00:08:41,570
in our label set that we want to classify.

121
00:08:41,670 --> 00:08:46,130
Here is a brief summary of what the model is going to do, you can see that

122
00:08:46,500 --> 00:08:49,530
we'll take the sentence and we will

123
00:08:49,550 --> 00:08:55,250
unroll it internally this you will not see in the code,

124
00:08:55,490 --> 00:08:58,850
because this unrolling is done internally

125
00:08:59,350 --> 00:09:03,170
within the CNTk engine making this really really convenient, because

126
00:09:03,260 --> 00:09:08,840
as you can see that if the length of that sentence changes

127
00:09:09,020 --> 00:09:12,000
the amount of unrolling that you need to do will be also

128
00:09:12,340 --> 00:09:15,450
changing. And you don't have to worry about it

129
00:09:15,740 --> 00:09:18,620
when using this tool kit to be able to tag

130
00:09:18,980 --> 00:09:21,750
the words to their individual Class Labels.

131
00:09:26,550 --> 00:09:29,010
Now that you are quite familiar with the dataset,

132
00:09:29,120 --> 00:09:31,710
how it's going to look like the model structure

133
00:09:32,090 --> 00:09:34,960
it will be a piece of cake for us to walk through the tutorial

134
00:09:35,130 --> 00:09:39,440
and get familiar with how to translate those pictorial views into code.

135
00:09:41,970 --> 00:09:47,840
So let's start with first thing first we download the data here

136
00:09:48,180 --> 00:09:55,290
and you can see that, we look for the train test

137
00:09:55,360 --> 00:10:01,090
the vocabulary and the slots, these are the vocabulary words

138
00:10:01,210 --> 00:10:07,950
and these are the label words. We will import

139
00:10:08,030 --> 00:10:12,880
a few things NumPy, math and the CNTK library

140
00:10:12,990 --> 00:10:16,050
this block is corresponding to some

141
00:10:16,140 --> 00:10:17,850
internal testing that we do

142
00:10:17,940 --> 00:10:22,970
and should not-- is not you don't need to fiddle around with it.

143
00:10:23,340 --> 00:10:28,220
This is the structure that we have alluded to in the

144
00:10:28,300 --> 00:10:35,360
slides before here you can see that S1 is the intent part

145
00:10:35,440 --> 00:10:39,950
above which had 26 different intents and we are not using it we're just

146
00:10:40,030 --> 00:10:45,310
going to use the S0 labels and the S2 labels.

147
00:10:45,420 --> 00:10:49,050
So there the tokens, this one shows show flights from

148
00:10:49,140 --> 00:10:54,510
burbank to st.Louis on Monday end of sentence and corresponding to

149
00:10:54,580 --> 00:10:58,680
that there are labels, if you see st.Louis is one place

150
00:10:58,770 --> 00:11:02,190
but there are split into two tokens. So you have

151
00:11:02,210 --> 00:11:06,920
a label that signifies that when this beginning of that

152
00:11:06,940 --> 00:11:11,060
to location city name and there is intermediate to location

153
00:11:11,080 --> 00:11:16,170
city name. So this is  an additional tag which means

154
00:11:16,190 --> 00:11:21,100
that with this engine that you are going to learn you can put any sentence

155
00:11:21,120 --> 00:11:26,200
and as long as you have these tokens that are labeled with

156
00:11:26,220 --> 00:11:28,670
whatever maybe the label you want you

157
00:11:28,690 --> 00:11:32,740
would be able to classify them. I Don't believe that

158
00:11:32,760 --> 00:11:35,020
this tutorial by lead reading all this

159
00:11:35,040 --> 00:11:37,860
material, because I've already walked you through the

160
00:11:37,880 --> 00:11:40,800
corresponding components in the slides.

161
00:11:40,820 --> 00:11:45,340
But please take a look at it closely and get familiar with the

162
00:11:45,380 --> 00:11:48,240
input data sequence syntax

163
00:11:48,260 --> 00:11:55,580
and again here is the unrolled loop, that I was talking to you during the slides

164
00:11:55,600 --> 00:12:01,100
preceding this video okay.

165
00:12:01,120 --> 00:12:06,620
This is what our code is going to look like, it looks off fully simple

166
00:12:10,390 --> 00:12:16,240
makes it really simple, we make it really simple how to interpret and create models.

167
00:12:16,260 --> 00:12:20,420
So here the vocabulary size is 943 number of labels

168
00:12:20,440 --> 00:12:26,820
was 129 this is unused again and a model dimensions here,

169
00:12:26,840 --> 00:12:30,080
the input dimension going to the vocabulary size,the label dimensions

170
00:12:30,100 --> 00:12:32,570
number of labels the embedding dim

171
00:12:32,590 --> 00:12:36,080
dimension is 150 you can change it, You can play around with it

172
00:12:36,100 --> 00:12:40,140
and see what impact it has in your classification results,

173
00:12:40,160 --> 00:12:45,340
from the hidden_dim that,we use for the LSTM block is set to 300.

174
00:12:45,360 --> 00:12:50,180
We create two containers, x standing for the input,Y stands for the labels

175
00:12:50,200 --> 00:12:54,580
and create the model, this one we have

176
00:12:54,600 --> 00:12:58,170
initialized with a default initial state

177
00:12:58,190 --> 00:13:01,520
and then we create our model.

178
00:13:01,540 --> 00:13:05,680
This is a sequential construct that you might be familiar

179
00:13:05,700 --> 00:13:09,540
if those of you have used libraries like Kara's.

180
00:13:09,560 --> 00:13:13,160
What it doing is taking the input

181
00:13:13,180 --> 00:13:15,640
whatever you pass it through would you

182
00:13:15,660 --> 00:13:17,820
at some point you would pass it to this model,

183
00:13:17,840 --> 00:13:21,140
it will process it to the embedding

184
00:13:21,160 --> 00:13:23,960
the output of the embedding would be concatenate would be

185
00:13:23,980 --> 00:13:26,620
passed into the recurrence model.

186
00:13:26,640 --> 00:13:31,780
And here you can see that we have recurrent blocks

187
00:13:31,800 --> 00:13:35,250
with the each of the blocks being an LSTM

188
00:13:35,270 --> 00:13:38,320
of the hidden dim and then you have

189
00:13:38,340 --> 00:13:43,700
the final output layer,which is the dense layer, which projects the

190
00:13:43,720 --> 00:13:52,740
internal states of the LSTM and into  number of labels

191
00:13:52,760 --> 00:13:58,250
that you want to classify. So let's go through this and see

192
00:13:58,270 --> 00:14:01,000
how we can take a peak into the model.

193
00:14:01,020 --> 00:14:08,860
Here if you can see here, you create the  model z and in case note

194
00:14:08,880 --> 00:14:13,000
that we haven't yet passed any input parameters

195
00:14:13,020 --> 00:14:18,450
yet input shape or data it has no notion of this x ok!.

196
00:14:18,590 --> 00:14:23,800
So we just call create model as shown here, and it creates that

197
00:14:23,820 --> 00:14:27,900
function at this point it's a simple function and when we try to

198
00:14:27,940 --> 00:14:32,060
look at the shape of the embedding.

199
00:14:32,080 --> 00:14:36,800
It gives me -1 to 150  with this** all of these being initialized to 0

200
00:14:36,820 --> 00:14:48,040
right, so sorry. Let's print the value of z component

201
00:14:48,060 --> 00:14:50,200
the embedding component of z.

202
00:14:50,220 --> 00:14:52,850
Let's print the embedding component of the z,

203
00:14:52,870 --> 00:14:56,200
and here we print the shape of the embedding matrix.

204
00:14:56,220 --> 00:15:00,800
You can see it's -1 and 150, -1 indicates that

205
00:15:00,820 --> 00:15:03,310
this model doesn't know

206
00:15:03,370 --> 00:15:06,350
what kind of input is going to get.

207
00:15:06,370 --> 00:15:09,690
Because there is no notion of x here

208
00:15:09,710 --> 00:15:13,130
in that and then we can also print

209
00:15:13,150 --> 00:15:17,060
the classify layer which is the Dense layer here,

210
00:15:17,080 --> 00:15:21,380
and print the bias values.

211
00:15:21,400 --> 00:15:23,740
So in this case you can see that

212
00:15:23,760 --> 00:15:27,080
there are going to be 129 of these

213
00:15:27,100 --> 00:15:29,660
and you would be able to see that

214
00:15:29,680 --> 00:15:34,280
all the bias cells have been initialized to 0.

215
00:15:34,300 --> 00:15:42,130
Now, if I pass the input parameter x

216
00:15:42,160 --> 00:15:49,640
which is defined here which is a container of length

217
00:15:49,660 --> 00:15:54,330
or 943 defined by the vocabulary size

218
00:15:54,350 --> 00:15:57,510
and then print the

219
00:15:57,530 --> 00:15:59,750
shape of the embedding matrix

220
00:15:59,820 --> 00:16:04,080
you can see that it becomes 943,150.

221
00:16:04,170 --> 00:16:06,660
So what I'm showing here is there are different ways you can

222
00:16:06,680 --> 00:16:09,400
inspect the model at different layers

223
00:16:09,420 --> 00:16:13,040
and different stages so feel free to explore that.

224
00:16:13,060 --> 00:16:16,930
Let's take a brief look at the data

225
00:16:16,960 --> 00:16:20,460
and the data reading component of it so here

226
00:16:20,480 --> 00:16:27,260
you will provide the data that the CNTK toolkit

227
00:16:27,280 --> 00:16:32,530
accepts and you can read more about the CNTKTextFormatReader

228
00:16:32,560 --> 00:16:36,550
there's a link here and it converts

229
00:16:36,570 --> 00:16:42,730
a sentence like this into a format

230
00:16:42,760 --> 00:16:47,770
that I talked about  here yep.

231
00:16:47,800 --> 00:16:53,970
And to facilitate some of these processing you there's some built-in

232
00:16:54,000 --> 00:16:56,950
convenience function that you can use

233
00:16:56,980 --> 00:17:00,680
to convert a text to a CTF format.

234
00:17:00,700 --> 00:17:05,510
And then once you have that let's define the reader

235
00:17:05,540 --> 00:17:11,080
which will read the different streams in this case query

236
00:17:11,100 --> 00:17:14,280
the unused intent and the labels

237
00:17:14,300 --> 00:17:20,330
and with that we are ready to trainer model.

238
00:17:20,350 --> 00:17:23,510
Without being equipped with the data

239
00:17:23,530 --> 00:17:24,710
in the right format

240
00:17:24,730 --> 00:17:27,040
and the model structure that you have,

241
00:17:27,130 --> 00:17:30,730
learned in the video and seen it in the code

242
00:17:30,900 --> 00:17:35,710
let's do the Train test and the predict workflow.

