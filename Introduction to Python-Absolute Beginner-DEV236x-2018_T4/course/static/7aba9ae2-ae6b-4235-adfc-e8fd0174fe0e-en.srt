0
00:00:00,100 --> 00:00:01,800
Now that you are familiar with the

1
00:00:01,820 --> 00:00:04,410
pre-processing steps that we have used

2
00:00:04,430 --> 00:00:09,000
to convert the solar.csv data, raw data

3
00:00:09,020 --> 00:00:12,840
from the IoT panel. Let's figure out

4
00:00:12,860 --> 00:00:17,970
how we feed it into our models, we have

5
00:00:17,990 --> 00:00:21,800
already said that we are going to look into the

6
00:00:21,820 --> 00:00:26,960
data for only 14 time points per day

7
00:00:26,980 --> 00:00:32,710
and we use a value of 20,000 to normalize

8
00:00:32,730 --> 00:00:36,500
the input into zero to one range then we call the

9
00:00:36,520 --> 00:00:39,440
generate_solar_data to read the

10
00:00:39,460 --> 00:00:42,840
data from this solar.csv file.

11
00:00:42,860 --> 00:00:45,420
Here's a small utility that would allow

12
00:00:45,440 --> 00:00:51,150
us to sample data sets from these

13
00:00:51,170 --> 00:00:55,960
data X and Y values, the input and the

14
00:00:55,980 --> 00:00:57,490
corresponding label values that have

15
00:00:57,510 --> 00:01:01,570
been read from the input data in that

16
00:01:01,590 --> 00:01:06,160
size of 14 times 10 which is 140 you can

17
00:01:06,180 --> 00:01:10,840
play around with that value and the data

18
00:01:10,860 --> 00:01:12,970
looks exactly like we have discussed in

19
00:01:12,990 --> 00:01:15,280
the previous video where there are

20
00:01:15,300 --> 00:01:18,840
different individual readings for the

21
00:01:18,860 --> 00:01:22,630
time point at which we are recording

22
00:01:22,650 --> 00:01:24,760
from the solar panel and the

23
00:01:24,780 --> 00:01:28,090
corresponding output which is the power

24
00:01:28,110 --> 00:01:31,290
generated for that particular day

25
00:01:31,310 --> 00:01:33,700
aggregated over the entire day so in

26
00:01:33,720 --> 00:01:35,350
this case you can see that these are

27
00:01:35,370 --> 00:01:39,160
different time instances for any

28
00:01:39,180 --> 00:01:42,100
given day and these numbers are the

29
00:01:42,120 --> 00:01:43,840
same because the total amount of current

30
00:01:43,860 --> 00:01:45,760
generated for that particular day is the

31
00:01:45,780 --> 00:01:47,560
same, but if you go and change these

32
00:01:47,580 --> 00:01:50,619
indexes to something else which I would

33
00:01:50,630 --> 00:01:52,600
encourage you to do so you can figure

34
00:01:52,620 --> 00:01:55,119
out that these numbers change and so

35
00:01:55,130 --> 00:01:57,250
does these numbers as well but at the

36
00:01:57,270 --> 00:02:00,369
end of the day the values of X and the

37
00:02:00,380 --> 00:02:03,970
corresponding Y's are what is needed for

38
00:02:03,990 --> 00:02:06,789
us at that point the notion of time the

39
00:02:06,800 --> 00:02:08,440
specific date doesn't make any

40
00:02:08,460 --> 00:02:11,220
difference, the network learns how to

41
00:02:11,240 --> 00:02:21,000
predict this Y's given any instances of the X's which is on sequence with

42
00:02:21,020 --> 00:02:25,659
length of 8 to 14.

43
00:02:25,670 --> 00:02:30,140
Here is our LSTM network setup, note that

44
00:02:30,160 --> 00:02:33,140
we do not specify the maximum length as

45
00:02:33,160 --> 00:02:35,540
a part of our model and we do not need

46
00:02:35,560 --> 00:02:39,230
to pad our variable length sequences so

47
00:02:39,250 --> 00:02:41,329
the sequences can be of length from 8 to

48
00:02:41,340 --> 00:02:46,099
14 and we do not have to pad zeros or

49
00:02:46,110 --> 00:02:49,519
any other values to our sequences that

50
00:02:49,530 --> 00:02:51,140
are less than 14 that toolkit

51
00:02:51,160 --> 00:02:55,000
automatically takes care of that, in this case

52
00:02:55,020 --> 00:02:58,370
we will have the LSTM cells hidden

53
00:02:58,390 --> 00:03:00,530
dimension or the internal state that we

54
00:03:00,550 --> 00:03:02,870
have referred to a certain values. You

55
00:03:02,890 --> 00:03:04,390
can play around with it in this case

56
00:03:04,410 --> 00:03:07,519
coincidentally it happened so that 14

57
00:03:07,530 --> 00:03:09,290
seems to be a pretty decent number you

58
00:03:09,310 --> 00:03:12,079
can try out different numbers for the

59
00:03:12,090 --> 00:03:15,609
hidden dimensions of the LSTM cells.

60
00:03:15,620 --> 00:03:18,290
This figure should be quite familiar to

61
00:03:18,310 --> 00:03:23,409
you where the individual data points are

62
00:03:23,420 --> 00:03:26,900
input to the LSTM cells this is part of

63
00:03:26,920 --> 00:03:29,900
the recurrence units

64
00:03:29,920 --> 00:03:33,560
and we just sample the output from the

65
00:03:33,580 --> 00:03:37,549
last LSTM cell and then we add a little

66
00:03:37,560 --> 00:03:40,640
bit of a drop out to prevent overfitting

67
00:03:40,660 --> 00:03:45,799
and then project the output to a single

68
00:03:45,810 --> 00:03:47,900
value which corresponds to the

69
00:03:47,920 --> 00:03:51,500
predicted output of the solar panel

70
00:03:51,520 --> 00:03:53,569
based on the input values that we have

71
00:03:53,580 --> 00:03:58,790
here, the network code is a direct

72
00:03:58,810 --> 00:04:01,640
translation of what the figure you have seen before,

73
00:04:01,660 --> 00:04:06,290
we have a set of recurrences with each

74
00:04:06,310 --> 00:04:08,950
of the cells being LSTM's and their

75
00:04:08,970 --> 00:04:12,180
internal state being hidden dimensions of 14.

76
00:04:12,200 --> 00:04:15,260
We sample the data or we pick up

77
00:04:15,280 --> 00:04:24,010
the data rather from the last cell, pass it to a dropout

78
00:04:24,030 --> 00:04:30,100
and then project our predicted

79
00:04:30,120 --> 00:04:33,330
value of the solar panel output here.

80
00:04:33,350 --> 00:04:37,300
Once we have the model then we are going

81
00:04:37,320 --> 00:04:40,030
to train this is again much too familiar

82
00:04:40,050 --> 00:04:42,880
to you except for the fact in this case

83
00:04:42,900 --> 00:04:53,470
instead of just C.input_variable you have a C.sequence.input_variable, this is

84
00:04:53,490 --> 00:04:55,390
important because we are dealing with

85
00:04:55,410 --> 00:05:00,270
sequence data, we get our model here

86
00:05:00,290 --> 00:05:06,610
and the corresponding labels are also a set

87
00:05:06,630 --> 00:05:09,700
of layer set of scalars for this

88
00:05:09,720 --> 00:05:15,340
particular set of input values yep!

89
00:05:15,360 --> 00:05:17,140
We set a learning rate, a fixed learning

90
00:05:17,160 --> 00:05:23,010
rate in this case and each create a schedule learning rate

91
00:05:23,030 --> 00:05:25,260
schedule in many applications you may

92
00:05:25,280 --> 00:05:27,060
want to explore with different learning

93
00:05:27,080 --> 00:05:28,740
rates and different learning rate

94
00:05:28,760 --> 00:05:32,220
schedules. You can try these things out

95
00:05:32,240 --> 00:05:35,760
in this tutorial if you want to. Our loss

96
00:05:35,780 --> 00:05:38,150
function in this case is different from

97
00:05:38,170 --> 00:05:40,440
what we have been using for them this

98
00:05:40,460 --> 00:05:42,300
data because that task was from

99
00:05:42,320 --> 00:05:43,830
classification in this case it's a

100
00:05:43,850 --> 00:05:46,280
regression, so we use a squared error

101
00:05:46,300 --> 00:05:48,900
where we minimize the difference between

102
00:05:48,920 --> 00:05:51,630
the predicted solar panel output and the

103
00:05:51,650 --> 00:05:53,850
observed output of the solar panel

104
00:05:53,870 --> 00:05:56,870
and the same is used for the error

105
00:05:56,890 --> 00:06:00,210
function then we can instantiate our

106
00:06:00,230 --> 00:06:03,420
trainer object. Once the trainer object

107
00:06:03,440 --> 00:06:07,800
is instantiated then you can run through

108
00:06:07,820 --> 00:06:13,700
the entire data set and you can

109
00:06:13,720 --> 00:06:16,970
train the parameters of the model by

110
00:06:16,990 --> 00:06:19,010
calling trainer.train_minibatch

111
00:06:21,130 --> 00:06:23,690
by feeding the corresponding minibatch

112
00:06:23,710 --> 00:06:26,900
features and the corresponding labels.

113
00:06:26,920 --> 00:06:29,400
This notebook is going to take a while

114
00:06:29,420 --> 00:06:33,120
to run and once it's done in this case

115
00:06:33,140 --> 00:06:36,150
you can see that the loss in general

116
00:06:36,170 --> 00:06:40,110
reduces quite a bit, here is the plot of

117
00:06:40,130 --> 00:06:43,220
how the training progresses over the

118
00:06:43,240 --> 00:06:49,500
number of different iterations. And you

119
00:06:49,520 --> 00:06:54,840
can then start plotting the mean square

120
00:06:54,860 --> 00:06:58,820
error for the training validation and test data

121
00:06:58,840 --> 00:07:03,220
here and finally you can visualize the predictions.

122
00:07:03,240 --> 00:07:07,790
Here you can see that the

123
00:07:07,810 --> 00:07:11,030
blue is the raw observations and the

124
00:07:11,050 --> 00:07:17,890
orange one is the predictions and in general they are pretty close.

125
00:07:17,910 --> 00:07:23,060
Even with 200 iterations is what we had

126
00:07:23,080 --> 00:07:25,310
started-off let me quickly check, if it

127
00:07:25,330 --> 00:07:31,450
was you have trained this one for 200 iterations.

128
00:07:31,470 --> 00:07:34,700
Let me quickly check that, yes it is

129
00:07:34,720 --> 00:07:38,960
indeed 200 passes that we have used but

130
00:07:38,980 --> 00:07:43,010
if you were to use a much larger amount

131
00:07:43,030 --> 00:07:47,570
of training which is set to 2,000 then

132
00:07:47,590 --> 00:07:49,880
you will see this curves becoming even

133
00:07:49,900 --> 00:07:52,630
closer to the observed values.

134
00:07:52,650 --> 00:07:55,140
With that you are ready

135
00:07:55,160 --> 00:08:00,000
to do the assignments associated with this lab.

