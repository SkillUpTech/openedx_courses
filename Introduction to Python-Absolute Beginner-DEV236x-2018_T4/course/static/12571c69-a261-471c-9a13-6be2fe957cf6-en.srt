0
00:00:00,790 --> 00:00:06,222
So what we do with this bowl-like function?

1
00:00:06,222 --> 00:00:10,820
We first start with a randomly initialized parameter value.

2
00:00:10,820 --> 00:00:14,250
Say this theta was initialized to

3
00:00:14,250 --> 00:00:15,880
theta 1 using some random value.

4
00:00:15,880 --> 00:00:18,400
So this would mean that the weights and the corresponding

5
00:00:18,400 --> 00:00:22,120
biases would be initialized with some random value, say, here.

6
00:00:24,900 --> 00:00:28,125
And we want to get to the bottom of the bowl.

7
00:00:29,310 --> 00:00:34,690
If we flatten this, that we are starting from a point here,

8
00:00:34,690 --> 00:00:38,940
and we want to get to this center point where

9
00:00:38,940 --> 00:00:43,510
the loss function has the least difference

10
00:00:43,510 --> 00:00:47,660
between the observed labels and the predicted labels.

11
00:00:49,790 --> 00:00:53,430
If you think about the way one would do it is, start

12
00:00:53,430 --> 00:00:58,500
marching along the slope of this bowl where it is the steepest.

13
00:00:58,500 --> 00:01:02,040
That would lead to the fastest way you can reach this

14
00:01:02,040 --> 00:01:02,880
minimum point.

15
00:01:05,270 --> 00:01:09,120
Mathematically one will do here is compute the gradient

16
00:01:09,120 --> 00:01:12,690
at any given point, so this would correspond to

17
00:01:12,690 --> 00:01:14,850
this point on this surface that you see here.

18
00:01:16,710 --> 00:01:22,348
You'll compute the gradient, the slope at that point and

19
00:01:22,348 --> 00:01:25,923
you would step towards the minimum.

20
00:01:25,923 --> 00:01:29,423
You'll take a step, and the direction of the step would be

21
00:01:29,423 --> 00:01:32,638
on the opposite direction where you have the steepest

22
00:01:32,638 --> 00:01:35,924
ascent because we want to descend along the curve, and

23
00:01:35,924 --> 00:01:38,010
hence the name gradient descent.

24
00:01:39,850 --> 00:01:44,450
You can see this value is referred to as the gradient

25
00:01:44,450 --> 00:01:48,030
of the loss with respect to that particular parameter, set theta.

26
00:01:49,995 --> 00:01:51,860
Mu is a new parameter, and

27
00:01:51,860 --> 00:01:54,560
it's extremely important that you recognize this parameter.

28
00:01:54,560 --> 00:01:55,620
It's called the learning rate.

29
00:01:56,700 --> 00:02:04,030
This controls how much our first step you want to take.

30
00:02:04,030 --> 00:02:09,146
If this mu is .001, you take

31
00:02:09,146 --> 00:02:14,680
a tiny step, very small step.

32
00:02:14,680 --> 00:02:18,790
And if you say, maybe at 1, then it will be a whole lot bigger.

33
00:02:20,630 --> 00:02:25,230
This is an important parameter in deep learning and one has

34
00:02:25,230 --> 00:02:29,350
to play with this parameter in a variety of different ways.

35
00:02:29,350 --> 00:02:31,673
That you'll learn as you go along in this course.

36
00:02:33,033 --> 00:02:37,706
This greatly helps finding the new set of parameters

37
00:02:37,706 --> 00:02:38,986
theta prime.

38
00:02:40,247 --> 00:02:45,636
So what we will do here is as shown in this animation,

39
00:02:45,636 --> 00:02:50,410
you would start with the point say here you will

40
00:02:50,410 --> 00:02:52,991
update the parameters.

41
00:02:52,991 --> 00:02:57,839
And in each iteration you would inch your way towards

42
00:02:57,839 --> 00:03:02,358
that final point in this lost function space that

43
00:03:02,358 --> 00:03:07,316
would have the least error between the label data Y and

44
00:03:07,316 --> 00:03:11,410
the predictions that are made by the model.

45
00:03:17,553 --> 00:03:20,570
However computing this total loss for

46
00:03:20,570 --> 00:03:24,725
large data set is expensive, and often redundant.

47
00:03:29,524 --> 00:03:32,679
So typically with deep learning,

48
00:03:32,679 --> 00:03:37,800
you do not compute the total loss across all the images.

49
00:03:37,800 --> 00:03:40,920
In this case, n, as we have introduced so far it was 60k.

50
00:03:42,210 --> 00:03:44,435
All the images in your training set.

51
00:03:48,424 --> 00:03:52,070
There are more details that you can find on this link below.

52
00:03:52,070 --> 00:03:56,230
But let me quickly take you to the variant of the gradient

53
00:03:56,230 --> 00:04:02,230
descent which has vastly changed the way

54
00:04:02,230 --> 00:04:06,800
you can scale up the learnings of very, very deep model.

55
00:04:06,800 --> 00:04:09,430
So there are two variants of

56
00:04:09,430 --> 00:04:12,000
the basic Gradient Descent Technique.

57
00:04:12,000 --> 00:04:15,690
The first one is called Stochastic Gradient Descent.

58
00:04:15,690 --> 00:04:17,830
So in Stochastic Gradient Descent,

59
00:04:17,830 --> 00:04:21,820
you update the parameters much more frequently than you would

60
00:04:21,820 --> 00:04:23,780
do in regular gradient descent.

61
00:04:23,780 --> 00:04:29,002
So theta 1 to theta 2 in the case of gradient

62
00:04:29,002 --> 00:04:35,364
descent was done using all the 60,000 images.

63
00:04:36,871 --> 00:04:42,430
In Stochastic Gradient Descent, you update theta

64
00:04:42,430 --> 00:04:47,480
1 to theta 2 by looking at one image at a time.

65
00:04:51,130 --> 00:04:55,210
Consequently, what happens is, say your loss for

66
00:04:55,210 --> 00:04:56,350
theta 1 was here.

67
00:04:57,600 --> 00:05:00,910
And since you are looking at only one image,

68
00:05:00,910 --> 00:05:05,200
the way the loss function gets reduced is very noisy.

69
00:05:05,200 --> 00:05:07,770
In the sense that sometimes after one iteration

70
00:05:07,770 --> 00:05:09,990
you would move in this direction.

71
00:05:09,990 --> 00:05:13,040
The new loss point would be here, then the second one,

72
00:05:13,040 --> 00:05:14,540
it might come here.

73
00:05:14,540 --> 00:05:18,320
Third one might even go in the wrong direction.

74
00:05:18,320 --> 00:05:24,400
But what we have seen is if you do this kind of optimization,

75
00:05:24,400 --> 00:05:29,290
with one image at a time, one of the short comings is that it

76
00:05:29,290 --> 00:05:31,935
might take a long time to reach the minimum.

77
00:05:33,385 --> 00:05:38,775
So here is a variant of SGD, which is a hybrid of the concept

78
00:05:38,775 --> 00:05:41,545
for Gradient Descent and the Stochastic Gradient Descent.

79
00:05:41,545 --> 00:05:43,455
And what I mean by that is,

80
00:05:43,455 --> 00:05:46,955
in this case you update the parameters using a mini-batch

81
00:05:46,955 --> 00:05:53,400
set, which means that you don't update it with only one image.

82
00:05:53,400 --> 00:05:57,560
You do not update it with the whole set either, but you can

83
00:05:57,560 --> 00:06:02,900
define, say, at any given time you're going to compute the loss

84
00:06:03,930 --> 00:06:08,075
from the first image to the 32nd image.

85
00:06:08,075 --> 00:06:10,936
So for a mini batch set of 32,

86
00:06:10,936 --> 00:06:15,559
this mini batch size indicates that there are 32

87
00:06:15,559 --> 00:06:19,757
samples of images that you are going to take.

88
00:06:19,757 --> 00:06:24,616
And the average loss for each of these images is gonna

89
00:06:24,616 --> 00:06:28,823
be your loss for that particular iteration.

90
00:06:28,823 --> 00:06:30,937
And based on this loss,

91
00:06:30,937 --> 00:06:36,365
you will update your theta 1 to the corresponding data two.

92
00:06:36,365 --> 00:06:40,042
Consequently what happens is you can start with a point,

93
00:06:40,042 --> 00:06:43,795
it will still be wiggly, it won't be as smooth as it was for

94
00:06:43,795 --> 00:06:47,415
the Gradient Descent, but it will be somewhat less so.

95
00:06:49,421 --> 00:06:54,339
And the fact that you can load a certain set of data points in to

96
00:06:54,339 --> 00:06:58,969
the memory rather quickly, the computational load that

97
00:06:58,969 --> 00:07:03,116
comes across is very easily parallelizable.

98
00:07:03,116 --> 00:07:07,379
I won't go into the details because that's beyond the scope

99
00:07:07,379 --> 00:07:11,474
of the course that we have here, but you can read a lot about

100
00:07:11,474 --> 00:07:15,335
the learners in the link that I have provided you here.

101
00:07:16,982 --> 00:07:19,697
One of the important things, though,

102
00:07:19,697 --> 00:07:23,479
to keep in mind is that SGD is not the only learner that is

103
00:07:23,479 --> 00:07:25,947
available to you as a programmer and

104
00:07:25,947 --> 00:07:30,260
as a person who is training deep learning models.

105
00:07:30,260 --> 00:07:33,425
Here are some of the popular learners that you have.

106
00:07:33,425 --> 00:07:38,180
Momentum-SGD, Nesterov, Adagrad, Adadelta,

107
00:07:38,180 --> 00:07:43,970
and Adam, and you can see here in this animation

108
00:07:43,970 --> 00:07:47,050
that they have different kinds of convergence property.

109
00:07:52,480 --> 00:07:54,740
Some of them wiggle more then the others,

110
00:07:54,740 --> 00:07:56,233
others are much smoother.

111
00:07:56,233 --> 00:08:00,979
And at the end of the day all of them find the optimal operating

112
00:08:00,979 --> 00:08:05,353
point which in this case is marked by the star this will be

113
00:08:05,353 --> 00:08:08,290
your final value of the lose.

114
00:08:08,290 --> 00:08:10,200
And this will be your model parameter.

115
00:08:12,300 --> 00:08:15,775
You will choose in your final selection of the model.

