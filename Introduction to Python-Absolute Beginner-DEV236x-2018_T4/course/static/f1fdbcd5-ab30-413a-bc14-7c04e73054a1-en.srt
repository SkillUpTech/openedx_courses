0
00:00:00,140 --> 00:00:02,276
Welcome to the lab for module 3,

1
00:00:02,276 --> 00:00:06,560
Recognizing MNIST Data with the Multi-layer Perceptron Model.

2
00:00:07,880 --> 00:00:11,300
This lab is stored as CNTK_103C.

3
00:00:11,300 --> 00:00:12,526
And I'm gonna open that now.

4
00:00:17,807 --> 00:00:22,630
And let's walk through the code here.

5
00:00:23,940 --> 00:00:26,300
And just to remind you of what our digits look like.

6
00:00:26,300 --> 00:00:29,490
These are 28x28 greyscale values.

7
00:00:29,490 --> 00:00:33,158
You can see different examples of the digit 5 here.

8
00:00:35,351 --> 00:00:39,605
All right, we're gonna start with importing our libraries,

9
00:00:39,605 --> 00:00:41,861
matplotlib, numpy, sys, os.

10
00:00:41,861 --> 00:00:43,880
And then here is cntk.

11
00:00:43,880 --> 00:00:46,166
We're gonna refer to that with capital C.

12
00:00:46,166 --> 00:00:48,433
And there's our matplotlib inline again.

13
00:00:52,160 --> 00:00:55,213
Here's the test code for the notebook itself, and

14
00:00:55,213 --> 00:00:56,420
we can ignore that.

15
00:00:57,690 --> 00:01:02,430
And like before, I'm gonna start skipping over some of these code

16
00:01:02,430 --> 00:01:05,040
sections that are the same as the previous labs.

17
00:01:05,040 --> 00:01:06,070
So this is one of them.

18
00:01:06,070 --> 00:01:06,676
But again,

19
00:01:06,676 --> 00:01:09,899
it's good to know that our input dimensions are still 784.

20
00:01:09,899 --> 00:01:13,570
And our number of output classes are 10.

21
00:01:13,570 --> 00:01:15,110
The data reading is the same.

22
00:01:15,110 --> 00:01:21,030
We create a reader that can read the CTF format for

23
00:01:21,030 --> 00:01:22,710
our labels and features.

24
00:01:22,710 --> 00:01:26,150
And then we find the directory that our test and

25
00:01:26,150 --> 00:01:28,290
training files are in.

26
00:01:28,290 --> 00:01:30,430
And we print that out here.

27
00:01:30,430 --> 00:01:32,270
Then we create our model.

28
00:01:32,270 --> 00:01:33,433
And this part is different.

29
00:01:33,433 --> 00:01:35,660
So let's take a look at this.

30
00:01:35,660 --> 00:01:40,415
We're still taking our 28x28 image data as a flat

31
00:01:40,415 --> 00:01:42,536
array of pixel values.

32
00:01:42,536 --> 00:01:44,020
And that's our input to our model.

33
00:01:45,680 --> 00:01:52,530
We're feeding that into a dense layer that is 400 wide.

34
00:01:52,530 --> 00:01:55,936
And the output of that dense layer goes into another hidden

35
00:01:55,936 --> 00:01:56,779
dense layer.

36
00:01:56,779 --> 00:01:58,553
So that's our second hidden layer.

37
00:01:58,553 --> 00:02:00,582
It's also 400 wide.

38
00:02:00,582 --> 00:02:03,986
And then our output layer is 10 nodes of a dense

39
00:02:03,986 --> 00:02:07,059
layer to predict each of our 10 classes.

40
00:02:08,390 --> 00:02:11,120
So this graph, it is slightly outdated.

41
00:02:11,120 --> 00:02:13,940
It says the middle layer is 200, but

42
00:02:13,940 --> 00:02:17,024
it's really gonna be 400 for our case.

43
00:02:17,024 --> 00:02:19,910
And then we'll do the same as before.

44
00:02:19,910 --> 00:02:22,236
We won't have a sigmoid or

45
00:02:22,236 --> 00:02:27,323
any other activation function at the last output layer.

46
00:02:27,323 --> 00:02:29,235
We'll take care of that with our loss function.

47
00:02:31,471 --> 00:02:33,910
So let's go down to the code.

48
00:02:33,910 --> 00:02:35,220
We're gonna set some variables.

49
00:02:35,220 --> 00:02:37,785
Number of hidden layers is gonna be 2.

50
00:02:37,785 --> 00:02:40,443
And the hidden dimension of each hidden layer,

51
00:02:40,443 --> 00:02:42,909
the number of nodes in it is gonna be 400.

52
00:02:44,390 --> 00:02:49,123
And again, we set up two input variables, one for our features

53
00:02:49,123 --> 00:02:53,689
of size input_dim, which is 784, store that in input.

54
00:02:53,689 --> 00:02:56,313
And num classes, which would be our labels, and

55
00:02:56,313 --> 00:02:57,470
store that in label.

56
00:02:58,840 --> 00:02:59,970
Here's where we build our model.

57
00:02:59,970 --> 00:03:01,918
So this is actually the piece of code.

58
00:03:01,918 --> 00:03:05,270
I think it may be the only piece of code in the notebook that's

59
00:03:05,270 --> 00:03:07,520
different from the previous notebook.

60
00:03:08,980 --> 00:03:11,835
So again, we use this default_options and

61
00:03:11,835 --> 00:03:15,462
the glorot_uniform to initialize the random values of

62
00:03:15,462 --> 00:03:16,620
the weights.

63
00:03:16,620 --> 00:03:19,400
And in this case, we're also saying the activation for

64
00:03:19,400 --> 00:03:21,370
each layer by default will be relu.

65
00:03:23,450 --> 00:03:26,410
So we start with our features and we store that in h.

66
00:03:26,410 --> 00:03:29,430
And then we go through this loop for

67
00:03:29,430 --> 00:03:32,920
a number of times directed by num_hidden_layers.

68
00:03:32,920 --> 00:03:36,145
And we create a dense layer whose dimension is

69
00:03:36,145 --> 00:03:38,580
hidden_layers_dim, which would be 400.

70
00:03:38,580 --> 00:03:42,751
And we pass h in as the input, and we get it as the output.

71
00:03:42,751 --> 00:03:46,080
So with these, they're gonna change layers together.

72
00:03:46,080 --> 00:03:48,657
And then finally after that, all the hidden layers,

73
00:03:48,657 --> 00:03:51,586
we're gonna put one final dense layer for the output layer.

74
00:03:51,586 --> 00:03:55,817
And on it we're gonna say, activation is none and

75
00:03:55,817 --> 00:03:57,440
the width is 10.

76
00:03:57,440 --> 00:04:02,090
And finally, we'll return the value of the model we created.

77
00:04:02,090 --> 00:04:05,224
And then actually, this line is a typo.

78
00:04:05,224 --> 00:04:09,462
I think I may have created that by mistake at one point and

79
00:04:09,462 --> 00:04:10,380
left it in.

80
00:04:10,380 --> 00:04:13,001
Here's the real line that happens.

81
00:04:13,001 --> 00:04:14,990
So it overrides what was done here.

82
00:04:14,990 --> 00:04:17,210
It creates the model passing input,

83
00:04:17,210 --> 00:04:20,400
which is our binding input variable.

84
00:04:20,400 --> 00:04:24,670
And we divide by 255 again to normalize the data between 0

85
00:04:24,670 --> 00:04:25,263
and 1.

86
00:04:25,263 --> 00:04:27,260
That gives us our model.

87
00:04:27,260 --> 00:04:28,208
We store that in z.

88
00:04:30,727 --> 00:04:34,693
And then again, we do same as before.

89
00:04:34,693 --> 00:04:38,750
We'll do a cross_entropy_with_softmax loss.

90
00:04:38,750 --> 00:04:44,117
We'll do a classification_error metric or label error.

91
00:04:44,117 --> 00:04:46,464
And we're getting ready for our minibatch loop here.

92
00:04:46,464 --> 00:04:49,241
So we set our parameters, same as before, 0.2 for

93
00:04:49,241 --> 00:04:51,230
the learning rate.

94
00:04:51,230 --> 00:04:52,600
And we create our trainer.

95
00:04:52,600 --> 00:04:56,161
We have our helper function for the moving average and

96
00:04:56,161 --> 00:04:59,576
our helper function for print training progress.

97
00:04:59,576 --> 00:05:03,760
And then here's our minibatch parameters, 64 size,

98
00:05:03,760 --> 00:05:06,295
60,000 training samples.

99
00:05:06,295 --> 00:05:12,225
The number of epochs or time to sweep the whole data is 10.

100
00:05:12,225 --> 00:05:17,911
So we'll do 600,000 samples in our training.

101
00:05:17,911 --> 00:05:20,390
And as before, we create a reader.

102
00:05:20,390 --> 00:05:21,560
We create an input map.

103
00:05:22,750 --> 00:05:24,799
We set up these variables.

104
00:05:24,799 --> 00:05:25,860
We go through the loop.

105
00:05:26,930 --> 00:05:32,865
We grab our next set of data in the minibatch_size, which is 64.

106
00:05:32,865 --> 00:05:36,770
So our next 64 samples of data, and we train with those.

107
00:05:38,280 --> 00:05:40,902
And then we do our print progress.

108
00:05:40,902 --> 00:05:44,945
And then we take the three variables, minibatch number,

109
00:05:44,945 --> 00:05:46,602
the loss and the error.

110
00:05:46,602 --> 00:05:48,626
And we store them in our plotdata variable so

111
00:05:48,626 --> 00:05:50,330
we can have them for plotting later.

112
00:05:52,188 --> 00:05:55,389
And here's the actual run of our print progress that shows

113
00:05:55,389 --> 00:05:58,100
minibatches again going from 0 to 9,000.

114
00:05:58,100 --> 00:06:04,291
And our loss, starting at 2.3 and going pretty steadily down.

115
00:06:04,291 --> 00:06:06,290
It doesn't seem too much variation.

116
00:06:06,290 --> 00:06:10,905
It seems more, Closing in than

117
00:06:10,905 --> 00:06:13,904
previously on our logistic regression model.

118
00:06:13,904 --> 00:06:15,238
And the same goes true for the error.

119
00:06:15,238 --> 00:06:18,108
In fact, there's a bunch of zeros here for

120
00:06:18,108 --> 00:06:20,440
our error after we get up to there.

121
00:06:20,440 --> 00:06:23,760
So, again, we plot these using matplotlib.

122
00:06:23,760 --> 00:06:28,000
We create two plots, one for the loss and one for the error.

123
00:06:29,040 --> 00:06:31,656
And yeah, we can see that this goes pretty close to 0.

124
00:06:31,656 --> 00:06:36,562
And then this looks like it goes even more at 0 also.

125
00:06:39,400 --> 00:06:42,228
Let's evaluate it and see what our error is with

126
00:06:42,228 --> 00:06:45,290
our holdout data, which in this case is our test data.

127
00:06:46,470 --> 00:06:48,510
So to do that, again, we create a reader,

128
00:06:48,510 --> 00:06:50,471
we create a map, seeing the pattern now.

129
00:06:51,600 --> 00:06:54,845
We say, test_minibatch_size is gonna be 512 again because we're

130
00:06:54,845 --> 00:06:55,490
evaluating.

131
00:06:56,630 --> 00:07:00,907
And we go through the loop and trainer.test_minibatch.

132
00:07:00,907 --> 00:07:05,248
And we get out average test error is 1.74.

133
00:07:05,248 --> 00:07:08,710
So I think it was 7.8 in our logistic regression.

134
00:07:08,710 --> 00:07:11,694
So we've done notably better,

135
00:07:11,694 --> 00:07:17,340
dramatically better using this MLP with two hidden layers.

136
00:07:18,480 --> 00:07:20,543
Then again, we're going to do the prediction.

137
00:07:20,543 --> 00:07:23,386
So we extend it with the softmax.

138
00:07:23,386 --> 00:07:26,838
And we come through here, the same code as before.

139
00:07:26,838 --> 00:07:28,989
And now, if we look at our predicted label and

140
00:07:28,989 --> 00:07:34,320
our actual labels, They look much better.

141
00:07:34,320 --> 00:07:38,986
In fact, just eyeing these, I don't think I see a mismatch.

142
00:07:38,986 --> 00:07:39,860
So that's pretty good.

143
00:07:39,860 --> 00:07:42,400
Then we're gonna sample one of the labels again,

144
00:07:42,400 --> 00:07:46,620
image number 5, and we'll plot it out.

145
00:07:46,620 --> 00:07:47,785
And then we'll print the label.

146
00:07:47,785 --> 00:07:50,340
And this time, it's the same 9.

147
00:07:50,340 --> 00:07:53,380
And it looks like we got the label correct this time for 9.

148
00:07:53,380 --> 00:07:57,510
So that's it for our tour of the notebook.

149
00:07:57,510 --> 00:07:59,920
Now, it's time for you to go back and

150
00:07:59,920 --> 00:08:02,080
open the notebook on your own machine.

151
00:08:02,080 --> 00:08:03,870
Go through, run it once.

152
00:08:03,870 --> 00:08:07,064
Try changing some values.

153
00:08:07,064 --> 00:08:09,140
Try printing out intermediate values.

154
00:08:10,180 --> 00:08:13,028
Really try and understand everything that happens.

155
00:08:13,028 --> 00:08:17,510
And get a feel for the code.

156
00:08:17,510 --> 00:08:21,150
And we also suggest you try the exploration

157
00:08:21,150 --> 00:08:22,390
exercises at the bottom.

158
00:08:23,900 --> 00:08:25,070
Okay, that's it for this lab.

