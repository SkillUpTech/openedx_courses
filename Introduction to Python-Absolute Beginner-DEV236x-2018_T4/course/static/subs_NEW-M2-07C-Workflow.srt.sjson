{
  "start": [
    460, 
    4960, 
    7960, 
    11240, 
    12400, 
    14886, 
    17808, 
    23700, 
    29230, 
    31205, 
    36630, 
    38861, 
    44790, 
    46170, 
    49300, 
    51969, 
    57280, 
    61497, 
    65530, 
    68717, 
    76700, 
    79580, 
    83901, 
    88152, 
    95290, 
    97830, 
    100340, 
    104280, 
    107720, 
    112400, 
    116480, 
    121300, 
    126495, 
    131322, 
    136860, 
    140130, 
    144060, 
    148440, 
    153860, 
    157180, 
    162140, 
    166790, 
    171350, 
    174790, 
    178320, 
    185400, 
    190670, 
    194700, 
    201010, 
    205663, 
    210019, 
    216440, 
    219910, 
    226130, 
    232210, 
    235023, 
    238992, 
    239580, 
    244366, 
    248039, 
    253290, 
    258523, 
    266300, 
    270950, 
    276640, 
    280570, 
    284700, 
    288780, 
    293033, 
    297287, 
    300757, 
    302893, 
    306480, 
    312090, 
    316185, 
    317610, 
    322463, 
    325897, 
    330157, 
    335161, 
    340630, 
    344630, 
    348770, 
    352821, 
    355582, 
    360000, 
    364250, 
    370842, 
    376657, 
    379780, 
    382590, 
    386700, 
    390700, 
    393780, 
    396730, 
    403400, 
    407560, 
    411580, 
    415496, 
    419599, 
    424357
  ], 
  "end": [
    4960, 
    7960, 
    11240, 
    12400, 
    14886, 
    17808, 
    21900, 
    29230, 
    31205, 
    36630, 
    38861, 
    44790, 
    46170, 
    49300, 
    51969, 
    55420, 
    61497, 
    65530, 
    68717, 
    74429, 
    78440, 
    83901, 
    88152, 
    92960, 
    97830, 
    100340, 
    102510, 
    107720, 
    112400, 
    116480, 
    118660, 
    126495, 
    131322, 
    135150, 
    140130, 
    144060, 
    148440, 
    153860, 
    157180, 
    160820, 
    166790, 
    169010, 
    174790, 
    178320, 
    183140, 
    190670, 
    194700, 
    198310, 
    205663, 
    210019, 
    213090, 
    219910, 
    226130, 
    230140, 
    235023, 
    238992, 
    239580, 
    244366, 
    248039, 
    253290, 
    258523, 
    264760, 
    270950, 
    275400, 
    280570, 
    284700, 
    286620, 
    293033, 
    297287, 
    300757, 
    302893, 
    306480, 
    310810, 
    316185, 
    317610, 
    322463, 
    325897, 
    330157, 
    335161, 
    340630, 
    344630, 
    346890, 
    352821, 
    355582, 
    360000, 
    364250, 
    370842, 
    376657, 
    379780, 
    382590, 
    386700, 
    390700, 
    393780, 
    396730, 
    403400, 
    407560, 
    411580, 
    415496, 
    419599, 
    424357, 
    426240
  ], 
  "text": [
    "You have so far been introduced to building a model function.", 
    "You have know about lost functions and learners,", 
    "which help you find the optimal parameters", 
    "while training your model.", 
    "So, let's put these things together and", 
    "build our first workflow for the MNIST dataset.", 
    "So we start with the MNIST training database.", 
    "We will sample 128 handwritten images.", 
    "These are called mini-batches.", 
    "These mini-batches are laid out flat.", 
    "Just like we have talking about while building the model.", 
    "You can see I am choosing 128 images for my mini batch sample.", 
    "This could be any arbitrary one.", 
    "It doesn't have to 128.", 
    "And corresponding to those images,", 
    "we have the labels which are one-hot encoded.", 
    "The parameters in the logistic regression", 
    "models have the weights and the bias.", 
    "One weight matrix of dimensions 10 x 784.", 
    "And the bias vector has a dimension of 10.", 
    "Our model is very simple.", 
    "Weight times the image pixels plus the bias vector.", 
    "The output of the model is some values", 
    "generated from these matrix operation.", 
    "As I eluded to you in the previous video,", 
    "that we are going to convert this output", 
    "into probabilities using softmax.", 
    "Those probabilities then feed into my loss function,", 
    "which is cross entropy, and we use a combined", 
    "function called cross entropy with softmax, in our case.", 
    "An output of this function is gonna be the loss.", 
    "Optionally, in addition to this model and the loss function, you", 
    "can also define another function you call the classification", 
    "error, which compares the output of the model.", 
    "Classifies it into the corresponding category and", 
    "compares the value with the label.", 
    "So, in which case, if the input image was an image of", 
    "digit 3 and the label was 3, then there would be no error.", 
    "As opposed to if the label was 3 and the classification of", 
    "the input image was 8, then that would generate an error.", 
    "So you can count the number of mistakes the model makes", 
    "using this function, called the error function.", 
    "Now you can combine all these together", 
    "into a trainer object which takes the model,", 
    "the loss, the optional error, and the learner.", 
    "And you can call the trainer train_minibatch.", 
    "And this would update the model parameters", 
    "from theta 1 to theta 2 and so on and so forth.", 
    "So the learners as I'd alluded to before can be any one of", 
    "the SGD Autograd etc, any of your hands on exercises", 
    "you'll get to play along with that.", 
    "In the previous module we alluded to the fact that as you", 
    "train you also want to find out what particular settings", 
    "are gonna be the optimal choice for your final module.", 
    "And then one other the way of doing this is using", 
    "the validation workflow that was explained to you in the previous", 
    "module.", 
    "And the idea is that you choose the minimum loss that", 
    "you can find in your validation step and", 
    "use those heater stars as your final model parameters.", 
    "Once you have the final model parameters, you can", 
    "then test how the model would perform on the test database.", 
    "And this gives you a good idea how the model when deployed on", 
    "the field, say in a phone or a web service or some application.", 
    "How the performance of that model would be and", 
    "you report out the average error by iterating through", 
    "all the sample in the tested set.", 
    "So here is the test workflow, you don't use the training", 
    "dataset you use the test dataset, you sample a mini batch", 
    "will be different from the training dataset.", 
    "And in this case the models are already fixed.", 
    "So w * and b * are fixed referring to the theta *", 
    "parameter that I has illustrated in the previous slide.", 
    "You can now feed the individual data points from your test data", 
    "set into your model.", 
    "And compare it with one-hot encoded labels to calculate", 
    "the error that you would get from the test", 
    "dataset by running Trainer.test_minibatch.", 
    "And then finally you'd report the classification", 
    "error as percent incorrectly labeled MNIST image.", 
    "You can choose other error reporting functions as well, but", 
    "classification error is one of the popular ones.", 
    "Finally, after doing the due diligence of choosing", 
    "the right model, and you're happy", 
    "with the performance in the test database that you have,", 
    "you can then deploy this particular model into an app.", 
    "The app or the web service would sample a handwritten image,", 
    "provided there's an input into your eval function,", 
    "evaluation function here.", 
    "You'll take the image as an input and", 
    "generate the softmax probabilities associated", 
    "with that particular image, belonging to a particular class.", 
    "In this case, you can see that the last", 
    "index has the large value of point seven.", 
    "You can use a simple Numpy Argmax function to go", 
    "through this array and find the index where you have the highest", 
    "value in this case the nth index longs to the digit 9.", 
    "You can do one digit at a time or you can have the eval", 
    "function process a whole bunch of images in one go and", 
    "generated the corresponding digit from the classifier that", 
    "you have just trained."
  ]
}