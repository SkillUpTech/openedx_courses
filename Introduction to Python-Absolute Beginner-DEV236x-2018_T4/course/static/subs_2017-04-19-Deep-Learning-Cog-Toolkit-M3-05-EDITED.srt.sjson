{
  "start": [
    860, 
    4390, 
    8980, 
    14180, 
    16990, 
    21960, 
    26220, 
    29170, 
    33765, 
    39620, 
    43326, 
    46060, 
    49380, 
    54170, 
    58820, 
    61730, 
    65410, 
    75449, 
    82080, 
    87190, 
    91730, 
    95882, 
    100302, 
    104904, 
    109917, 
    113800, 
    118310, 
    122115, 
    125930, 
    131542, 
    134800, 
    139870, 
    145694, 
    152400, 
    157347, 
    163840, 
    168786, 
    173287, 
    177810, 
    182550, 
    185030, 
    188330, 
    192844, 
    198285, 
    208946, 
    212160, 
    216510, 
    219840, 
    223387, 
    227205, 
    233870, 
    237920, 
    242360, 
    246670, 
    252370, 
    258486, 
    264802, 
    268288, 
    273157, 
    279441, 
    284010, 
    289372, 
    291277, 
    293667, 
    301110, 
    303899
  ], 
  "end": [
    4390, 
    7920, 
    12130, 
    16990, 
    20121, 
    26220, 
    29170, 
    33765, 
    39620, 
    43326, 
    46060, 
    47590, 
    52590, 
    57620, 
    61730, 
    65410, 
    72609, 
    80370, 
    87190, 
    89240, 
    95882, 
    100302, 
    104904, 
    107270, 
    113800, 
    118310, 
    122115, 
    125930, 
    129306, 
    133430, 
    139870, 
    145694, 
    150630, 
    157347, 
    163840, 
    168786, 
    173287, 
    177810, 
    182550, 
    185030, 
    186700, 
    190636, 
    195906, 
    206590, 
    212160, 
    216510, 
    219840, 
    220571, 
    227205, 
    232125, 
    237920, 
    241100, 
    246670, 
    250440, 
    258486, 
    264802, 
    268288, 
    273157, 
    279441, 
    284010, 
    289372, 
    291277, 
    293667, 
    298336, 
    303899, 
    308150
  ], 
  "text": [
    "Now, let's look at some of the other options we have for", 
    "activations function, starting with the hyperbolic tangent.", 
    "Now let's look at the hyperbolic tangent, tanh.", 
    "Here, we map the input,", 
    "instead of zero to one, to a range of -1 to 1.", 
    "And it's, like sigmoid, its activation also saturates but", 
    "its output is zero centered, that means", 
    "if you have an input value of zero the output is also zero.", 
    "Tanh is usually preferred over sigmoid because of this", 
    "even though it does have the same issue with saturation and", 
    "the two ends.", 
    "And tanh can be scaled,", 
    "a representation of a sigmoid function.", 
    "Now let's come to one of the most popular", 
    "activation functions called ReLU.", 
    "ReLU stands for rectified linear units.", 
    "And it's very popular, simply thresholds values below 0.", 
    "One of the big pros for ReLU is its fast convergence.", 
    "Stochastic gradient descent convergence much faster", 
    "compared to sigmoid and tanh.", 
    "It's arguably due to its linear non saturating form,", 
    "what it means in this range there is no notion of saturation", 
    "like in case of sigmoid, we saw that saturation happening,", 
    "there is no such thing in here.", 
    "And it's relatively simple to implement.", 
    "It involves thresholding of an activation matrix at zero.", 
    "If any value of this w is less than 0,", 
    "then you simply output 0.", 
    "So it's pretty straight forward to implement.", 
    "However, it is a bit fragile.", 
    "It can irreversibly die when large gradient flows.", 
    "What it means is if you, say, your function output, say,", 
    "was 2 at some point, which would be here.", 
    "And all of a sudden, for a new update,", 
    "the output became, say, -1 of this function,", 
    "then in this case the out put would have", 
    "been 2, output of the ReLU unit.", 
    "Here, the output will simply be 0.", 
    "And once it is 0, in subsequent iterations,", 
    "the neuron pretty much doesn't update anything.", 
    "It kind of dies.", 
    "So it has stopped learning new things.", 
    "To overcome some of these limitations of ReLU,", 
    "People have proposed Leaky ReLU to fix the dying ReLU problem.", 
    "For a negative input, instead of zero,", 
    "there is a small negative slope, alpha, which is here.", 
    "You can see that slope.", 
    "It's not zero.", 
    "And this slope which is, for Leaky ReLU,", 
    "is a small slope is typically set to a value of 0.1.", 
    "Another version of this Leaky ReLU is called", 
    "parametric ReLU or called PReLU.", 
    "The slope here is parameterised, in other words, you learn", 
    "what this value of alpha will be during the training process.", 
    "An even more advanced technique called maxOut, and it's", 
    "basically dot product between the weights and the data.", 
    "If it sounds a bit complicated, please bear with me.", 
    "It is the max of two different lines,", 
    "each parameterized by these two variables a and", 
    "c, which can be learned, again.", 
    "And you can see that both of these are equations of lines,", 
    "so these are lines.", 
    "And ReLU, Leaky ReLU,", 
    "parametric ReLU are special cases of Maxout.", 
    "It enjoys the benefit of all", 
    "except the number of parameters double."
  ]
}