{
  "start": [
    280, 
    7390, 
    11690, 
    15890, 
    19680, 
    24345, 
    27026, 
    33153, 
    35816, 
    40933, 
    47510, 
    52010, 
    56430, 
    59099, 
    60776, 
    65450, 
    70418, 
    75266, 
    78933, 
    82740, 
    89499, 
    96256, 
    103178, 
    113760, 
    117974, 
    124360, 
    128640, 
    133650, 
    138571, 
    146640, 
    151890, 
    157480, 
    159730, 
    165560, 
    174460, 
    180071, 
    185816, 
    191048, 
    196177, 
    201750, 
    206880, 
    211010, 
    215260, 
    217490, 
    221050, 
    225570, 
    230590, 
    233250, 
    237300, 
    241110, 
    244020, 
    247130, 
    250980, 
    253800, 
    258910, 
    262770, 
    266460, 
    270280, 
    273415, 
    280120, 
    285838, 
    291331, 
    297513, 
    302545, 
    307167, 
    311788, 
    315503, 
    318861, 
    320660, 
    328040, 
    332880, 
    337622, 
    342720, 
    345250, 
    349240, 
    353520, 
    357270, 
    360700, 
    363133, 
    367355, 
    372718, 
    378080, 
    383670, 
    388373, 
    392729, 
    397356, 
    401280, 
    405950, 
    410520, 
    416520, 
    421800, 
    427480, 
    432813, 
    438145, 
    444367, 
    449719, 
    453959, 
    459789, 
    464771, 
    470640, 
    475730, 
    482210, 
    487530, 
    493130, 
    497586, 
    502274
  ], 
  "end": [
    7390, 
    10230, 
    15890, 
    18320, 
    24345, 
    27026, 
    30115, 
    35816, 
    40933, 
    45850, 
    52010, 
    56430, 
    59099, 
    60776, 
    62720, 
    70418, 
    75266, 
    78933, 
    82740, 
    89499, 
    96256, 
    101183, 
    111199, 
    117974, 
    122690, 
    128640, 
    133650, 
    138571, 
    143960, 
    151890, 
    157480, 
    159730, 
    165560, 
    172450, 
    180071, 
    185816, 
    191048, 
    196177, 
    201750, 
    206880, 
    211010, 
    215260, 
    217490, 
    221050, 
    225570, 
    229020, 
    233250, 
    237300, 
    238820, 
    244020, 
    247130, 
    250980, 
    252780, 
    257360, 
    262770, 
    266460, 
    268450, 
    273415, 
    277200, 
    285838, 
    291331, 
    297513, 
    302545, 
    307167, 
    311788, 
    315503, 
    318861, 
    320660, 
    328040, 
    331780, 
    336319, 
    342720, 
    345250, 
    349240, 
    353520, 
    357270, 
    360700, 
    363133, 
    367355, 
    372718, 
    378080, 
    383670, 
    388373, 
    392729, 
    397356, 
    401280, 
    405950, 
    409210, 
    416520, 
    421800, 
    427480, 
    432813, 
    438145, 
    444367, 
    449719, 
    453959, 
    459789, 
    464771, 
    470640, 
    475730, 
    482210, 
    487530, 
    493130, 
    497586, 
    502274, 
    504117
  ], 
  "text": [
    "Now let's train a text classifier using the dataset.", 
    "You have trained the classifier using the analyst digit.", 
    "We have trained a recurrence based model", 
    "using the solar panel times series data.", 
    "In this case we are gonna adopt the same workflow that you", 
    "are much too familiar with, and", 
    "use it to build our text classifier.", 
    "In this case, our data here,", 
    "our sentences from the ATIS data set, and this sentences", 
    "are also called sequences in the world of recurrence.", 
    "Our model is going to be what we described just before in this", 
    "lecture and we'll see how it fits in the workflow.", 
    "And the rest of it is quite boiler plate.", 
    "We are much too familiar with so", 
    "will not spend much more time on it.", 
    "So start with the ATIS training data set here, and", 
    "we will draw 96 sentences well in this case will", 
    "call them sequences sentences, and", 
    "sequences will be used anonymously.", 
    "You can see that the first sentence has 23 tokens,", 
    "each token corresponds to a word, In the sentence,", 
    "and we have drawn 96 set sentences.", 
    "Each token is represented by a vector of length 943 elements.", 
    "Corresponding to the training samples that we have drawn.", 
    "There are 96 such sequences with the able.", 
    "So for the first sentence here you will", 
    "have 23 such slots re positions with each of", 
    "this positions having a one-hot", 
    "encoded of length 129 is also shown here.", 
    "And then, you'll have 96 such sequences of one-hot encoded", 
    "vectors as your corresponding label Y,", 
    "now this is pretty simple now.", 
    "What we do is, during the training process we'll take", 
    "the input token, we'll embed it into a 150 dimensional space.", 
    "The output of that will be fed into the recurrent locks", 
    "composed of LSTM units with the dimensions of 300.", 
    "The output of those LSTM units are going to be projected into", 
    "129 dimensional space corresponding to the classes,", 
    "in which the input tokens are going to be classified.", 
    "Now note that we do not have a specific directive here", 
    "to say that the first sequence has 23 tokens, the second", 
    "sequence has 15 tokens, the third one has nine tokens.", 
    "We do not have any specific directive.", 
    "The tool kit that we're gonna be using automatically", 
    "takes care number of recurring units needed to process the data", 
    "that you are going to be feeding into this trainer.", 
    "As the training progresses it automatically", 
    "takes care of the number of recurrent units that are needed", 
    "to model that particular sequence.", 
    "Since we're gonna be classifying each of the tokens", 
    "into the corresponding 129 classes.", 
    "We use cross_entropy_with_softmax,", 
    "where the predicted token label", 
    "is compared with the label provided in the training set.", 
    "You can also use classification_error as our", 
    "chosen function here to categorize how many", 
    "tokens were correctly classified.", 
    "And then instantiate the trainer object with the model,", 
    "the loss and the error function and the learner of your choice.", 
    "And then update the parameters in the model using the trainer", 
    "object and feeding the trainer with multiple instances of", 
    "mini batches of it is training samples using train_minibatch.", 
    "Some of the learners that worked really well in case of text", 
    "data or atom and auto grand you can try other solvers or", 
    "learners that we have in the tool kit to see what effect", 
    "the learners have on modelling accuracy.", 
    "Once you are comfortable with the trained model and", 
    "have chosen a final one.", 
    "We can used the Test Data set to measure the error in classifying", 
    "the tokens in the Test Data set and report the average error.", 
    "So, to test the model of these start with the ATIS Data set.", 
    "And draw 32 samples, in this case these are 32 sequences,", 
    "each of the sequence having multiple tokens.", 
    "Corresponding to each of those tokens here, there's gonna be", 
    "a one-hot encoded label which will be used to measure", 
    "how well our model is doing with the trained parameters.", 
    "In this case, the model parameters are frozen,", 
    "just like it has been done before.", 
    "And we use the trainer.test_minibatch to", 
    "measure what the error is going to be for that particular", 
    "minibatch, and we repeatedly sample new data sets from", 
    "this test data base and report the average classification", 
    "error as the percent incorrectly label tokens.", 
    "So now that you have trained the classifier let's see how", 
    "the prediction work flow would look like so, say a user comes", 
    "to your search engine and provides us string data.", 
    "In this case, it's flight from New York to Seattle, BOS stands", 
    "for beginning of sentence, EOS stands for end of sentence.", 
    "And you wanna classify these tokens, in this case note that", 
    "even though New York got two tokens they should be classified", 
    "as the from destination and Seattle as the to destination.", 
    "In this case there are eight tokens, one,", 
    "two, three, four, five, six, seven,", 
    "eight, and each of those eight tokens have one app", 
    "encoding of vector 943 elements each.", 
    "Each of those tokens are gonna get classified,", 
    "into the corresponding labels using the model.eval function,", 
    "which will take the features on the sequence data here,", 
    "as the input, and in the corresponding probabilities.", 
    "And these probabilities are going to be, a vector of lent", 
    "129, one for each token so they're gonna be eight of those.", 
    "You can use the same numpy.argmax function", 
    "to fish out the index in this are which has the maximum value.", 
    "Then look it up in your label dictionary to find", 
    "the corresponding token label that each of these", 
    "tokens belong to."
  ]
}