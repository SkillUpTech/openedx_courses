0
00:00:00,280 --> 00:00:07,390
Now let's train a text classifier using the dataset.

1
00:00:07,390 --> 00:00:10,230
You have trained the classifier using the analyst digit.

2
00:00:11,690 --> 00:00:15,890
We have trained a recurrence based model

3
00:00:15,890 --> 00:00:18,320
using the solar panel times series data.

4
00:00:19,680 --> 00:00:24,345
In this case we are gonna adopt the same workflow that you

5
00:00:24,345 --> 00:00:27,026
are much too familiar with, and

6
00:00:27,026 --> 00:00:30,115
use it to build our text classifier.

7
00:00:33,153 --> 00:00:35,816
In this case, our data here,

8
00:00:35,816 --> 00:00:40,933
our sentences from the ATIS data set, and this sentences

9
00:00:40,933 --> 00:00:45,850
are also called sequences in the world of recurrence.

10
00:00:47,510 --> 00:00:52,010
Our model is going to be what we described just before in this

11
00:00:52,010 --> 00:00:56,430
lecture and we'll see how it fits in the workflow.

12
00:00:56,430 --> 00:00:59,099
And the rest of it is quite boiler plate.

13
00:00:59,099 --> 00:01:00,776
We are much too familiar with so

14
00:01:00,776 --> 00:01:02,720
will not spend much more time on it.

15
00:01:05,450 --> 00:01:10,418
So start with the ATIS training data set here, and

16
00:01:10,418 --> 00:01:15,266
we will draw 96 sentences well in this case will

17
00:01:15,266 --> 00:01:18,933
call them sequences sentences, and

18
00:01:18,933 --> 00:01:22,740
sequences will be used anonymously.

19
00:01:22,740 --> 00:01:29,499
You can see that the first sentence has 23 tokens,

20
00:01:29,499 --> 00:01:36,256
each token corresponds to a word, In the sentence,

21
00:01:36,256 --> 00:01:41,183
and we have drawn 96 set sentences.

22
00:01:43,178 --> 00:01:51,199
Each token is represented by a vector of length 943 elements.

23
00:01:53,760 --> 00:01:57,974
Corresponding to the training samples that we have drawn.

24
00:01:57,974 --> 00:02:02,690
There are 96 such sequences with the able.

25
00:02:04,360 --> 00:02:08,640
So for the first sentence here you will

26
00:02:08,640 --> 00:02:13,650
have 23 such slots re positions with each of

27
00:02:13,650 --> 00:02:18,571
this positions having a one-hot

28
00:02:18,571 --> 00:02:23,960
encoded of length 129 is also shown here.

29
00:02:26,640 --> 00:02:31,890
And then, you'll have 96 such sequences of one-hot encoded

30
00:02:31,890 --> 00:02:37,480
vectors as your corresponding label Y,

31
00:02:37,480 --> 00:02:39,730
now this is pretty simple now.

32
00:02:39,730 --> 00:02:45,560
What we do is, during the training process we'll take

33
00:02:45,560 --> 00:02:52,450
the input token, we'll embed it into a 150 dimensional space.

34
00:02:54,460 --> 00:03:00,071
The output of that will be fed into the recurrent locks

35
00:03:00,071 --> 00:03:05,816
composed of LSTM units with the dimensions of 300.

36
00:03:05,816 --> 00:03:11,048
The output of those LSTM units are going to be projected into

37
00:03:11,048 --> 00:03:16,177
129 dimensional space corresponding to the classes,

38
00:03:16,177 --> 00:03:21,750
in which the input tokens are going to be classified.

39
00:03:21,750 --> 00:03:26,880
Now note that we do not have a specific directive here

40
00:03:26,880 --> 00:03:31,010
to say that the first sequence has 23 tokens, the second

41
00:03:31,010 --> 00:03:35,260
sequence has 15 tokens, the third one has nine tokens.

42
00:03:35,260 --> 00:03:37,490
We do not have any specific directive.

43
00:03:37,490 --> 00:03:41,050
The tool kit that we're gonna be using automatically

44
00:03:41,050 --> 00:03:45,570
takes care number of recurring units needed to process the data

45
00:03:45,570 --> 00:03:49,020
that you are going to be feeding into this trainer.

46
00:03:50,590 --> 00:03:53,250
As the training progresses it automatically

47
00:03:53,250 --> 00:03:57,300
takes care of the number of recurrent units that are needed

48
00:03:57,300 --> 00:03:58,820
to model that particular sequence.

49
00:04:01,110 --> 00:04:04,020
Since we're gonna be classifying each of the tokens

50
00:04:04,020 --> 00:04:07,130
into the corresponding 129 classes.

51
00:04:07,130 --> 00:04:10,980
We use cross_entropy_with_softmax,

52
00:04:10,980 --> 00:04:12,780
where the predicted token label

53
00:04:13,800 --> 00:04:17,360
is compared with the label provided in the training set.

54
00:04:18,910 --> 00:04:22,770
You can also use classification_error as our

55
00:04:22,770 --> 00:04:26,460
chosen function here to categorize how many

56
00:04:26,460 --> 00:04:28,450
tokens were correctly classified.

57
00:04:30,280 --> 00:04:33,415
And then instantiate the trainer object with the model,

58
00:04:33,415 --> 00:04:37,200
the loss and the error function and the learner of your choice.

59
00:04:40,120 --> 00:04:45,838
And then update the parameters in the model using the trainer

60
00:04:45,838 --> 00:04:51,331
object and feeding the trainer with multiple instances of

61
00:04:51,331 --> 00:04:57,513
mini batches of it is training samples using train_minibatch.

62
00:04:57,513 --> 00:05:02,545
Some of the learners that worked really well in case of text

63
00:05:02,545 --> 00:05:07,167
data or atom and auto grand you can try other solvers or

64
00:05:07,167 --> 00:05:11,788
learners that we have in the tool kit to see what effect

65
00:05:11,788 --> 00:05:15,503
the learners have on modelling accuracy.

66
00:05:15,503 --> 00:05:18,861
Once you are comfortable with the trained model and

67
00:05:18,861 --> 00:05:20,660
have chosen a final one.

68
00:05:20,660 --> 00:05:28,040
We can used the Test Data set to measure the error in classifying

69
00:05:28,040 --> 00:05:31,780
the tokens in the Test Data set and report the average error.

70
00:05:32,880 --> 00:05:36,319
So, to test the model of these start with the ATIS Data set.

71
00:05:37,622 --> 00:05:42,720
And draw 32 samples, in this case these are 32 sequences,

72
00:05:42,720 --> 00:05:45,250
each of the sequence having multiple tokens.

73
00:05:45,250 --> 00:05:49,240
Corresponding to each of those tokens here, there's gonna be

74
00:05:49,240 --> 00:05:53,520
a one-hot encoded label which will be used to measure

75
00:05:53,520 --> 00:05:57,270
how well our model is doing with the trained parameters.

76
00:05:57,270 --> 00:06:00,700
In this case, the model parameters are frozen,

77
00:06:00,700 --> 00:06:03,133
just like it has been done before.

78
00:06:03,133 --> 00:06:07,355
And we use the trainer.test_minibatch to

79
00:06:07,355 --> 00:06:12,718
measure what the error is going to be for that particular

80
00:06:12,718 --> 00:06:18,080
minibatch, and we repeatedly sample new data sets from

81
00:06:18,080 --> 00:06:23,670
this test data base and report the average classification

82
00:06:23,670 --> 00:06:28,373
error as the percent incorrectly label tokens.

83
00:06:28,373 --> 00:06:32,729
So now that you have trained the classifier let's see how

84
00:06:32,729 --> 00:06:37,356
the prediction work flow would look like so, say a user comes

85
00:06:37,356 --> 00:06:41,280
to your search engine and provides us string data.

86
00:06:41,280 --> 00:06:45,950
In this case, it's flight from New York to Seattle, BOS stands

87
00:06:45,950 --> 00:06:49,210
for beginning of sentence, EOS stands for end of sentence.

88
00:06:50,520 --> 00:06:56,520
And you wanna classify these tokens, in this case note that

89
00:06:56,520 --> 00:07:01,800
even though New York got two tokens they should be classified

90
00:07:01,800 --> 00:07:07,480
as the from destination and Seattle as the to destination.

91
00:07:07,480 --> 00:07:12,813
In this case there are eight tokens, one,

92
00:07:12,813 --> 00:07:18,145
two, three, four, five, six, seven,

93
00:07:18,145 --> 00:07:24,367
eight, and each of those eight tokens have one app

94
00:07:24,367 --> 00:07:29,719
encoding of vector 943 elements each.

95
00:07:29,719 --> 00:07:33,959
Each of those tokens are gonna get classified,

96
00:07:33,959 --> 00:07:39,789
into the corresponding labels using the model.eval function,

97
00:07:39,789 --> 00:07:44,771
which will take the features on the sequence data here,

98
00:07:44,771 --> 00:07:50,640
as the input, and in the corresponding probabilities.

99
00:07:50,640 --> 00:07:55,730
And these probabilities are going to be, a vector of lent

100
00:07:55,730 --> 00:08:02,210
129, one for each token so they're gonna be eight of those.

101
00:08:02,210 --> 00:08:07,530
You can use the same numpy.argmax function

102
00:08:07,530 --> 00:08:13,130
to fish out the index in this are which has the maximum value.

103
00:08:13,130 --> 00:08:17,586
Then look it up in your label dictionary to find

104
00:08:17,586 --> 00:08:22,274
the corresponding token label that each of these

105
00:08:22,274 --> 00:08:24,117
tokens belong to.

