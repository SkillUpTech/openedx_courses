{
  "start": [
    420, 
    4210, 
    7860, 
    16020, 
    21130, 
    27280, 
    33580, 
    42170, 
    47430, 
    53530, 
    59130, 
    64788, 
    72955, 
    75700, 
    79320, 
    82960, 
    87930, 
    90550, 
    93980, 
    96840, 
    102080, 
    106960, 
    113150, 
    117240, 
    120280, 
    124380, 
    129050, 
    135600, 
    139923, 
    145580, 
    152110, 
    156210, 
    160300, 
    163270, 
    166730, 
    170570, 
    173750, 
    177775, 
    179890, 
    184700, 
    186240, 
    189530, 
    193940, 
    194710, 
    196620, 
    199940, 
    203680, 
    205160, 
    210180, 
    214720, 
    217910, 
    221000, 
    223790, 
    226860, 
    230470, 
    233330, 
    236690, 
    240480, 
    246160, 
    251850, 
    256130, 
    260750, 
    262860, 
    264560, 
    267840, 
    270690, 
    276070, 
    278940, 
    282910, 
    284190, 
    289850, 
    291710, 
    295530, 
    297470, 
    298680, 
    302480, 
    305590, 
    310520, 
    315900, 
    317770, 
    318950, 
    323210, 
    328970, 
    332290, 
    336200, 
    340390, 
    345170, 
    348954, 
    356929, 
    361393, 
    366060, 
    370765, 
    374943
  ], 
  "end": [
    4210, 
    7860, 
    13430, 
    19299, 
    26000, 
    33580, 
    40860, 
    47430, 
    53530, 
    56740, 
    64788, 
    69835, 
    74190, 
    79320, 
    82960, 
    87930, 
    90550, 
    92910, 
    96840, 
    100640, 
    106960, 
    113150, 
    115900, 
    120280, 
    124380, 
    127500, 
    135600, 
    139923, 
    144380, 
    152110, 
    156210, 
    159160, 
    163270, 
    166730, 
    170570, 
    173750, 
    177775, 
    179890, 
    184700, 
    186240, 
    189530, 
    193940, 
    194710, 
    196620, 
    199940, 
    203680, 
    205160, 
    210180, 
    214720, 
    217910, 
    221000, 
    223790, 
    226860, 
    230470, 
    233330, 
    236690, 
    240480, 
    246160, 
    251850, 
    256130, 
    260750, 
    262860, 
    264560, 
    267840, 
    270690, 
    274440, 
    278940, 
    281449, 
    284190, 
    289850, 
    291710, 
    295530, 
    297470, 
    298680, 
    302480, 
    305590, 
    309350, 
    313700, 
    317770, 
    318950, 
    322010, 
    326920, 
    332290, 
    335100, 
    338880, 
    343809, 
    348954, 
    352963, 
    361393, 
    366060, 
    370765, 
    374943, 
    375787
  ], 
  "text": [
    "Now that you are familiar with the basic components of building", 
    "a model, namely the weights, biases and the activation", 
    "functions, let's try to build our first deep network.", 
    "We start with our input features.", 
    "And we put our first layer in place.", 
    "Remember we said that we are going to take this 784, so", 
    "this input vector x, which is 784 pixels", 
    "as my input and the output is going to be 400 activations.", 
    "So, this means these nodes are gonna be 400 of these, and", 
    "note i'm using the relu activation function here.", 
    "So corresponding to this layer, we have one weight matrix which", 
    "400 rows and 784 columns, and the 400 biases.", 
    "We'll add another layer.", 
    "This time we would have the input be", 
    "the output of the previous layer which is 400,", 
    "and output of this layer would be 200.", 
    "So what's the intuition behind", 
    "putting a layer on top of another one?", 
    "The key thing here is to understand", 
    "that we are trying to build one layer on top of the other.", 
    "So we are trying to first layer to the input images, the raw", 
    "pixels and transform it into some interaction between them.", 
    "And resulted in a set of features say 400 of those.", 
    "Now we take this set of 400 features and", 
    "try to find interaction between these features and", 
    "here we are gonna generate 200 of those.", 
    "So these 200 features would represent another matrix,", 
    "this layer would have a weight matrix of", 
    "200 by 400 and the 200 biases.", 
    "And now we feel that with the combination of these features", 
    "in the first layer and the interaction of", 
    "the features that we have in the second layer.", 
    "We feel pretty comfortable that let's", 
    "take these combination of the features that we have and", 
    "project it directly into the ten categories that we want to get", 
    "to, the categories being the digits zero to nine.", 
    "Now you could arguably change the number of layer,", 
    "the nodes within one layer,", 
    "say from 200 to 400 or you could add more layers.", 
    "That's something you can experiment", 
    "as you are building different models for your dataset.", 
    "And note that as you have more layers you are adding more and", 
    "more parameters.", 
    "So in this case, the first layer has weights and", 
    "biases, the second layer has more weights and biases.", 
    "So if you have more parameters you would need more data", 
    "to train on.", 
    "So a lot of time your decision to how deep your network", 
    "can be is often determined by how much data you have.", 
    "And obviously, as you have more parameters you would have to", 
    "fit those parameters, you have to find the right weights for", 
    "those, and those take computational time.", 
    "So it's always gonna be a trade off between", 
    "your model architecture, the amount of data you have and", 
    "the number of iterations or the computation time", 
    "budget that you might be having at your disposal.", 
    "So let's put the final layer here and", 
    "we have here, the input being 200 will", 
    "map it to our final 10 digits, 1 for each digit.", 
    "And note that I'm gonna use softmax because softmax does", 
    "a very good job in taking the outputs after the network and", 
    "categorizing them as probabilities.", 
    "So I'll put the activation as none.", 
    "The last layer of your deep network typically doesn't have", 
    "an activation on it and", 
    "this is going to be our first deep model.", 
    "Here, there are a lot of terminologies but the one that I", 
    "like to use is that there are two hidden layers.", 
    "And the one output layer.", 
    "So this is hidden, hidden, output, yep.", 
    "And what is the output value?", 
    "It's some value of your model function z.", 
    "It's going to be a single value and", 
    "you'll get an array of those.", 
    "And this dimension is 10, 10 values.", 
    "You will map it to the softmax function exactly like", 
    "the way we did it for that logistic regression model.", 
    "And we convert them into some probabilistic representation.", 
    "Loss functions comes next which you", 
    "are intimately familiar with.", 
    "It's very similar to what we did for logistic regression.", 
    "In this case, what we do is take the input data", 
    "and feed it into the model.", 
    "The weights and biases of the models are learned.", 
    "And when learned properly, it would have", 
    "emitted a set of probabilities corresponding to these digits.", 
    "And ideally these, for the digit three,", 
    "we would have a value of 1 and 0 elsewhere.", 
    "But that rarely happens in reality, so what we do is we", 
    "compare the predicted probability against the one hard", 
    "encoded label that comes along with the training data.", 
    "And in this case we will use the cross entropy error as my loss", 
    "function."
  ]
}