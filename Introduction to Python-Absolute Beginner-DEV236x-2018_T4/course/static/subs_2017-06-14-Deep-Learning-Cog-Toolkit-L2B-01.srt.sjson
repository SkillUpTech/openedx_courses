{
  "start": [
    200, 
    5584, 
    9200, 
    11600, 
    17872, 
    21552, 
    25312, 
    28032, 
    31376, 
    38272, 
    40336, 
    44880, 
    47552, 
    51168, 
    54512, 
    58224, 
    63344, 
    65930, 
    70048, 
    72352, 
    77280, 
    79824, 
    82176, 
    86000, 
    90896, 
    96240, 
    98520, 
    104112, 
    105584, 
    109936, 
    113310, 
    118540, 
    122040, 
    124304, 
    127776, 
    133440, 
    136896, 
    142704, 
    146576, 
    149920, 
    152752, 
    155070, 
    157856, 
    160128, 
    162480, 
    167280, 
    170128, 
    173760, 
    175488, 
    180400, 
    184330, 
    188416, 
    192176, 
    194976, 
    199840, 
    205936, 
    210096, 
    214680, 
    217712, 
    221424, 
    226000, 
    228256, 
    232352, 
    237280, 
    240416, 
    244422, 
    246320, 
    252160, 
    255240, 
    259160, 
    261360, 
    264460, 
    267900, 
    269600, 
    272160, 
    274490, 
    276540, 
    281790, 
    285520, 
    287080, 
    290780, 
    293520, 
    296480, 
    298840, 
    301940, 
    304200, 
    309540, 
    313310, 
    316100, 
    321680, 
    325000, 
    330490, 
    333200, 
    336480, 
    338600, 
    341060, 
    345210, 
    347320, 
    351220, 
    353690, 
    357200
  ], 
  "end": [
    5530, 
    9136, 
    11536, 
    17776, 
    21488, 
    25248, 
    27936, 
    31040, 
    38160, 
    40288, 
    44816, 
    47460, 
    51104, 
    54448, 
    58140, 
    63296, 
    65872, 
    69952, 
    72288, 
    77200, 
    79776, 
    82080, 
    85920, 
    90832, 
    96192, 
    98448, 
    104016, 
    105536, 
    109872, 
    113200, 
    118464, 
    121984, 
    124208, 
    127680, 
    133344, 
    136816, 
    142624, 
    146528, 
    149872, 
    152688, 
    155024, 
    157776, 
    160032, 
    162416, 
    167232, 
    170064, 
    173696, 
    175408, 
    180336, 
    184272, 
    188352, 
    192112, 
    194912, 
    199760, 
    205850, 
    210000, 
    214624, 
    217648, 
    221360, 
    225936, 
    227888, 
    232200, 
    237232, 
    240320, 
    244272, 
    245920, 
    252090, 
    255180, 
    258180, 
    261250, 
    264380, 
    267840, 
    269530, 
    271890, 
    274400, 
    276440, 
    281700, 
    285420, 
    287030, 
    290720, 
    293450, 
    296340, 
    298760, 
    301610, 
    304090, 
    307520, 
    313210, 
    316050, 
    321630, 
    324940, 
    330440, 
    333110, 
    336170, 
    338530, 
    340920, 
    345130, 
    347250, 
    351160, 
    353610, 
    357090, 
    359580
  ], 
  "text": [
    "Hello and welcome to the part 2 of the lab for module two,", 
    "Logistic Regression with the MNIST data.", 
    "I'm going to open that now from", 
    "CNTK 103 B. Okay, in the first notebook we", 
    "just reviewed, we downloaded the data", 
    "and formatted it to a local file in the CTF format", 
    "and this notebook we're actually going to read that data", 
    "and use it to train our model.", 
    "So, let's start with-- let's review", 
    "the two types of Logistic Regression.", 
    "So there's a Binary Logistic Regression and a Multi-class form.", 
    "In the binary form, we have a", 
    "single node which takes the weighted sum of the inputs", 
    "and runs that through a sigmoid function to squash the value", 
    "between 0 and 1 and treat city value", 
    "output from 0 to 0.5 is the first class prediction and anything above", 
    "0.5 as the second class prediction.", 
    "In Multi-class Linear Regression which is the form we're going to use", 
    "in this notebook, we have a", 
    "separate node for each output class that we're trying to predict.", 
    "And each node has its own set of weights", 
    "and takes the weighted sum of all the inputs", 
    "and we run collectively, we run all three", 
    "outputs into softmax such that we get out probabilities for each", 
    "selected class and the output with the largest probability is", 
    "the prediction for the associated class.", 
    "Alright, our code starts out with again bringing in some libraries", 
    "that we're going to be using.", 
    "Matplotlib, numpy, sys, os and then here we see CNTK", 
    "being brought in and we're going to refer to that as a capital C.", 
    "And again, we're using our matplotlib inline to say any plots we create", 
    "we want them to be in the output pane, not as a floating window", 
    "just to make them easier to manage.", 
    "Here's a little section and we can", 
    "ignore but what this does is, it switches the CNTK system to use", 
    "either CPU or GPU depending on", 
    "environmental setting. But this is just used for internal CNTK testing.", 
    "Alright! In our initialization section", 
    "we call np random seed,", 
    "setting that to zero, this will ensure that", 
    "because there's some fixed number here", 
    "that we're going to get a consistent set of", 
    "sequence of random numbers.", 
    "And we have two variables here,", 
    "one is input dimension and number of classes. Input dimension is going to be", 
    "our 28 by 28 pixels for each record", 
    "which is 784 values and we're trying to", 
    "predict the digits 0 through 9,", 
    "so that's a total of 10 classes that we going to output or predictions for", 
    "and here's a reminder of what our data is looking like", 
    "in the CTF format, each record will start with a vertical bar and labels", 
    "and then be followed by the 1-hot encoding of that label", 
    "and followed by features", 
    "and then the 784 values from 0 to 255", 
    "that make up the pixel Grayscale values.", 
    "So this code block here is going to", 
    "create our reader object that will use in our training", 
    "and it's job is just going to be to read the", 
    "next set of data 32 or 64", 
    "however many-- many batch, whatever many batch size we use,", 
    "it'll read that many records.", 
    "So we sort of start here label stream, we can see", 
    "we're defining in a stream, it's field, is going to be called labels", 
    "that's the vertical bar and the word it's going to look for", 
    "and it's going to have a ten values", 
    "from num label classes.", 
    "The features-- feature stream will be defined using the", 
    "features keyword with vertical bar in front of it.", 
    "And it's going to have 784 values.", 
    "Then we put both these streams together", 
    "and this CTFDeserializer.", 
    "And finally we wrap all that with a MinibatchSource.", 
    "And the MinibatchSource is our actual", 
    "reader object that we can use in our training loop.", 
    "And then some parameters here", 
    "as taking the Deserializer", 
    "and then it's saying that, using the name parameter randomize", 
    "and it sets it to true if it's training or false if it's not training", 
    "because we don't have any", 
    "reason to randomize if we're doing that just evaluation.", 
    "Randomization is really used", 
    "for during training. And our max sweeps", 
    "is going to be infinity, we're not going to", 
    "put a maximum on here, if we're doing training,", 
    "otherwise if we're doing", 
    "testing we're just going to say go through the data and maximum of one time.", 
    "Down here, we're trying to figure out", 
    "where our data is? We're trying to figure out the data directory.", 
    "Essentially, we first look in the examples directory", 
    "and if we don't find it there, we look in the--", 
    "the data in this directory and when we're", 
    "finally done, essentially we actually", 
    "loop through these two sets of directories and we try them both.", 
    "And when we find the file,", 
    "we say data found is true when we break out.", 
    "So if we didn't-- if we didn't find the file", 
    "we say ok, raise an exception.", 
    "Otherwise we print where the data is and you can see the last time I run this", 
    "it says Data directory is data MNIST.", 
    "So now we've got our directory, we've got our reader to use.", 
    "Now, we're going to create our model."
  ]
}