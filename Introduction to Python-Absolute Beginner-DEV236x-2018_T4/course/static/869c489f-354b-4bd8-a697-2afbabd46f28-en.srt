0
00:00:00,670 --> 00:00:05,190
Now that we have understood the details of the model,

1
00:00:05,190 --> 00:00:08,110
we went into a little bit of depth.

2
00:00:08,110 --> 00:00:12,270
How the parameters are calculated for each layers and

3
00:00:12,270 --> 00:00:16,337
how they influence our decisions as to what the model

4
00:00:16,337 --> 00:00:21,430
architecture should be, let's see how we can train this model.

5
00:00:24,000 --> 00:00:28,095
Just like before, we create a loss function here,

6
00:00:28,095 --> 00:00:31,110
cross_entropy_with_softmax.

7
00:00:31,110 --> 00:00:34,390
And we also create an error function,

8
00:00:34,390 --> 00:00:39,370
which will evaluate how many of the predictions we made

9
00:00:39,370 --> 00:00:42,779
are correct or incorrect in this case.

10
00:00:44,320 --> 00:00:47,790
We encapsulate that inside our criterion function.

11
00:00:47,790 --> 00:00:50,790
We are just going to do this because, unlike the previous

12
00:00:50,790 --> 00:00:54,040
tutorial, we are going to train a couple of models, and

13
00:00:54,040 --> 00:00:58,550
you want to be able to reduce code duplication as always, so

14
00:00:58,550 --> 00:01:00,440
we create some helper functions.

15
00:01:02,010 --> 00:01:04,970
The following helper function helps us print the training

16
00:01:04,970 --> 00:01:07,560
progress, which remains the same as the previous tutorial.

17
00:01:09,220 --> 00:01:12,170
In this case, we are combining the training and

18
00:01:12,170 --> 00:01:15,680
testing in one function itself, because you are not

19
00:01:15,680 --> 00:01:18,650
quite familiar with the training and test process.

20
00:01:18,650 --> 00:01:21,820
So we will do that, train and

21
00:01:21,820 --> 00:01:27,020
test in one pass itself, in one function, one code block itself.

22
00:01:27,020 --> 00:01:29,672
So we have the train_reader, we need the data for

23
00:01:29,672 --> 00:01:32,525
reading the test value so that's the test_reader.

24
00:01:32,525 --> 00:01:35,887
model func is the modeled function that you want to

25
00:01:35,887 --> 00:01:40,650
train on, and then the number of sweeps you want to train on.

26
00:01:40,650 --> 00:01:43,590
We scale the input values to a range of zero to one just like

27
00:01:43,590 --> 00:01:48,440
before here, and then we instantiate loss and

28
00:01:48,440 --> 00:01:51,086
the label_error functions using

29
00:01:51,086 --> 00:01:54,410
the create_criterion_function here.

30
00:01:54,410 --> 00:01:56,130
Rest of it is pretty boilerplate,

31
00:01:56,130 --> 00:01:59,520
just like before you set the learning rate.

32
00:01:59,520 --> 00:02:04,238
Instantiate the learner and then in this case you just

33
00:02:04,238 --> 00:02:08,940
trainer.train_minibatch(data).

34
00:02:08,940 --> 00:02:12,710
This is straight out of the train test predict workflow.

35
00:02:12,710 --> 00:02:15,240
Where in the first stage, where we are training the model.

36
00:02:15,240 --> 00:02:19,510
Once the model is trained, you can then take that model and

37
00:02:19,510 --> 00:02:23,300
evaluate how well it did in previously unforeseen data

38
00:02:23,300 --> 00:02:28,110
from the test batch, and we print the average test error.

39
00:02:32,120 --> 00:02:34,245
So once we run the model,

40
00:02:34,245 --> 00:02:39,230
here you can see we are calling the train_test function.

41
00:02:39,230 --> 00:02:47,330
And we can see that the loss decreases over iterations and

42
00:02:47,330 --> 00:02:54,070
the error also vastly reduces, and here the average tester.

43
00:02:54,070 --> 00:02:58,372
Remember that in the previous MLP model we had errors

44
00:02:58,372 --> 00:03:00,434
something like 1.7.

45
00:03:00,434 --> 00:03:06,525
And with this simple convolution network

46
00:03:06,525 --> 00:03:12,797
we have reduced the error by about .2%.

47
00:03:12,797 --> 00:03:16,850
Let's see how we can do even better.

48
00:03:20,417 --> 00:03:24,127
Before we do that, I would like you to also convince yourself

49
00:03:24,127 --> 00:03:27,320
that the model indeed has learned some values.

50
00:03:27,320 --> 00:03:30,834
So previously the z.classify.b.value had

51
00:03:30,834 --> 00:03:32,910
the parameter set as zero.

52
00:03:32,910 --> 00:03:37,659
Now you can see it has learned these values given

53
00:03:37,659 --> 00:03:40,040
the training process.

54
00:03:40,040 --> 00:03:44,600
So we have successfully been able to learn the parameters,

55
00:03:44,600 --> 00:03:49,344
sometimes investigating the parameters is quite useful when

56
00:03:49,344 --> 00:03:53,446
trying to debug models and see what's going on if your

57
00:03:53,446 --> 00:03:56,758
results are not coming the way you expect.

58
00:03:56,758 --> 00:04:00,950
The evaluation and prediction is the same as before.

59
00:04:00,950 --> 00:04:05,121
We create our model, in this case, because we are going to

60
00:04:05,121 --> 00:04:08,527
evaluate we just take the output of the model and

61
00:04:08,527 --> 00:04:11,354
pass it through the soft max function.

62
00:04:11,354 --> 00:04:14,777
And then we can do the evaluation just like that,

63
00:04:14,777 --> 00:04:16,350
just like before.

64
00:04:16,350 --> 00:04:19,330
Here you can see that most of the predictions

65
00:04:19,330 --> 00:04:21,650
are correct in this sample.

66
00:04:21,650 --> 00:04:26,720
The error rate is fairly, fairly low so we, it's like 1.52 which

67
00:04:26,720 --> 00:04:32,030
means you'll probably get two, three errors in 200 samples.

68
00:04:33,520 --> 00:04:36,090
It might be a worthwhile exercise to see what all images

69
00:04:36,090 --> 00:04:39,060
were falsely labeled and you might be able to see some

70
00:04:39,060 --> 00:04:42,270
interesting pattern why they might be falsely labeled.

71
00:04:42,270 --> 00:04:45,780
Now see as a human whether that task would be easy for

72
00:04:45,780 --> 00:04:46,860
you to do or not.

73
00:04:48,700 --> 00:04:52,830
Here are randomly sampled data mined it was one pretty simple.

74
00:04:52,830 --> 00:04:53,843
Looks pretty good.

