0
00:00:00,570 --> 00:00:05,421
Hopefully, you have had a moment to think through,

1
00:00:05,421 --> 00:00:09,610
from the previous video, how we can overcome

2
00:00:09,610 --> 00:00:13,703
the limitation of having limited history.

3
00:00:13,703 --> 00:00:16,644
So let's come back to that question.

4
00:00:16,644 --> 00:00:18,319
What can we do about limited memory?

5
00:00:20,410 --> 00:00:25,620
First thing, you can add memory, you can grow memory, right?

6
00:00:25,620 --> 00:00:30,611
We have a computer system, we can add more memory, what else?

7
00:00:33,472 --> 00:00:36,380
We can become more efficient in using the memory.

8
00:00:38,720 --> 00:00:41,460
It would mean that you would maybe

9
00:00:42,790 --> 00:00:46,470
try to forget some things that are not as relevant.

10
00:00:46,470 --> 00:00:50,950
Maybe the words like is, the, and, some of

11
00:00:50,950 --> 00:00:55,130
these repeated words that keep coming over and over, you can

12
00:00:56,310 --> 00:01:00,850
not give as much importance to them or even forget it.

13
00:01:02,380 --> 00:01:06,790
So we will see how we can add more memory to our system.

14
00:01:08,220 --> 00:01:12,527
And also become more efficient in forgetting

15
00:01:12,527 --> 00:01:16,460
those somewhat not so important things.

16
00:01:18,340 --> 00:01:22,626
And that's what the LSTM cell will enable you to do.

17
00:01:22,626 --> 00:01:28,422
Long-Short Term Memory cell is one of the possible solutions

18
00:01:28,422 --> 00:01:33,330
that are very, very influential in deep networks.

19
00:01:34,970 --> 00:01:39,740
Because it enables you to remember context for

20
00:01:39,740 --> 00:01:41,540
a much longer period.

21
00:01:41,540 --> 00:01:44,880
And is much more effective in

22
00:01:44,880 --> 00:01:49,270
solving many practical real world problems that you'll see.

23
00:01:49,270 --> 00:01:56,584
So let's start with our history at time t-1.

24
00:01:56,584 --> 00:01:58,630
Remember the dimensions.

25
00:01:58,630 --> 00:02:00,220
This is M dimensional vector.

26
00:02:02,740 --> 00:02:05,465
And we have our current input,

27
00:02:05,465 --> 00:02:11,280
x(t), which is N dimensional.

28
00:02:13,042 --> 00:02:21,500
And the capital X bar is our new Input, which combines x and h.

29
00:02:21,500 --> 00:02:26,244
You could also think of it as x star,

30
00:02:26,244 --> 00:02:30,362
which we have used last video.

31
00:02:33,613 --> 00:02:36,339
Based on this history, and the new input,

32
00:02:36,339 --> 00:02:39,460
you want to generate the new, updated history.

33
00:02:41,250 --> 00:02:43,850
And now we talked about adding new memory.

34
00:02:44,960 --> 00:02:50,628
So let's say that c stands for our new memory concept that

35
00:02:50,628 --> 00:02:55,823
has the ability to add memory to our existing set up,

36
00:02:55,823 --> 00:03:01,040
which encapsulates history through this h vector.

37
00:03:04,030 --> 00:03:08,440
And based on the new input past history and

38
00:03:08,440 --> 00:03:13,490
the past memory, we want to update our memory.

39
00:03:15,990 --> 00:03:19,400
First let's try to forget things

40
00:03:19,400 --> 00:03:22,080
that are not gonna be of much use.

41
00:03:23,430 --> 00:03:30,540
We project x through a sigmoid and a linear function here.

42
00:03:31,890 --> 00:03:35,790
And the output of this one, a value between zero and one.

43
00:03:35,790 --> 00:03:42,655
And we can multiply the output of this function and

44
00:03:42,655 --> 00:03:47,540
where element wise multiplication with my memory,

45
00:03:47,540 --> 00:03:55,450
which has the same dimensions as the output of this forget unit.

46
00:03:55,450 --> 00:03:58,590
We would be able to wipe out things that we do not want to

47
00:03:58,590 --> 00:04:03,150
remember or keep the things we want to remember and

48
00:04:03,150 --> 00:04:05,500
all the intermediate stages.

49
00:04:05,500 --> 00:04:09,130
Depending on the value of this sigmoid function.

50
00:04:11,390 --> 00:04:14,770
Because we control the what to forget and

51
00:04:14,770 --> 00:04:19,390
what not to forget, this concept is called a gate.

52
00:04:19,390 --> 00:04:21,230
And this is called the forget gate.

53
00:04:22,720 --> 00:04:27,330
Now that we have pruned our memory with things we want to

54
00:04:27,330 --> 00:04:31,870
forget, let's figure out what we want to remember.

55
00:04:31,870 --> 00:04:37,550
So we take the input X here, the capital X, and

56
00:04:37,550 --> 00:04:42,330
we project it just like we were doing earlier into an M

57
00:04:42,330 --> 00:04:46,690
dimensional output, this things is n plus m dimension, n plus m.

58
00:04:48,860 --> 00:04:53,490
And the output of this unit is gonna be m dimensional.

59
00:04:55,380 --> 00:04:59,940
And we want to keep the things that we want to update.

60
00:05:00,970 --> 00:05:05,530
That means these are the things in the processed

61
00:05:05,530 --> 00:05:09,730
input channel that we want to retain and update.

62
00:05:10,740 --> 00:05:14,257
So this is called an update gate.

63
00:05:14,257 --> 00:05:18,903
Again notice that this is a sigmoid

64
00:05:18,903 --> 00:05:24,037
function with output being 0 and 1.

65
00:05:24,037 --> 00:05:29,064
This is an essential function to be used for the gates,

66
00:05:29,064 --> 00:05:34,000
because we want to have states only between 0 and 1.

67
00:05:35,860 --> 00:05:40,340
Now that we know what to update our memory with,

68
00:05:40,340 --> 00:05:47,610
let's go ahead and add it to our pruned memory here.

69
00:05:47,610 --> 00:05:54,044
And we will update the next date of my memory which is here.

70
00:05:54,044 --> 00:05:58,818
Which is c of t-1 times the output of

71
00:05:58,818 --> 00:06:03,438
the forget date plus the input which

72
00:06:03,438 --> 00:06:07,904
combines both x and h at t-1 xt and

73
00:06:07,904 --> 00:06:14,064
multiplied with the output of the update gate so

74
00:06:14,064 --> 00:06:18,838
it will be the thing we can remember,

75
00:06:18,838 --> 00:06:23,780
and that becomes the new cell memory.

76
00:06:26,350 --> 00:06:28,540
Now that we have the new cell memory updated,

77
00:06:28,540 --> 00:06:32,560
let's figure out what we want to do with my new history.

78
00:06:34,100 --> 00:06:39,668
So here, what we do is we create a result gate

79
00:06:39,668 --> 00:06:44,933
which takes the input capital expector,

80
00:06:44,933 --> 00:06:50,800
here, and we map parts of the updated memory,

81
00:06:50,800 --> 00:06:54,877
blend it with the result gates.

82
00:06:54,877 --> 00:07:00,721
So that we know how we want to take things from my updated

83
00:07:00,721 --> 00:07:06,327
memory, and pass it as the new state of the history.

84
00:07:07,650 --> 00:07:12,771
Now that we have updated the history here,

85
00:07:12,771 --> 00:07:17,180
we can also pass it through a dense and

86
00:07:17,180 --> 00:07:22,585
a soft max layer to emit the new output state,

87
00:07:22,585 --> 00:07:25,165
should you need one.

88
00:07:28,835 --> 00:07:31,720
So you can see that mathematically,

89
00:07:31,720 --> 00:07:34,890
we are introducing a lot of parameters.

90
00:07:37,210 --> 00:07:39,843
And whenever you do a lot of parameters,

91
00:07:39,843 --> 00:07:42,407
you increase the model capacity here.

92
00:07:42,407 --> 00:07:46,651
So you can see that we have added many more weights and

93
00:07:46,651 --> 00:07:47,430
biases.

94
00:07:48,800 --> 00:07:50,560
Whenever you add these parameters.

95
00:07:50,560 --> 00:07:52,390
So in LSTM cells,

96
00:07:53,690 --> 00:07:57,210
you can see that we have added a lot more parameters.

97
00:07:57,210 --> 00:07:58,170
And by doing so,

98
00:07:58,170 --> 00:08:04,010
we have increased the capacity of the LSTM unit.

99
00:08:04,010 --> 00:08:08,010
Which is one of the popular building blocks for

100
00:08:08,010 --> 00:08:08,670
recount units.

101
00:08:09,720 --> 00:08:13,857
So instead of just having one set of weights and

102
00:08:13,857 --> 00:08:16,621
bias, you now have many more.

103
00:08:16,621 --> 00:08:21,065
This is a key concept, where you are able to increase

104
00:08:21,065 --> 00:08:25,610
the memory as well as by including the forget gates and

105
00:08:25,610 --> 00:08:29,257
updating the ones that you want to update.

106
00:08:29,257 --> 00:08:33,840
You are becoming more and more efficient in

107
00:08:33,840 --> 00:08:39,189
utilizing these existing parameters of the model

108
00:08:39,189 --> 00:08:44,770
to better capture the different time steps.

109
00:08:44,770 --> 00:08:48,960
And as you build longer recurrent units, as you saw in

110
00:08:48,960 --> 00:08:53,430
that passage generation part where we had to refer to

111
00:08:53,430 --> 00:09:00,470
the BBC, that 75 blocks away, it becomes increasingly feasible.

112
00:09:00,470 --> 00:09:04,680
There are other recurrent units like GRU or

113
00:09:04,680 --> 00:09:07,770
Gated Recurrent Units, we haven't covered that here.

114
00:09:07,770 --> 00:09:11,400
For this course, we are gonna focus on LSTM

115
00:09:11,400 --> 00:09:13,410
as our building block of recurrent units.

