{
  "start": [
    12, 
    5930, 
    10910, 
    16800, 
    21580, 
    24350, 
    28140, 
    32168, 
    36761, 
    39948, 
    42778, 
    46970, 
    50990, 
    54110, 
    57450, 
    61394, 
    65414, 
    69803, 
    73813, 
    76729, 
    79290, 
    81670, 
    85610, 
    91260, 
    97280, 
    102380, 
    107126, 
    112450, 
    115150, 
    118010, 
    119700, 
    125080, 
    131040, 
    135550, 
    140961, 
    147650, 
    153800, 
    157270, 
    161890, 
    166560, 
    169180, 
    172620, 
    176790, 
    180670, 
    185710, 
    190490, 
    194690, 
    201690, 
    205570
  ], 
  "end": [
    5930, 
    10910, 
    14250, 
    21580, 
    23150, 
    28140, 
    29530, 
    36761, 
    39948, 
    42778, 
    46970, 
    50990, 
    54110, 
    57450, 
    59620, 
    65414, 
    67229, 
    73813, 
    76729, 
    79290, 
    81670, 
    85610, 
    88140, 
    94950, 
    102380, 
    107126, 
    112450, 
    115150, 
    116890, 
    118600, 
    125080, 
    131040, 
    135550, 
    140961, 
    146600, 
    153800, 
    157270, 
    160800, 
    166560, 
    169180, 
    172620, 
    176790, 
    180670, 
    185710, 
    190490, 
    194690, 
    200420, 
    205570, 
    208580
  ], 
  "text": [
    "One-hot representation can be considered as a kind of", 
    "embedding which means you take an entity, in this case,", 
    "it was the word token and you map it into a new micro vector.", 
    "So, one-hot encoding is a Numerical representation of text", 
    "in this case, as shown here.", 
    "You have an array of 943 elements", 
    "represented as the input.", 
    "Word Embedding is another kind of encoding where specific", 
    "techniques are developed to map words or", 
    "phrases to vector of real numbers.", 
    "Note in One-hot encoding, we had either zero or one.", 
    "All the elements for zero except the one word that was present", 
    "and it's corresponding index in the vocabulary.", 
    "In the case of Word Embeddings, we will map words or", 
    "phrases to vector of real numbers.", 
    "This maps the one-hot encoded vector to a lower", 
    "dimensional space.", 
    "Instead of having a whole vector with a single number", 
    "representing that particular word,", 
    "it would compress the encoding.", 
    "So instead of having 943 elements,", 
    "it would have a much shorter representation and", 
    "those representation would have real numbers.", 
    "One example of word embedding is linear embedding.", 
    "What we do here is, with some transformation that takes", 
    "this one-hot encoding, we can compress it and represent it in", 
    "let's say, 150 element vector instead of the 943 ones.", 
    "And this could be a variable that you can tune for", 
    "however long or short you may want.", 
    "What does it do?", 
    "We first take the vector and", 
    "multiply a matrix with this one-hot encoded vector.", 
    "The vector is of size 1 X 943 times 943 elements.", 
    "And the matrix has a size of the number of elements that you want", 
    "to compress it to, times the one-hot encoding vector length.", 
    "And this matrix can be learned as a part of your network.", 
    "You could initialize it with a random number and", 
    "you can then learn this compact representation", 
    "which would transfer your one-hot encoded input into", 
    "a more dense embedding.", 
    "Or, you can use some popular embedding techniques,", 
    "such as Glove, or Word2Vec.", 
    "These are more advanced techniques, that are widely", 
    "available for use and people have code available of", 
    "these locations where you can pre-generate these embeddings.", 
    "Depending on you vocabulary and use them into your network", 
    "either directly here, or you could initialize your We matrix,", 
    "with the embedding from Glove or Word2vec, and", 
    "then subsequently refine them as you train your network."
  ]
}