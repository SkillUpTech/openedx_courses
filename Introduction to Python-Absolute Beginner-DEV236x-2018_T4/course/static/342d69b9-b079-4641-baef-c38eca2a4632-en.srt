0
00:00:00,540 --> 00:00:04,370
You have seen how adding convolution

1
00:00:04,370 --> 00:00:08,330
has helped us reduce the error rate to the MNIST data.

2
00:00:10,540 --> 00:00:13,210
Let's see how we can further improve the performance.

3
00:00:14,390 --> 00:00:17,100
One of the key aspect of training these

4
00:00:17,100 --> 00:00:20,090
networks is to guard against overfitting because

5
00:00:20,090 --> 00:00:22,660
you're dealing with a lot of parameters.

6
00:00:22,660 --> 00:00:25,270
And one of the effective ways of dealing with it is

7
00:00:25,270 --> 00:00:26,160
something called pooling.

8
00:00:27,410 --> 00:00:30,180
So pooling is typically inserted between successive

9
00:00:30,180 --> 00:00:31,720
Convolution layers.

10
00:00:31,720 --> 00:00:34,770
And the goal is to reduce the number of parameters and

11
00:00:34,770 --> 00:00:35,760
control overfitting.

12
00:00:37,520 --> 00:00:40,263
So some of the popular pooling options,

13
00:00:40,263 --> 00:00:43,430
we'll go through two of them are max pooling.

14
00:00:43,430 --> 00:00:48,950
What we do here is we take a region and

15
00:00:48,950 --> 00:00:50,750
find the maximum value within it.

16
00:00:52,640 --> 00:00:57,310
And that becomes the output of that particular patch.

17
00:00:59,010 --> 00:01:00,930
And in this case we are illustrating it with

18
00:01:00,930 --> 00:01:01,880
a stride of two.

19
00:01:01,880 --> 00:01:05,300
So we have a two by two region with a stride of two.

20
00:01:05,300 --> 00:01:10,050
So this window is going to slide over to this part here.

21
00:01:10,050 --> 00:01:15,820
And then we find the max within that region and,

22
00:01:15,820 --> 00:01:19,270
as stated here, and you repeat it over the whole image.

23
00:01:19,270 --> 00:01:22,660
So you can see that we're using max pooling, we are grabbing

24
00:01:22,660 --> 00:01:27,710
the most salient output from that particular input.

25
00:01:27,710 --> 00:01:30,810
And also at the same time reducing the number of

26
00:01:30,810 --> 00:01:35,550
parameters associated by reducing the size of the output

27
00:01:35,550 --> 00:01:36,450
of the max pooling layer.

28
00:01:37,460 --> 00:01:42,950
You can also do average pooling, which is, you slide a window

29
00:01:42,950 --> 00:01:47,233
over the different areas of the image that you have.

30
00:01:47,233 --> 00:01:51,090
And then wherever you slide that, you compute the average of

31
00:01:51,090 --> 00:01:53,190
it and that becomes the corresponding output.

32
00:01:56,281 --> 00:02:00,763
A typical convolution network with max pooling would look like

33
00:02:00,763 --> 00:02:02,830
something like this.

34
00:02:02,830 --> 00:02:04,940
So, you have a convolution layer,

35
00:02:05,960 --> 00:02:07,190
then you would add a pooling layer.

36
00:02:08,770 --> 00:02:11,600
And you can control the size of the region

37
00:02:11,600 --> 00:02:13,110
that you want to pool.

38
00:02:13,110 --> 00:02:17,140
And also the strikes associated with it.

39
00:02:17,140 --> 00:02:20,450
And you keep doing this for a few times.

40
00:02:20,450 --> 00:02:23,600
You layer it on top of the other and

41
00:02:23,600 --> 00:02:28,610
the final layer you project the output of the previous layer.

42
00:02:28,610 --> 00:02:33,540
And the output dimension is kept to 10, so that we can

43
00:02:33,540 --> 00:02:38,180
recognize the 10 digits that correspond to the input image.

44
00:02:38,180 --> 00:02:41,180
So here is the corresponding model that you can

45
00:02:41,180 --> 00:02:43,650
train as a part of your assignment.

46
00:02:43,650 --> 00:02:48,400
Here you will have a convolution to the operation

47
00:02:48,400 --> 00:02:53,670
corresponding to this particular block here and the parameters

48
00:02:53,670 --> 00:02:56,220
have one to one correspondence to each of the blocks.

49
00:02:58,210 --> 00:03:01,370
And what can you get by doing this convolution

50
00:03:01,370 --> 00:03:03,100
combined with max pooling operation?

51
00:03:05,350 --> 00:03:09,907
The error rate for MNIST data comes down to close to a 1%,

52
00:03:09,907 --> 00:03:13,627
which is a significant jump from our LR model and

53
00:03:13,627 --> 00:03:18,283
also a significant improvement over our previous models.

