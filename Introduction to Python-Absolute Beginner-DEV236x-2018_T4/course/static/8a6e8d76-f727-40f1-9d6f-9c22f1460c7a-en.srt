0
00:00:00,560 --> 00:00:05,393
Let's start walking through the process where we

1
00:00:05,393 --> 00:00:09,658
can learn this model, in our case, mx + b.

2
00:00:09,658 --> 00:00:14,018
You want to learn these parameters m and b.

3
00:00:16,182 --> 00:00:17,680
So this is Ystar.

4
00:00:17,680 --> 00:00:19,940
This is given an arbitrary x.

5
00:00:19,940 --> 00:00:23,290
We will predict the output of the solar panel.

6
00:00:23,290 --> 00:00:28,950
You can use least square fit to come up with the estimates for

7
00:00:28,950 --> 00:00:29,760
the m and b.

8
00:00:31,030 --> 00:00:36,210
However, in this course, we want to use a machinery that can

9
00:00:36,210 --> 00:00:40,000
not just fit a model with few variables, in this case

10
00:00:40,000 --> 00:00:44,840
x was just a simple scalar, But rather large number of features.

11
00:00:47,020 --> 00:00:50,650
So this machinery I'm gonna walk you through is gonna be used

12
00:00:50,650 --> 00:00:52,670
throughout the course, and

13
00:00:52,670 --> 00:00:58,010
it's fundamental to be able to train deep learning models.

14
00:00:58,010 --> 00:01:00,320
So we'll take some time and walk you through the process.

15
00:01:02,170 --> 00:01:06,130
So what you do is you sample certain amount of data.

16
00:01:06,130 --> 00:01:09,500
So you take the readings for a certain

17
00:01:09,500 --> 00:01:14,790
set of observations of X, which is my daytime temperatures for

18
00:01:14,790 --> 00:01:18,190
whatever period of time you may have and

19
00:01:18,190 --> 00:01:23,180
the corresponding predictions Y, okay.

20
00:01:23,180 --> 00:01:25,370
This set is gonna be called my training set.

21
00:01:27,540 --> 00:01:32,980
Now, what I want to do, I'm gonna take this training set and

22
00:01:32,980 --> 00:01:38,540
predate the output of the solar panel using the model x + b.

23
00:01:38,540 --> 00:01:44,390
So I'm gonna take the X which is the daytime temperature

24
00:01:44,390 --> 00:01:50,230
as my input and predict these output of the solar panel.

25
00:01:50,230 --> 00:01:56,067
Then you say, but wait a minute, how would I start with m and b?

26
00:01:56,067 --> 00:02:00,639
So what we typically do in deep learning and machine learning,

27
00:02:00,639 --> 00:02:04,779
one of the approaches is to start with a random assignment

28
00:02:04,779 --> 00:02:05,570
of m and b.

29
00:02:07,060 --> 00:02:10,190
So say we have m1 and b1.

30
00:02:13,200 --> 00:02:18,000
What we get now is we have Y, and we have Ystar.

31
00:02:18,000 --> 00:02:20,600
We can take the difference of that because for

32
00:02:20,600 --> 00:02:25,930
each of these X's I know the corresponding Y's.

33
00:02:25,930 --> 00:02:28,989
So I can take the difference and

34
00:02:28,989 --> 00:02:33,232
I can square them and this becomes my loss.

35
00:02:35,885 --> 00:02:40,501
Further more, I have a set of observations here, say n,

36
00:02:40,501 --> 00:02:43,867
that means we draw a sample here, right,

37
00:02:43,867 --> 00:02:47,820
which means we have n samples that are drawn.

38
00:02:47,820 --> 00:02:52,072
So we can sum up the losses, 1 to n, and that is gonna

39
00:02:52,072 --> 00:02:56,444
be the value that I would get in this curve right here.

40
00:02:57,800 --> 00:03:01,830
Now imagine that I try out some other set of parameters.

41
00:03:01,830 --> 00:03:05,668
So instead of m1 and b1, I get m2, b2.

42
00:03:07,550 --> 00:03:10,014
And that gives me a loss l2,

43
00:03:10,014 --> 00:03:15,740
loss 2,or let's make it easier on us, we'll call it l2.

44
00:03:15,740 --> 00:03:18,570
So this becomes l2 falls right there.

45
00:03:19,820 --> 00:03:23,050
We can continue to iterate and

46
00:03:23,050 --> 00:03:25,808
search for different parameters mn, bn.

47
00:03:27,820 --> 00:03:30,930
And at this point, say we got ln.

48
00:03:33,070 --> 00:03:38,230
So the idea is that we find these m2, b2's, mn,

49
00:03:38,230 --> 00:03:42,910
bn's such that the loss gets lower and lower.

50
00:03:44,690 --> 00:03:48,930
So you would say that this would be my choice

51
00:03:48,930 --> 00:03:54,040
of the model because it fits my training data with the least

52
00:03:54,040 --> 00:03:55,030
amount of difference.

53
00:03:56,600 --> 00:03:59,477
Now that's a big, big pitfall

54
00:03:59,477 --> 00:04:04,470
that you want to prevent yourself from falling into.

55
00:04:05,980 --> 00:04:08,210
What I mean is, when I reach this point,

56
00:04:08,210 --> 00:04:11,850
you have a model that fits your training data very well.

57
00:04:11,850 --> 00:04:16,426
So you have a model Z which is parameters by (mn,

58
00:04:16,426 --> 00:04:21,461
bn) and it fits training data really, really well.

59
00:04:23,836 --> 00:04:29,262
However, it's often the case that when you take data that

60
00:04:29,262 --> 00:04:35,016
is not in the training sample, so not in the training sample,

61
00:04:35,016 --> 00:04:40,220
the loss for not training becomes very high.

62
00:04:40,220 --> 00:04:42,100
This doesn't help us,

63
00:04:42,100 --> 00:04:46,800
because we want our models to be deployed at a later time when

64
00:04:46,800 --> 00:04:51,594
data samples drawn from outside the training set should have

65
00:04:51,594 --> 00:04:56,487
comparable performance as we had during the training phase.

66
00:04:58,744 --> 00:05:04,030
This problem is very well known and is called overfitting.

67
00:05:04,030 --> 00:05:07,530
In training model we need to be careful not to overfit.

