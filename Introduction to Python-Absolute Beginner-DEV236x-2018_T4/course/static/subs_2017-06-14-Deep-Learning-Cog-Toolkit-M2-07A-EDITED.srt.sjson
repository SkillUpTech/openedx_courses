{
  "start": [
    1530, 
    6560, 
    9440, 
    13950, 
    16070, 
    21845, 
    27160, 
    31600, 
    33144, 
    37939, 
    42136, 
    47550, 
    53680, 
    55860, 
    59260, 
    65000, 
    71460, 
    74400, 
    78430, 
    82020, 
    90440, 
    92360, 
    95550, 
    100150, 
    105680, 
    111170, 
    112340, 
    117820, 
    121470, 
    127890, 
    132720, 
    135680, 
    139410, 
    143220, 
    148090, 
    150550, 
    154930, 
    160930, 
    167080, 
    171030, 
    177100, 
    180140, 
    183610, 
    188515, 
    193025, 
    196875, 
    199965, 
    204055, 
    208305, 
    212265, 
    213580, 
    220680, 
    223770, 
    228930, 
    233270, 
    235550, 
    238680, 
    243330, 
    247063, 
    252500, 
    256280, 
    260350, 
    263510, 
    266720, 
    269420, 
    276141, 
    279889, 
    284220, 
    288000, 
    291620, 
    298070, 
    302160, 
    306590, 
    309530, 
    315580, 
    317290, 
    321590, 
    323880, 
    330660, 
    334790, 
    338740, 
    345050, 
    349040, 
    353650, 
    356500, 
    359569, 
    369021, 
    372726, 
    376354, 
    381670, 
    383850, 
    387770, 
    391230, 
    396170, 
    400610
  ], 
  "end": [
    6560, 
    7900, 
    13950, 
    16070, 
    21845, 
    27160, 
    29200, 
    33144, 
    37939, 
    42136, 
    47550, 
    52370, 
    55860, 
    59260, 
    65000, 
    71460, 
    74400, 
    78430, 
    82020, 
    86200, 
    92360, 
    95550, 
    99120, 
    105680, 
    111170, 
    112340, 
    116520, 
    119890, 
    127890, 
    132720, 
    135680, 
    139410, 
    143220, 
    146260, 
    150550, 
    153930, 
    159540, 
    165320, 
    171030, 
    174700, 
    180140, 
    181880, 
    188515, 
    193025, 
    196875, 
    199965, 
    204055, 
    208305, 
    212265, 
    213580, 
    217450, 
    223770, 
    227139, 
    233270, 
    235550, 
    238680, 
    243330, 
    244673, 
    252500, 
    256280, 
    260350, 
    262474, 
    266720, 
    269420, 
    274007, 
    279889, 
    282420, 
    288000, 
    291620, 
    296240, 
    302160, 
    306590, 
    309530, 
    315580, 
    317290, 
    319790, 
    323880, 
    329550, 
    334790, 
    338740, 
    343130, 
    349040, 
    353650, 
    355110, 
    359569, 
    365377, 
    372726, 
    376354, 
    379760, 
    383850, 
    387770, 
    391230, 
    396170, 
    400610, 
    402170
  ], 
  "text": [
    "In this section, we will train a logistic regression model", 
    "followed by testing it.", 
    "And we will then have the final model that can be deployed", 
    "as a web service or on the phone or", 
    "any application while given an unforeseen digit.", 
    "We will be able to classify it into the corresponding", 
    "digit going from zero to nine.", 
    "Let's start with our model here.", 
    "Z, which is weight", 
    "matrix times our", 
    "input feature x + b.", 
    "We will then need to compare the prediction made by the model", 
    "with the labeled data.", 
    "So each of these excess come with the corresponding labels.", 
    "This is same as what we had called y, so what", 
    "we are going to do is the loss or L for that particular image.", 
    "Say we are looking at the I of instance so", 
    "would be the loss function, whatever that might be,", 
    "between the prediction made by the model which is for", 
    "the sample, and the corresponding label.", 
    "Now in the previous module,", 
    "we had come up with different parameters.", 
    "At that time, we had said it was m1b1.", 
    "Then we had another one, m2 b2 all", 
    "the way up to (mn, bn).", 
    "In this case,", 
    "the model parameters are going to be w and b.", 
    "We'll refer to them as tita.", 
    "Our goal is to find tita one, tita two, all the way to tita n,", 
    "such that the last one is greater than loss two.", 
    "Is all the way down to loss 10.", 
    "That means we reduce the loss as we find out", 
    "new model parameters that minimize the difference between", 
    "the prediction made by the model and the corresponding label.", 
    "Now we could arbitrarily search for", 
    "these model parameters, randomly looking for them.", 
    "However, that is time consuming and may not be very efficient.", 
    "There are better ways of finding out these model parameters.", 
    "In the world of machine learning, you probably have been", 
    "introduced to the concept of gradient descent.", 
    "We will use a variant of this gradient descent", 
    "that is amenable to deep learning.", 
    "And we'll see how that variant can help us build", 
    "large models and find parameters where instead of in this", 
    "case there are only two sets of parameters, weights and biases.", 
    "These weight matrices can have larger number of", 
    "elements inside it, which is the parameter of the model.", 
    "But in deep learning these sets of parameters can be large.", 
    "And the number of elements within those parameter sets make", 
    "it even larger.", 
    "Some of the models have hundred million plus parameters.", 
    "How can we efficiently find the perimeter values", 
    "that reduces the loss over multiple iterations.", 
    "These are facilitated by techniques that are often", 
    "leveraged from the optimization literature's.", 
    "In the case of deep learning literature you will see them", 
    "appear as learners, optimizers or solvers.", 
    "They are synonymous.", 
    "So what we do is, first compute the loss for a given image.", 
    "For one image you have the probability of a given", 
    "handwritten digit belonging to the particular class here,", 
    "in this case, the handwritten digit is 3.", 
    "And that is reflected with the large value that you can see", 
    "here relative to the other values in this array.", 
    "And this last function we had called it the cross entropy.", 
    "You know that this probability vector is generated has", 
    "the output of the soft max function.", 
    "Later in the lecture, you will see that we will use a joint", 
    "function called cross entropy with softmax that combines", 
    "the cross entropy loss and the softmax operation in one step.", 
    "But before we get to that stage, let's understand how we can", 
    "identify parameters of our model that lead to", 
    "that curve which we want to get to the loss function.", 
    "Which, as we iterate, the loss would reduce progressively.", 
    "So we start with computing the loss for", 
    "one function where we take the first sample,", 
    "compute its prediction made by the model,", 
    "with some value of theta, and we compare it with the label.", 
    "This label is the one hand-encoded label and", 
    "we some it up across all the different classes that we want", 
    "to predict and get the value of loss for that 1 image.", 
    "We can add up the loss across all the images in our data set.", 
    "In this case, there are 60,000 images in our training set and", 
    "we can compute the total loss.", 
    "This we will call it,", 
    "as I is equal to 1 to mli moving forward.", 
    "Now if you plot this last function over the different sets", 
    "of parameter values that this variable theta can take,", 
    "you'll see it comes like a bowl like function here.", 
    "In this case, it's called a convex function,", 
    "and a convex function can have 1 and only 1 minimum.", 
    "The bowl like function is particularly true for", 
    "convex function, not always you'll be the bowl like function", 
    "but for logistic regression we have convex function and", 
    "it has the shape of a bowl."
  ]
}