{
  "start": [
    1260, 
    6849, 
    12323, 
    20577, 
    24018, 
    29123, 
    35070, 
    39300, 
    42450, 
    47780, 
    50070, 
    52660, 
    55660, 
    61410, 
    66080, 
    68580, 
    72130, 
    76058, 
    79835, 
    84626, 
    88308, 
    90638, 
    93869, 
    98100, 
    100800, 
    105260, 
    108830, 
    112930, 
    115140, 
    117855, 
    122740, 
    126470, 
    131453, 
    140082, 
    145853, 
    150689, 
    153487, 
    158033, 
    163207, 
    170480, 
    174120, 
    176800, 
    178150, 
    182380, 
    185300, 
    189217, 
    193200, 
    197691, 
    203951, 
    211541, 
    219974, 
    224198, 
    225520, 
    228262, 
    232829, 
    236710, 
    240700, 
    242408, 
    246530, 
    249080, 
    252810, 
    255560, 
    258027, 
    263769, 
    265937, 
    271099, 
    274922, 
    279800, 
    282580, 
    287740, 
    288400, 
    293620, 
    299880, 
    305960, 
    311340, 
    314830, 
    318690, 
    322922, 
    330040, 
    333980, 
    336530, 
    341570, 
    343820, 
    349590, 
    357020, 
    361619, 
    366764, 
    372650, 
    376700, 
    382568, 
    388219, 
    396708, 
    398125, 
    404405, 
    406000, 
    410310, 
    414230, 
    417630, 
    420330, 
    422940, 
    424860, 
    426393
  ], 
  "end": [
    6849, 
    12323, 
    17930, 
    24018, 
    29123, 
    35070, 
    39300, 
    42450, 
    45730, 
    50070, 
    51100, 
    55660, 
    59620, 
    63960, 
    67170, 
    70860, 
    76058, 
    79835, 
    82163, 
    88308, 
    90638, 
    93869, 
    96290, 
    100800, 
    105260, 
    106960, 
    112930, 
    114010, 
    117855, 
    120859, 
    125250, 
    131453, 
    135960, 
    145853, 
    150689, 
    153487, 
    158033, 
    163207, 
    169330, 
    174120, 
    176800, 
    178150, 
    180910, 
    185300, 
    189217, 
    191840, 
    197691, 
    201552, 
    208367, 
    217427, 
    224198, 
    225520, 
    226184, 
    232829, 
    236710, 
    240700, 
    242408, 
    246530, 
    249080, 
    252810, 
    255560, 
    258027, 
    260748, 
    265937, 
    271099, 
    274922, 
    279800, 
    282580, 
    285790, 
    288400, 
    293620, 
    298050, 
    305960, 
    309645, 
    312920, 
    318690, 
    322922, 
    327900, 
    333980, 
    336530, 
    339170, 
    343820, 
    349590, 
    357020, 
    361619, 
    366764, 
    371380, 
    376700, 
    381200, 
    388219, 
    393884, 
    398125, 
    402916, 
    406000, 
    410310, 
    414230, 
    417630, 
    420330, 
    422940, 
    424860, 
    426393, 
    429387
  ], 
  "text": [
    "By now, whenever I say that there is model parameters and", 
    "we are increasing the number of model parameters, I\u00b4m", 
    "assuming that you are thinking one thing, what is that?", 
    "I am thinking overfitting.", 
    "Which means that your training performance on the model", 
    "will emit really, really low error during training.", 
    "But when you are exposing that model to the outside world for", 
    "new unforseen data on which it will run,", 
    "you'll get errors much, much, much larger.", 
    "So whenever you add more parameters to the model,", 
    "you have to guard against that.", 
    "Now there are several ways of doing it.", 
    "And in neural networks, dropout is one of them.", 
    "So let's see what dropout means.", 
    "Before we get into that,", 
    "the problem we are trying to address here is overfitting.", 
    "What it means is, the model works great with training data,", 
    "with new data however, which is unseen during training,", 
    "results in high prediction error.", 
    "So, the classical approach, which I assume some of you or", 
    "most of you should be familiar with,", 
    "has a part of your ML basics is the regularization,", 
    "which is L1 and L2 regularization.", 
    "Some other techniques that are also taught in", 
    "many machine learning courses are data augmentation or", 
    "train with noise added.", 
    "Early stopping is another technique that is often", 
    "used in machine learning.", 
    "If you are not familiar with these technique,", 
    "I'll recommend that you go back to your machine learning course", 
    "to get a refresher on these topics.", 
    "In this one, we will talk about dropout, which is extremely", 
    "effective to tackle overfitting in neural networks.", 
    "You're familiar with multi-layer perception.", 
    "Let's work out what multi-layer perceptions would", 
    "be in the context of dropouts.", 
    "So dropouts means simply ignoring", 
    "the activations from certain number of", 
    "randomly chosen nodes during training time.", 
    "It was a neat little innovation from the Toronto group which is", 
    "captured in this paper.", 
    "You can look it up.", 
    "It's a fairly simple technique, but very powerful.", 
    "You choose with certain probability that you're gonna", 
    "knock off a random set of outputs or", 
    "the activations from the nodes.", 
    "Which means that during training phase, you're not gonna update", 
    "the weights and the biases associated with these nodes.", 
    "In one iteration, you may knock off the nodes shown here.", 
    "And in another instance, you may choose a different set of nodes.", 
    "So what does this mean in terms of how the activations are being", 
    "calculated?", 
    "Let's work that out.", 
    "Let's say we have the probability p with which", 
    "we are gonna drop these nodes.", 
    "And this probability can be anywhere between zero and one.", 
    "You do not want to choose zero or", 
    "one would be the degenerate case where we don't drop anything.", 
    "So we don't want to drop all the nodes, and", 
    "you don't want to keep all of them because that would be", 
    "equivalent to not having dropouts.", 
    "So now say we drop half of these nodes.", 
    "So that means probability would be 0.5.", 
    "If say I take this node and", 
    "I am gonna not update it with a probability of P to this node,", 
    "those weights aren't gonna get updated for", 
    "that particular iteration when it is not chosen.", 
    "So in this case, this situation it is chosen.", 
    "But in this situation, this one is not chosen.", 
    "In other words,", 
    "the output that would come out of this node would be scaled", 
    "back by the probability that you pass in as a parameter.", 
    "So the output would be chosen P times the output and 1 minus", 
    "P times the times that are not chosen, the output would be 0.", 
    "So effectively what you'll have", 
    "the nodes emit is p times output 2.", 
    "Now, all the nodes are gonna be randomly dropped, and", 
    "correspondingly the output will be scaled down by a factor of p.", 
    "So what do we do with a network where you have these", 
    "fully connected nodes, and", 
    "this one where the output is scaled down by P?", 
    "One thought might be,", 
    "at one time, you're gonna run inputs through all the nodes.", 
    "And at each node, you can scale it up by a factor of p.", 
    "Now remember that at test time or at predict time,", 
    "if you have to multiply each output of a node with a factor", 
    "of p, that would eat into your computation time.", 
    "And if you're running it on the device, any device,", 
    "it would eat up some cycles of that device to do the scale up.", 
    "So what popularly people do is instead of scaling the output at", 
    "test time, you can do something called an inverted dropout.", 
    "And what it means,", 
    "it performs the scaling of the neurons at training time itself.", 
    "And at prescale.", 
    "So these nodes at training time itself, will get prescaled, so", 
    "that at run time, you don't have to do any of the scaling.", 
    "So it's a neat little trick, very important one.", 
    "You don't have to worry about this, because the tool kits that", 
    "are available and the ones you are gonna use for", 
    "this course already take care of it.", 
    "But it's important for", 
    "you to understand what the dropout node does."
  ]
}