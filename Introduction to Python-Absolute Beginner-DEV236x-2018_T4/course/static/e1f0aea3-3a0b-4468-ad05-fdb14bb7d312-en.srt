0
00:00:00,460 --> 00:00:04,960
You have so far been introduced to building a model function.

1
00:00:04,960 --> 00:00:07,960
You have know about lost functions and learners,

2
00:00:07,960 --> 00:00:11,240
which help you find the optimal parameters

3
00:00:11,240 --> 00:00:12,400
while training your model.

4
00:00:12,400 --> 00:00:14,886
So, let's put these things together and

5
00:00:14,886 --> 00:00:17,808
build our first workflow for the MNIST dataset.

6
00:00:17,808 --> 00:00:21,900
So we start with the MNIST training database.

7
00:00:23,700 --> 00:00:29,230
We will sample 128 handwritten images.

8
00:00:29,230 --> 00:00:31,205
These are called mini-batches.

9
00:00:31,205 --> 00:00:36,630
These mini-batches are laid out flat.

10
00:00:36,630 --> 00:00:38,861
Just like we have talking about while building the model.

11
00:00:38,861 --> 00:00:44,790
You can see I am choosing 128 images for my mini batch sample.

12
00:00:44,790 --> 00:00:46,170
This could be any arbitrary one.

13
00:00:46,170 --> 00:00:49,300
It doesn't have to 128.

14
00:00:49,300 --> 00:00:51,969
And corresponding to those images,

15
00:00:51,969 --> 00:00:55,420
we have the labels which are one-hot encoded.

16
00:00:57,280 --> 00:01:01,497
The parameters in the logistic regression

17
00:01:01,497 --> 00:01:05,530
models have the weights and the bias.

18
00:01:05,530 --> 00:01:08,717
One weight matrix of dimensions 10 x 784.

19
00:01:08,717 --> 00:01:14,429
And the bias vector has a dimension of 10.

20
00:01:16,700 --> 00:01:18,440
Our model is very simple.

21
00:01:19,580 --> 00:01:23,901
Weight times the image pixels plus the bias vector.

22
00:01:23,901 --> 00:01:28,152
The output of the model is some values

23
00:01:28,152 --> 00:01:32,960
generated from these matrix operation.

24
00:01:35,290 --> 00:01:37,830
As I eluded to you in the previous video,

25
00:01:37,830 --> 00:01:40,340
that we are going to convert this output

26
00:01:40,340 --> 00:01:42,510
into probabilities using softmax.

27
00:01:44,280 --> 00:01:47,720
Those probabilities then feed into my loss function,

28
00:01:47,720 --> 00:01:52,400
which is cross entropy, and we use a combined

29
00:01:52,400 --> 00:01:56,480
function called cross entropy with softmax, in our case.

30
00:01:56,480 --> 00:01:58,660
An output of this function is gonna be the loss.

31
00:02:01,300 --> 00:02:06,495
Optionally, in addition to this model and the loss function, you

32
00:02:06,495 --> 00:02:11,322
can also define another function you call the classification

33
00:02:11,322 --> 00:02:15,150
error, which compares the output of the model.

34
00:02:16,860 --> 00:02:20,130
Classifies it into the corresponding category and

35
00:02:20,130 --> 00:02:24,060
compares the value with the label.

36
00:02:24,060 --> 00:02:28,440
So, in which case, if the input image was an image of

37
00:02:28,440 --> 00:02:33,860
digit 3 and the label was 3, then there would be no error.

38
00:02:33,860 --> 00:02:37,180
As opposed to if the label was 3 and the classification of

39
00:02:37,180 --> 00:02:40,820
the input image was 8, then that would generate an error.

40
00:02:42,140 --> 00:02:46,790
So you can count the number of mistakes the model makes

41
00:02:46,790 --> 00:02:49,010
using this function, called the error function.

42
00:02:51,350 --> 00:02:54,790
Now you can combine all these together

43
00:02:54,790 --> 00:02:58,320
into a trainer object which takes the model,

44
00:02:58,320 --> 00:03:03,140
the loss, the optional error, and the learner.

45
00:03:05,400 --> 00:03:10,670
And you can call the trainer train_minibatch.

46
00:03:10,670 --> 00:03:14,700
And this would update the model parameters

47
00:03:14,700 --> 00:03:18,310
from theta 1 to theta 2 and so on and so forth.

48
00:03:21,010 --> 00:03:25,663
So the learners as I'd alluded to before can be any one of

49
00:03:25,663 --> 00:03:30,019
the SGD Autograd etc, any of your hands on exercises

50
00:03:30,019 --> 00:03:33,090
you'll get to play along with that.

51
00:03:36,440 --> 00:03:39,910
In the previous module we alluded to the fact that as you

52
00:03:39,910 --> 00:03:46,130
train you also want to find out what particular settings

53
00:03:46,130 --> 00:03:50,140
are gonna be the optimal choice for your final module.

54
00:03:52,210 --> 00:03:55,023
And then one other the way of doing this is using

55
00:03:55,023 --> 00:03:58,992
the validation workflow that was explained to you in the previous

56
00:03:58,992 --> 00:03:59,580
module.

57
00:03:59,580 --> 00:04:04,366
And the idea is that you choose the minimum loss that

58
00:04:04,366 --> 00:04:08,039
you can find in your validation step and

59
00:04:08,039 --> 00:04:13,290
use those heater stars as your final model parameters.

60
00:04:13,290 --> 00:04:18,523
Once you have the final model parameters, you can

61
00:04:18,523 --> 00:04:24,760
then test how the model would perform on the test database.

62
00:04:26,300 --> 00:04:30,950
And this gives you a good idea how the model when deployed on

63
00:04:30,950 --> 00:04:35,400
the field, say in a phone or a web service or some application.

64
00:04:36,640 --> 00:04:40,570
How the performance of that model would be and

65
00:04:40,570 --> 00:04:44,700
you report out the average error by iterating through

66
00:04:44,700 --> 00:04:46,620
all the sample in the tested set.

67
00:04:48,780 --> 00:04:53,033
So here is the test workflow, you don't use the training

68
00:04:53,033 --> 00:04:57,287
dataset you use the test dataset, you sample a mini batch

69
00:04:57,287 --> 00:05:00,757
will be different from the training dataset.

70
00:05:00,757 --> 00:05:02,893
And in this case the models are already fixed.

71
00:05:02,893 --> 00:05:06,480
So w * and b * are fixed referring to the theta *

72
00:05:06,480 --> 00:05:10,810
parameter that I has illustrated in the previous slide.

73
00:05:12,090 --> 00:05:16,185
You can now feed the individual data points from your test data

74
00:05:16,185 --> 00:05:17,610
set into your model.

75
00:05:17,610 --> 00:05:22,463
And compare it with one-hot encoded labels to calculate

76
00:05:22,463 --> 00:05:25,897
the error that you would get from the test

77
00:05:25,897 --> 00:05:30,157
dataset by running Trainer.test_minibatch.

78
00:05:30,157 --> 00:05:35,161
And then finally you'd report the classification

79
00:05:35,161 --> 00:05:40,630
error as percent incorrectly labeled MNIST image.

80
00:05:40,630 --> 00:05:44,630
You can choose other error reporting functions as well, but

81
00:05:44,630 --> 00:05:46,890
classification error is one of the popular ones.

82
00:05:48,770 --> 00:05:52,821
Finally, after doing the due diligence of choosing

83
00:05:52,821 --> 00:05:55,582
the right model, and you're happy

84
00:05:55,582 --> 00:06:00,000
with the performance in the test database that you have,

85
00:06:00,000 --> 00:06:04,250
you can then deploy this particular model into an app.

86
00:06:04,250 --> 00:06:10,842
The app or the web service would sample a handwritten image,

87
00:06:10,842 --> 00:06:16,657
provided there's an input into your eval function,

88
00:06:16,657 --> 00:06:19,780
evaluation function here.

89
00:06:19,780 --> 00:06:22,590
You'll take the image as an input and

90
00:06:22,590 --> 00:06:26,700
generate the softmax probabilities associated

91
00:06:26,700 --> 00:06:30,700
with that particular image, belonging to a particular class.

92
00:06:30,700 --> 00:06:33,780
In this case, you can see that the last

93
00:06:33,780 --> 00:06:36,730
index has the large value of point seven.

94
00:06:36,730 --> 00:06:43,400
You can use a simple Numpy Argmax function to go

95
00:06:43,400 --> 00:06:47,560
through this array and find the index where you have the highest

96
00:06:47,560 --> 00:06:51,580
value in this case the nth index longs to the digit 9.

97
00:06:51,580 --> 00:06:55,496
You can do one digit at a time or you can have the eval

98
00:06:55,496 --> 00:06:59,599
function process a whole bunch of images in one go and

99
00:06:59,599 --> 00:07:04,357
generated the corresponding digit from the classifier that

100
00:07:04,357 --> 00:07:06,240
you have just trained.

