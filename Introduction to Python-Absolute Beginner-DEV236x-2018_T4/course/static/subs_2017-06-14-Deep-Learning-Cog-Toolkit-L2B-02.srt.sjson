{
  "start": [
    660, 
    2900, 
    6200, 
    7480, 
    11330, 
    14280, 
    17700, 
    19680, 
    21220, 
    25220, 
    28360, 
    32120, 
    35520, 
    38100, 
    39820, 
    43600, 
    46400, 
    48160, 
    51020, 
    54760, 
    57880, 
    62600, 
    64220, 
    66000, 
    68760, 
    72760, 
    77060, 
    78800, 
    81680, 
    83480, 
    86560, 
    90020, 
    92440, 
    96306, 
    99340, 
    102760, 
    106060, 
    107580, 
    110120, 
    113180, 
    115220, 
    119680, 
    121820, 
    124540, 
    128666, 
    130653, 
    133600, 
    135460, 
    139080, 
    142300, 
    144690, 
    149120, 
    152360, 
    158140, 
    161980, 
    165100, 
    169740, 
    171540, 
    173860, 
    175900, 
    178460, 
    181340, 
    185040, 
    188600, 
    191920, 
    194400, 
    198200, 
    201560, 
    203940, 
    206500, 
    209440, 
    211120, 
    212900, 
    215900, 
    219120, 
    220680, 
    223820, 
    225380, 
    229000, 
    232260, 
    235100, 
    238540, 
    240500, 
    242720, 
    245240, 
    247820, 
    251120, 
    253380, 
    257000, 
    260060, 
    264380, 
    268100, 
    271780, 
    273980, 
    275653, 
    278820, 
    282560, 
    286720, 
    291220, 
    294000, 
    295866, 
    297986, 
    301280, 
    304380, 
    308260, 
    311060, 
    314720, 
    319020, 
    323300, 
    325140, 
    328100, 
    331720, 
    334000, 
    338440, 
    341740, 
    345700, 
    349080, 
    351520, 
    354060, 
    356940, 
    359920, 
    364890, 
    371640, 
    373880, 
    376990, 
    379640, 
    385240, 
    389550, 
    394890, 
    400600, 
    404520, 
    408840, 
    413240, 
    418620, 
    423290, 
    426730, 
    432810, 
    438520, 
    443760, 
    447470, 
    450060, 
    455560, 
    461340, 
    467740, 
    470730, 
    475550, 
    482170, 
    488730, 
    491930, 
    496440, 
    499290, 
    501520, 
    507230, 
    509480, 
    511920, 
    514840, 
    518430, 
    522640, 
    528010, 
    530460, 
    534240, 
    539770, 
    544300, 
    549210, 
    553710, 
    556880, 
    560760, 
    563710, 
    568540, 
    574200, 
    578090, 
    584560, 
    588380, 
    592970, 
    596060, 
    602800, 
    607740, 
    612270, 
    617420, 
    623340, 
    626810, 
    631770, 
    638290, 
    642490, 
    645890, 
    649370, 
    655040, 
    660140, 
    664140, 
    668420, 
    672620, 
    675370, 
    678740, 
    684820, 
    689070, 
    693390, 
    697390, 
    700570, 
    702670, 
    706420, 
    711320, 
    713470, 
    718170, 
    721390, 
    725340, 
    728190, 
    733490, 
    737340, 
    744090, 
    750290, 
    755620, 
    759890, 
    762090, 
    766370, 
    770220, 
    774220, 
    777970, 
    781820, 
    785570, 
    790570, 
    793690, 
    796140, 
    800540, 
    805120, 
    810190, 
    812820
  ], 
  "end": [
    2800, 
    6100, 
    7410, 
    11060, 
    14160, 
    17640, 
    19180, 
    21160, 
    24740, 
    28300, 
    31980, 
    35400, 
    37960, 
    39500, 
    42720, 
    46320, 
    48100, 
    50300, 
    54700, 
    57800, 
    62180, 
    64180, 
    65940, 
    68640, 
    72660, 
    75980, 
    78720, 
    81060, 
    83453, 
    86420, 
    89720, 
    92260, 
    96240, 
    98840, 
    102706, 
    105420, 
    107500, 
    109640, 
    113100, 
    115140, 
    119180, 
    121740, 
    124440, 
    127680, 
    130540, 
    133420, 
    135200, 
    138800, 
    142200, 
    144640, 
    149080, 
    152200, 
    155040, 
    161820, 
    165040, 
    169600, 
    171460, 
    173800, 
    175560, 
    178400, 
    181280, 
    183820, 
    188520, 
    191660, 
    194340, 
    197160, 
    201140, 
    203320, 
    206400, 
    208500, 
    211020, 
    212640, 
    215780, 
    219026, 
    220600, 
    223740, 
    225300, 
    228520, 
    232080, 
    234400, 
    238470, 
    240420, 
    242040, 
    245160, 
    247340, 
    250986, 
    253260, 
    256460, 
    260000, 
    264000, 
    267980, 
    271386, 
    273920, 
    275573, 
    278060, 
    281800, 
    286520, 
    291120, 
    293880, 
    295813, 
    297460, 
    301160, 
    304240, 
    308120, 
    311000, 
    313420, 
    318600, 
    322720, 
    325020, 
    328000, 
    331500, 
    333760, 
    338320, 
    341640, 
    345520, 
    348960, 
    351200, 
    353920, 
    356280, 
    359020, 
    364800, 
    371570, 
    373800, 
    376920, 
    379600, 
    385170, 
    389500, 
    394800, 
    400550, 
    404450, 
    408770, 
    413120, 
    418550, 
    423220, 
    426670, 
    432750, 
    438470, 
    443700, 
    447420, 
    449970, 
    455500, 
    461220, 
    467600, 
    470650, 
    475420, 
    482120, 
    488650, 
    491870, 
    496370, 
    499220, 
    501470, 
    507150, 
    509420, 
    511850, 
    514750, 
    518370, 
    522550, 
    527920, 
    530400, 
    534120, 
    539700, 
    544220, 
    549120, 
    553600, 
    556770, 
    560700, 
    563620, 
    568450, 
    574120, 
    578000, 
    584470, 
    588300, 
    592890, 
    596000, 
    602420, 
    607720, 
    612250, 
    617400, 
    623320, 
    626790, 
    631750, 
    638270, 
    642470, 
    645870, 
    649350, 
    655020, 
    660120, 
    664120, 
    668400, 
    672600, 
    675350, 
    678700, 
    684800, 
    689050, 
    693370, 
    697370, 
    700550, 
    702650, 
    706400, 
    711300, 
    713450, 
    718150, 
    721370, 
    725320, 
    728170, 
    733450, 
    737320, 
    744070, 
    750270, 
    755600, 
    759870, 
    762070, 
    766350, 
    770200, 
    774200, 
    777950, 
    781800, 
    785550, 
    790550, 
    793670, 
    796120, 
    800520, 
    805100, 
    810150, 
    812800, 
    815750
  ], 
  "text": [
    "OK! To create our model let's first", 
    "review the big picture of what we're doing here?", 
    "We're taking each record of", 
    "784 pixels that's our input to our model.", 
    "The active part of our model is going to be", 
    "consisting of ten nodes each of them", 
    "fully connected to the inputs", 
    "each with their own set of weights.", 
    "So we'll end up with ten by 784 weights.", 
    "And then we'll have a bias for each of the nodes also over here", 
    "ten bias values. And the output of that", 
    "will then be used in a softmax", 
    "and cross-entropy for a loss function", 
    "we'll explain that when we get to it.", 
    "Let's scroll down here a little also up.", 
    "We have this notion of an input variable", 
    "that we'll be using and that's where we", 
    "can bind data to the model", 
    "after we built the model. So the first thing we do", 
    "is we say we're going to create an input variable", 
    "whose shape is input dim which is 784.", 
    "And we're going to call that input", 
    "that's so we're actually put in our features.", 
    "And next input variable is", 
    "shaped or sized as num output classes which is 10.", 
    "And that's going to be associated with our labeled data", 
    "and then here's our model creation", 
    "it's pretty small because this is a really simple model.", 
    "Essentially, what we're doing is this", 
    "kind of boilerplate code here", 
    "where we say let's use default options", 
    "and we're going to initialize our weights", 
    "with this function called glorot uniform,", 
    "a uniform distribution of values, random values.", 
    "And then we're going to create layer called dense", 
    "which means it's a fully called connected layer.", 
    "The number of nodes in it is going to be", 
    "the number of output classes which is 10.", 
    "And then our activation function is none", 
    "that's because we're going to take care", 
    "of the softmax kind of activation inside the loss function.", 
    "And we're passing it our input", 
    "features here. And then we return", 
    "the network that we've created, the model we've created is r.", 
    "Here, we're going to actually call that,", 
    "create model passing in our input binding", 
    "for features called input", 
    "and we'll do a divide of 255 right up front", 
    "which will see normalize our input data because our", 
    "input data is grayscale pixel values", 
    "between 0 and 255. So by dividing by 255", 
    "we'll end up with normalized values between 0 and 1", 
    "which help our training work better.", 
    "So next step is creating our loss function", 
    "this will be used to guide our optimizer", 
    "as we adjust weights and we're going to use cross entropy with softmax", 
    "which we've discussed in the lecture", 
    "it's-- it's a good loss function", 
    "when you're doing with categorical data.", 
    "The input to this is Z which is", 
    "the output of our model and label", 
    "which is our binding for our label data.", 
    "And for our metric for evaluation", 
    "we're going to use something called classification error", 
    "which is just returns a 1 if the", 
    "labels are the same and a 0 if they're different.", 
    "And we'll store that again we pass in Z and label", 
    "and we'll store that in label error.", 
    "So now we have our loss in our error function", 
    "we can actually create a trainer.", 
    "So let's work backwards here", 
    "this is the trainer we want to create", 
    "CNTK dot trainer is the function to do it.", 
    "We pass in Z we pass in our loss", 
    "and our label error which we already have.", 
    "And then we pass in-- in the list", 
    "because you could have multiple learners.", 
    "We pass in our learner and that's created just above here", 
    "for that we're using CNTK dot sgd", 
    "or Stochastic gradient descent.", 
    "We give it our model Z and then dot parameters", 
    "that returns the parameters of our model", 
    "because it's going to have to adjust those", 
    "And it's going to use a learning rate schedule", 
    "that we've defined right up above here.", 
    "And we created that with the standard", 
    "learning rate schedule from CNTK.", 
    "We gave it a learning rate of 0.2", 
    "and then we said the style of learning", 
    "is going to be minibatch. Alright!", 
    "We've got a model, we've got a trainer.", 
    "Here, we're going to define a couple of helper functions.", 
    "This first one is called moving average", 
    "and this is going to be used to", 
    "plot some data that we gather during training.", 
    "Essentially, what it does is this", 
    "little messy kind of expression here", 
    "is doing a sliding window average of five.", 
    "So every five elements it", 
    "creates a new average that slides over,", 
    "and creates a new average on that.", 
    "So that's just going to smooth out our data for plotting", 
    "then over here we have print training progress", 
    "we pass in a trainer minibatch number,", 
    "the frequency with which we want to print", 
    "and verbose flag set to one.", 
    "And we say, hey if the minibatch mod frequency is zero", 
    "that means, it's time to print. We will--", 
    "take off the trainer object", 
    "a field called previous minibatch loss average", 
    "and previous minibatch evaluation average,", 
    "will store those in two variables here.", 
    "And if it's verbose to set which is by default,", 
    "we will print out this information minibatch", 
    "the number, the loss value, the four digits", 
    "and the error value with a two decimal digits.", 
    "And then there are those parameters", 
    "and then finally we'll return", 
    "all three of those values.", 
    "Now we're ready to run the trainer,", 
    "so, the first thing we're going to do is set some parameters the minibatch size", 
    "is going to be 64. The number of samples per sweep that's 1 walk", 
    "through all of our data is going to be 60,000.", 
    "The number of sweeps to train with is 10.", 
    "So that means we're going to go through all of our data ten times for a", 
    "a total of 600,000 samples and the number of minibatches to train,", 
    "we can calculate as a number of samples per sweep", 
    "times the number of sweeps divided by the minibatch size. Ok,", 
    "so now we've come to the actual minibatch loop, so we create our reader", 
    "start and reader train, we give it our training file name", 
    "we say True because we're training we give it the input dimension 784.", 
    "The number of output classes 10, now we create an input map,", 
    "this maps between our input variables is called label and input", 
    "and the data in the stream that will be passing called the streams.label", 
    "and streams.features, this is coming from our reader.", 
    "We're going to set our progress output to 500 that mean every 500 minibatches", 
    "will print the progress, we're going to set up a variable called plotdata,", 
    "that's going to be holding it says batchsize,", 
    "but it's really the minibatch number and then the loss value", 
    "and the error value for that minibatch", 
    "and then here's the--- the actual actual loop right here.", 
    "So we set up a range to go from 0 to the number of minibatches to train minus 1,", 
    "we call on the reader object, reader train, we call next minibatch,", 
    "we give it a minibatch size and our input map.", 
    "So that will actually get us the next, since minibatch size is 64", 
    "it'll give us our next 64 records of training data and labels.", 
    "And then we call trainer.train minibatch passing the data.", 
    "So this is where our weights actually get adjusted based on", 
    "the current predictions that it makes with the current set of weights.", 
    "Then we'll call print training progress", 
    "and we'll pass it the needed parameters", 
    "and that will print every 500 minibatches", 
    "and it will also return these three values", 
    "that we're going to need to store in plotdata.", 
    "So that we can plot them later", 
    "and here you can see from the previous run,", 
    "we have minibatch going from 0, 500,000 all the way to 9000.", 
    "The law starts at 2.2 and goes lower and higher in some places", 
    "and then back downed again seems to be converging", 
    "and like wise the error starts pretty high at 80%", 
    "goes down, goes up to 12, down to 4 up to 9 back down to 3.", 
    "So it looks like it's converging to be great, if we could see this plotted", 
    "so that's the next section of code. This uses Matplotlib", 
    "to do the actual plotting, first though we're going to take our plotdata", 
    "and we're going to run it through our moving average helper function", 
    "that'll smooth out our data. So we start with plotdata loss,", 
    "we store it in something called plotdata average loss", 
    "and we take our plotdata error and we stored in plotdata avgerror.", 
    "Now we import matplotlib.pyplot as we're going to call that plt", 
    "and this will create a figure that we can plot into", 
    "this is a one of two plots and we'll start with the first one", 
    "and we're going to plot the plotdata batchsize,", 
    "which is the minibatch number against,", 
    "so this will be in the X and the Y would be the average loss", 
    "and that's what we see here. And so this is loss and we can see it, converging down.", 
    "We also only set the X label the Y label on the title.", 
    "Let me show it then we go to the second plot and we do a similar thing", 
    "or here we're back we're plotting the Minibatch number against,", 
    "the average error and we set the labels and we set the title and we show it", 
    "and so we can see that here.", 
    "All right so it looks like both our loss and our error are converging", 
    "and let's see how that model now holds up against test data.", 
    "So to evaluate it on test data we're going to have another", 
    "Minibatch loop like we did before it's going to look very similar only", 
    "things we used to say train are now going to say test.", 
    "So we have a reader underscore test the creating reader", 
    "from test underscore file and this time this parameter for training is false", 
    "and these two are the same. We're going to have a test input map", 
    "that associates our input variable label", 
    "with our streams.labels and our input variable called input", 
    "with our stream stream.features.", 
    "We set some parameters this time we're using", 
    "any batchsize of 512 and that's okay.", 
    "Because this is just test data we can actually run through as many", 
    "records at a time as we want as long as they can all fit into memory, 512", 
    "should be no problem at all. The number of samples is going to be 10,000", 
    "that's how many samples we have in our test set", 
    "and the number of Minibatches to test", 
    "we're going to say is the number of samples divided by the Minibatch size", 
    "and so far we're going to accumulate an average test result first a total", 
    "and then we'll divide it by the number of entries.", 
    "So it's going to start at 0 here's our for loop again,", 
    "this time we're controlling it by the number of Minibatches to test.", 
    "Here's our reader test getting the next batch of data,", 
    "next Minibatch passing the size and the map.", 
    "Then once we have the data we can call trainer.test this time last time", 
    "it was just want to point this out this", 
    "difference last time it was trainer.train Minibatch.", 
    "So that actually does weight adjustment this call trainer.test Minibatch", 
    "only does evaluation and it gives us back an error, evaluation error", 
    "that's our metric and we're going to just accumulate that and test result", 
    "and then at the end here.", 
    "We'll say divide it by the number of Minibatches to test,", 
    "that gives us an average we multiply it by a hundred to get a percentage", 
    "and we print that out average test error and it with two decimal places", 
    "and that's shown here 7.4 percent.", 
    "So that's not bad for logistic regression really the simplest", 
    "kind of model you can imagine,where we're in", 
    "single-digit error ballpark that's pretty good.", 
    "So now we're going to do a little prediction", 
    "this kind of simulates a deployment environment.", 
    "We want to take some data and predict just what the values are. So,", 
    "we're going to extend our models with one node,", 
    "we're going to actually put a soft max output now on z,", 
    "which is the previous last node of the model and it's", 
    "now going to give us a new output called out."
  ]
}