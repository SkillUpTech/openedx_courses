0
00:00:00,780 --> 00:00:03,540
So now you know that, if we continue to

1
00:00:03,640 --> 00:00:08,010
connect all the image pixels even using

2
00:00:08,080 --> 00:00:10,240
this patching scheme which takes into

3
00:00:10,300 --> 00:00:12,450
account the neighbourhood adjacency

4
00:00:12,780 --> 00:00:18,560
of the pixels. We end up having a huge

5
00:00:18,640 --> 00:00:21,350
number of parameters in our network.

6
00:00:21,740 --> 00:00:24,480
So what I mean by that here, let's go back a little bit

7
00:00:24,980 --> 00:00:29,560
and see as we scan through the image,

8
00:00:29,940 --> 00:00:35,020
each time we are learning a new set of weights.

9
00:00:35,640 --> 00:00:39,220
And that leads to a huge explosion in the number of

10
00:00:39,320 --> 00:00:41,020
parameters that we need to estimate.

11
00:00:42,520 --> 00:00:46,173
Convolution Networks is somewhat key

12
00:00:46,300 --> 00:00:50,000
to understanding how we can build deep networks.

13
00:00:51,100 --> 00:00:56,220
What we will do here is again do the scanning except for the fact that

14
00:00:56,480 --> 00:01:01,460
the Kernel weights that are being learned get updated

15
00:01:01,840 --> 00:01:06,560
every time we run the kernel through the entire image.

16
00:01:06,900 --> 00:01:11,893
In other words in a Convolutional layer, a single set of weights

17
00:01:12,160 --> 00:01:16,730
is being shared by all output nodes within the current filter.

18
00:01:17,140 --> 00:01:20,540
So consequently the values that get

19
00:01:20,660 --> 00:01:28,690
updated are only these three times, three values of the same weight matrix

20
00:01:28,800 --> 00:01:36,490
and the bias ofcourse. So with that for a filter shape of 3x3

21
00:01:36,640 --> 00:01:39,690
we have to learn only ten parameters.

22
00:01:40,600 --> 00:01:46,820
Now what we can do is try to train as many filters as possible.

23
00:01:48,420 --> 00:01:50,970
Now you would imagine that if I trained

24
00:01:51,060 --> 00:01:54,060
another filter, how would it be different

25
00:01:54,160 --> 00:01:57,840
from this filter. But remember during the

26
00:01:57,920 --> 00:02:02,460
initialization phase of our convolution operation

27
00:02:02,660 --> 00:02:05,020
this is applicable to the

28
00:02:05,070 --> 00:02:09,440
dense network as well. But in Convolution even more so

29
00:02:10,030 --> 00:02:13,380
that you would have multiple filters each of them would be

30
00:02:13,470 --> 00:02:17,730
initialized to a different value. OK.

31
00:02:17,800 --> 00:02:20,140
So you would have the first kernel

32
00:02:20,230 --> 00:02:23,290
that would have learned something about

33
00:02:23,320 --> 00:02:25,020
maybe the brightness of the image.

34
00:02:25,100 --> 00:02:28,940
Then you would have the second kernel, the orange one

35
00:02:29,000 --> 00:02:31,180
which would have learned something about

36
00:02:31,230 --> 00:02:33,020
maybe the contrast of the edges.

37
00:02:33,060 --> 00:02:35,020
And so and so forth you will build

38
00:02:35,060 --> 00:02:38,840
n filters. The total number of parameters

39
00:02:38,920 --> 00:02:41,890
for this filter set with n filters would

40
00:02:41,950 --> 00:02:46,620
be 9 times n plus n, a much smaller number.

41
00:02:46,660 --> 00:02:53,140
So in case of a 3x3 filter with 5 layers

42
00:02:53,230 --> 00:02:54,960
this would have 10 parameters,

43
00:02:55,040 --> 00:02:59,970
you would get about 500 parameters. OK.

44
00:03:00,200 --> 00:03:03,560
With larger images the number of parameters

45
00:03:03,660 --> 00:03:06,000
doesn't increase anymore. So instead of

46
00:03:06,080 --> 00:03:08,020
39 million you would get something like

47
00:03:08,080 --> 00:03:12,060
a thousand parameters. What does it let us do?

48
00:03:12,110 --> 00:03:16,520
It allows us to handle larger images

49
00:03:16,580 --> 00:03:21,330
trying larger filters, this allows us

50
00:03:21,420 --> 00:03:23,780
to learn more different kinds of

51
00:03:23,840 --> 00:03:28,850
spatial correlations, a relationship between the pixels within the patch.

52
00:03:28,960 --> 00:03:32,640
The patch is larger, we can learn more number of filters.

53
00:03:32,700 --> 00:03:35,220
So these can detect different

54
00:03:35,280 --> 00:03:37,170
kinds of relationship between the

55
00:03:37,230 --> 00:03:41,760
different patches and also

56
00:03:42,270 --> 00:03:46,090
deeper architecture with may be as high as 150

57
00:03:46,160 --> 00:03:50,170
odd layers. So what are these layers learning?

58
00:03:50,200 --> 00:03:54,690
Let's talk about the some of the first few layers

59
00:03:55,000 --> 00:03:57,970
we call the primitive features, they learn the edges.

60
00:03:58,040 --> 00:03:59,860
You can see that here it has learned

61
00:03:59,900 --> 00:04:02,000
how to detect the edge in this direction

62
00:04:02,060 --> 00:04:04,170
and this one has learned in that direction

63
00:04:04,270 --> 00:04:08,730
so and so forth. OK.

64
00:04:09,220 --> 00:04:12,250
If it was a color image you would have learned

65
00:04:12,350 --> 00:04:17,330
some of the color features and the

66
00:04:17,400 --> 00:04:21,000
deeper layers learn how to use these

67
00:04:21,070 --> 00:04:23,900
primitive features and detect more

68
00:04:23,960 --> 00:04:28,990
complex features like corners or combinations of

69
00:04:29,050 --> 00:04:35,010
different patches and different localized features in the image itself.

