0
00:00:00,650 --> 00:00:04,100
Let's build our CNN model now that you know

1
00:00:04,100 --> 00:00:07,020
the process of convolution and the strides and padding,

2
00:00:07,020 --> 00:00:09,750
which was a recap from the video before.

3
00:00:11,570 --> 00:00:14,990
In this case, we have two inputs.

4
00:00:14,990 --> 00:00:19,972
And the first one is the input data, the pixel data,

5
00:00:19,972 --> 00:00:23,708
which is specified by the input image,

6
00:00:23,708 --> 00:00:29,502
input_dim_model, which is specified to 128, 28.

7
00:00:29,502 --> 00:00:33,324
And the output here are going to be the number of classes and

8
00:00:33,324 --> 00:00:36,912
is also the labels, and the dimensions of the labels,

9
00:00:36,912 --> 00:00:39,350
which is one-hat encoded, again.

10
00:00:39,350 --> 00:00:44,154
This is the network we are gonna code, which is the first

11
00:00:44,154 --> 00:00:49,382
layer has a filter shape of 5 cross 5, it emits 8 filters.

12
00:00:49,382 --> 00:00:52,880
And we'll have strides of 2,2,

13
00:00:52,880 --> 00:00:58,368
which means in each dimension the stride is gonna be 2.

14
00:00:58,368 --> 00:00:59,480
And we are gonna pad.

15
00:01:00,480 --> 00:01:03,023
Activation is gonna be below here.

16
00:01:03,023 --> 00:01:06,052
Then the second layer is the same thing except we are gonna

17
00:01:06,052 --> 00:01:08,019
learn a more number of filters here.

18
00:01:08,019 --> 00:01:10,788
So imagine that the first layer actually

19
00:01:10,788 --> 00:01:15,360
get you the basic filters, basic characteristics of the images.

20
00:01:15,360 --> 00:01:19,921
And the second layer builds on top of those

21
00:01:19,921 --> 00:01:23,440
filters that we have learned and

22
00:01:23,440 --> 00:01:29,189
enables higher order features that are generated.

23
00:01:29,189 --> 00:01:33,795
We can have arbitrary number of convolutional

24
00:01:33,795 --> 00:01:38,755
layer stack over one another but the final task for

25
00:01:38,755 --> 00:01:43,714
us is to be able to predict the number of digits that

26
00:01:43,714 --> 00:01:47,040
we have in our training setting.

27
00:01:47,040 --> 00:01:48,620
In this case, it's 10.

28
00:01:48,620 --> 00:01:52,960
So the output value of the network has to be

29
00:01:52,960 --> 00:01:54,820
a vector of 10.

30
00:01:54,820 --> 00:01:58,668
In this case, we pass the output of the convolutional layer as

31
00:01:58,668 --> 00:02:01,040
the input to the dense layer.

32
00:02:01,040 --> 00:02:04,560
Which then gets projected into the output dimension of 10 here.

33
00:02:04,560 --> 00:02:08,470
And note that we're gonna use cross entropy with softmax

34
00:02:08,470 --> 00:02:09,410
as our last function,

35
00:02:09,410 --> 00:02:13,650
hence the last layer has the activation specified as none.

36
00:02:13,650 --> 00:02:15,350
Let's see how the code looks like.

37
00:02:15,350 --> 00:02:19,010
It's very simple as we've seen in the video lectures.

38
00:02:19,010 --> 00:02:22,476
We name the first layer to be first_conv,

39
00:02:22,476 --> 00:02:27,495
second one to be second_conv and the last layer as classify.

40
00:02:27,495 --> 00:02:30,953
Now this can be quite useful for you to do so,

41
00:02:30,953 --> 00:02:35,627
which is name the layer so that at the later time you can go and

42
00:02:35,627 --> 00:02:38,152
inspect what the parameters and

43
00:02:38,152 --> 00:02:42,850
the input output shapes are associated with those layers.

44
00:02:42,850 --> 00:02:47,630
Let's see what holds for the data that we have so far.

45
00:02:48,660 --> 00:02:52,757
So we create this model function X, and

46
00:02:52,757 --> 00:02:56,491
note that I have, X is here, what?

47
00:02:56,491 --> 00:03:01,681
X is my input, with the dimensions specified

48
00:03:01,681 --> 00:03:06,470
by this variable, input_dim_model.

49
00:03:06,470 --> 00:03:11,215
So we've created our model with that input and the first thing

50
00:03:11,215 --> 00:03:15,332
we see is the shape of the first convolutional layer.

51
00:03:15,332 --> 00:03:19,703
In this case you can see it's 8 times 14 times 14.

52
00:03:19,703 --> 00:03:24,002
Go back to the animations and the lectures if you are not

53
00:03:24,002 --> 00:03:27,940
convinced that is the right number as shown here.

54
00:03:27,940 --> 00:03:31,134
And once you convinced yourself you can go and

55
00:03:31,134 --> 00:03:34,076
investigate the second layer which I will

56
00:03:34,076 --> 00:03:38,125
provide the code yet, maybe you can do it as an exercise.

57
00:03:38,125 --> 00:03:42,038
And you can investigate the individual weights and

58
00:03:42,038 --> 00:03:44,863
the biases for those layers as well.

59
00:03:44,863 --> 00:03:46,838
Just for illustration,

60
00:03:46,838 --> 00:03:51,162
I have shown you how you can inspect the value of the bias

61
00:03:51,162 --> 00:03:55,871
layer of the last dense layer which is named classify here.

62
00:03:55,871 --> 00:04:01,497
Okay, here are the ways you can understand

63
00:04:01,497 --> 00:04:05,567
the model a little bit better.

64
00:04:05,567 --> 00:04:08,695
One of the things we do to suggest here is to understand

65
00:04:08,695 --> 00:04:11,139
the number of parameters your model has.

66
00:04:11,139 --> 00:04:15,185
And this is a function you can use to print out the number of

67
00:04:15,185 --> 00:04:18,210
parameters that this simply model had.

68
00:04:18,210 --> 00:04:21,036
And this one has 11,274.

69
00:04:21,036 --> 00:04:23,827
Now if you're wondering where these numbers are coming from,

70
00:04:23,827 --> 00:04:26,560
let's tackle the parameters tensors first.

71
00:04:26,560 --> 00:04:31,884
The six parameter tensor correspond to the weight and

72
00:04:31,884 --> 00:04:35,200
bias pair for each of the layers.

73
00:04:35,200 --> 00:04:38,722
So we have two convolution layers, which make up four

74
00:04:38,722 --> 00:04:42,182
parameter tensors and then we have one dense layer.

75
00:04:42,182 --> 00:04:45,164
So there comes another set of weight and bias.

76
00:04:45,164 --> 00:04:49,617
So a total of six parameters are being estimated.

77
00:04:49,617 --> 00:04:54,035
Now let's see what happens in the first convolution layer.

78
00:04:54,035 --> 00:04:59,579
The input data is of the dimension 128,28.

79
00:04:59,579 --> 00:05:00,923
So in this case,

80
00:05:00,923 --> 00:05:06,107
the filters that are gonna be learned will have the size 1, 5,

81
00:05:06,107 --> 00:05:10,910
5 because 5 and 5 are the filter sizes we have specified.

82
00:05:10,910 --> 00:05:14,817
And the output, we have 8 filters.

83
00:05:14,817 --> 00:05:20,492
So total number of values we will learn here are 200,

84
00:05:20,492 --> 00:05:23,400
which is 8 times 1 x 5 x 5.

85
00:05:23,400 --> 00:05:28,335
And then there are gonna be eight bias values correspond

86
00:05:28,335 --> 00:05:30,230
one for each filter.

87
00:05:30,230 --> 00:05:33,929
You can do the second convolution similarly, but

88
00:05:33,929 --> 00:05:38,244
except if you notice here, the input dimension of the next

89
00:05:38,244 --> 00:05:42,222
layer is the output dimension of the previous layer.

90
00:05:42,222 --> 00:05:46,187
In this case we have eight filters in the previous layer.

91
00:05:46,187 --> 00:05:51,965
So the dimension becomes 8 x 5 x 5, this is very important.

92
00:05:51,965 --> 00:05:54,828
So the total number of parameters coming in

93
00:05:54,828 --> 00:05:59,209
from this layer, is gonna be 16 x 8 x 5 x 5 is just 3,200.

94
00:05:59,209 --> 00:06:02,558
And then you can add 16 bias values corresponding to those 16

95
00:06:02,558 --> 00:06:03,470
filters.

96
00:06:03,470 --> 00:06:05,024
The last layer is the dense layer,

97
00:06:05,024 --> 00:06:07,067
which you are quite familiar how that works.

98
00:06:07,067 --> 00:06:11,459
It's all pairwise connected, so there are gonna be the however

99
00:06:11,459 --> 00:06:15,536
many outputs of the previous layer, which is 16x7x7.

100
00:06:15,536 --> 00:06:18,553
If you are not sure where it came from please run the code

101
00:06:18,553 --> 00:06:21,047
for the second layer, print out its shape and

102
00:06:21,047 --> 00:06:23,483
convince yourself that is the right thing.

103
00:06:23,483 --> 00:06:27,070
And then you can multiply it with the number of output values

104
00:06:27,070 --> 00:06:29,581
because these are pairwise connected and

105
00:06:29,581 --> 00:06:31,818
you will get the number 11274.

106
00:06:31,818 --> 00:06:37,100
Now why this is super important to understand the parameters,

107
00:06:37,100 --> 00:06:41,783
it is because it is kind of tied to the amount of data set or

108
00:06:41,783 --> 00:06:44,873
the size the data set you may need to be

109
00:06:44,873 --> 00:06:47,081
able to train this model.

110
00:06:47,081 --> 00:06:50,114
The number of parameters are very large.

111
00:06:50,114 --> 00:06:54,304
You would need a larger amount of data to train the model to

112
00:06:54,304 --> 00:06:56,407
prevent from overfitting.

113
00:06:56,407 --> 00:07:00,398
So it's super important that you understand the model complexity

114
00:07:00,398 --> 00:07:03,035
and also the number of parameters being more

115
00:07:03,035 --> 00:07:06,752
besides requiring more data would also take longer to train.

116
00:07:06,752 --> 00:07:11,365
And you will have, at run time, also you will have to evaluate

117
00:07:11,365 --> 00:07:15,981
the forward pass in the forward pass compute all the values for

118
00:07:15,981 --> 00:07:19,115
the layer to be able to do the prediction.

119
00:07:19,115 --> 00:07:22,436
So it's important you recognize the model complexity.

