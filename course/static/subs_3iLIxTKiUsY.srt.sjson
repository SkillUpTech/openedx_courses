{"start":[770,3750,6330,8070,9375,11640,15195,17130,18970,20580,23580,26790,30150,32895,35070,37635,41970,45060,49920,51600,54090,56490,59660,61230,62430,64685,67740,71460,75465,77453,78860,81150,83610,88490,91140,94080,96300,98745,103580,107840,109385,111725,112980,114630,117240,118590,121665,124500,126150,127950,131545,133140,134580,136950,139920,141810,145565,147960,150630,152670,155980,158565,161100,163225,166595,168210,169725,171300,173370,176475,179370,183180,185505,187725,190455,193380,196435,199800,201930,204715,207060,210660,214260,218170,221430,223200,226110,227955,229410,231750,233765,236100,237540,239325,241040,243225,245070,246915,248610,250605,252765,255620,258895,260520,262110,264230,266010,267995,270545,273990,278095,280440,283560,284640,286495,288660,291180,295365,298095,300495,302360,304860,308105,309845,311810,315215,317505,319790,321910,323050,324470,328340,330380,332945,336180,338195,340050,343545,346200,350230,353665,355810,358167,361675,364785,367860,369240,371700,374365,376095,378320,379470,381030,383010,384510,386460,389280,391100,393750,397230,401340,403140,405750,407475,409835,411045,412995],"end":[3750,6330,8070,9375,11640,15195,17130,18970,20580,23580,26790,30150,32895,35070,37635,41970,45060,49920,51600,54090,56490,59660,61230,62430,64685,67740,71460,75465,77453,78860,81150,83610,88490,91140,94080,96300,98745,103580,107840,109385,111725,112980,114630,117240,118590,121665,124500,126150,127950,131545,133140,134580,136950,139920,141810,145565,147960,150630,152670,155980,158565,161100,163225,166595,168210,169725,171300,173370,176475,179370,183180,185505,187725,190455,193380,196435,199800,201930,204715,207060,210660,214260,218170,221430,223200,226110,227955,229410,231750,233765,236100,237540,239325,241040,243225,245070,246915,248610,250605,252765,255620,258895,260520,262110,264230,266010,267995,270545,273990,278095,280440,283560,284640,286495,288660,291180,295365,298095,300495,302360,304860,308105,309845,311810,315215,317505,319790,321910,323050,324470,328340,330380,332945,336180,338195,340050,343545,346200,350230,353665,355810,358167,361675,364785,367860,369240,371700,374365,376095,378320,379470,381030,383010,384510,386460,389280,391100,393750,397230,401340,403140,405750,407475,409835,411045,412995,417320],"text":[">> For those of you that would like to go ahead and play","more with Spark to Cosmos DB.","I'm going to show you a quick demo","of this Notebook which is","the On-Time Flight Performance","using Apache Spark and Azure Cosmos DB.","This Notebook is available for you to","download so you can go ahead and play with it.","So we're just going to work with","specific components of this notebook just to give you","a taste of the power between combining","the distributed query engine that is","Apache Spark and the distributed storage engine","that is Cosmos DB.","The value behind it is that,","because Apache Spark is a distributed query engine with","multiple worker nodes and Cosmos DB is","a distributed storage engine with multiple storage nodes.","We can parallelize the","two such that the queries are going","from the individual worker nodes","to the individual data nodes of Cosmos DB,","so that way we can actually parallelize","the throughput and increase","the performance of your queries.","And so, this way Spark is able to make use of","Cosmos DB's native indexing to do predicate","push-down filtering to get only the information it needs.","And in order to combine the two together,","it's actually quite easy.","This first part of the Cosmos,","sorry, the Jupyter Notebook that we have here, actually,","is simply referencing the Spark connector jars.","So once you've configured","your Spark cluster to reference these jars,","there's actually only one additional command and this is","actually a PySpark Notebook that we have here,","where we specify in here then the connection points.","This is to my doctorwho Cosmos DB database.","This is the connection key.","I'm specifically looking for DepartureDelays.","That's the database in","the collection that I'm actually","accessing right now as we speak.","And then once we've run that,","we're able go ahead and uses one quick command,","the spark.read.format and once","we've done this, we've actually,","from this point onwards, you're just running","Spark queries directly against Cosmos DB.","So for example, if I want to run","a simple query, in other words,","what flights are originating from Las Vegas","instead of actually running any other queries,","I'm just running Spark queries and it's","automatically filtering the data from Cosmos DB.","I want to look at the top 10 delays.","So this is the query that I have in terms of","top 10 delays from","departing from Las Vegas group by the city.","And sure enough here we go. We have San Francisco,","Los Angeles, Denver, and so forth.","We can even do fun things like calculating","the median delays between the various different cities.","And by doing this, you can actually","quickly scan through","all the different cities departing from","Las Vegas using a combination of","Spark and Cosmos DB together to figure out","which cities are more inclined to have delay","and which cities are more inclined to depart on time.","Now, one of the cool things is,","in addition to the ability to run","graph gremlin queries within Cosmos DB,","you can actually also run Spark graph frames or","graph ex-queries within the exact same containers.","So as data is being loaded into Cosmos DB,","it's automatically showing up in","the container and Spark's able to make use of it.","So I'm now running in this particular case,","the same Spark Sequel queries but now against","my running graph frames queries as opposed to just","running standard Sequel queries.","And so, this first one is basically just again the delay.","And so, this case we're actually","simply looking at what flights are departing","from Las Vegas with the significant","average delays and it looks","like Honolulu unfortunately is","one of the more significant ones.","But once we switch over to","graph frames as opposed to","reading just stack Sequel queries,","I could run a query like this.","In this case, tripGraph.degrees.sort,","what that does is simply say,","from a graph perspective,","the number of degrees that are","associated with the vertex, in other words,","vertex is the airport and","the edges are the flights that go into it.","So then more number of edges that are going to a vertex,","the larger the degrees.","So which airport that case","actually has the most number of degrees?","So in other words, which airport","has the most important connections?","And in this particular data set,","quickly it's Atlanta and that does make","sense since Atlanta is the busiest airport in the world.","So with some additional fun stuff,","I want to leave or end off with,","are there any direct flights","with a breadth first search query?","You'll know, says BFS right here.","Again, these are relatively complicated to","run in Sequel but quite easy to run,","whether you're running a Cosmos DB graph gremlin query","or a Spark graph frames query.","And so, this case we can see that there are","plenty of flights between Seattle and San Jose.","And it's a direct flight because maxPathLength one","means that there is only one edge","between the two vertexes.","In other words, one flight between Seattle and San Jose.","But as you can see, there's plenty of flights.","But how about if I just go ahead and say,","\"Hey, between San Jose and Buffalo.\"","Well, in this case, as we","see Buffalo maxPathLength is one","but you'll notice the output is nothing.","In other words, there are no direct flights","between San Jose and Buffalo.","So let's go ahead and change that to two.","You'll notice that all I do is say, \"Okay,","give me the breadth","first search of San Jose to Buffalo where maxPathLength","was two which means I'm allowed two edges","between San Jose vertex and Buffalo vertex.","Meaning, there'll be a layover in one city.","In other words, two flights one layover city.","And it looks like there are a ton of flights now again.","And again, you'll notice that Boston is pretty popular.","But what's the most popular layover?","So far, I've been able to run some pretty fast queries","against the data and","get back the results relatively quickly.","But now, I want to figure out the transfer point.","Well, this is great. I can run","Sequel type queries right back again.","So I take it the paths,","I'm going to group by the ID and","the city and in this case,","the output you'll notice it's","actually not Boston which is","the sixth most popular layover city","between San Jose and Buffalo,","but it's actually Las Vegas.","So there's a lot of good stuff here where you can combine","the powers of Apache Spark and Azure Cosmos DB together.","This Notebook actually is quite lengthy,","so I encourage you to go ahead and give it a try.","So you can play","with the data and it's all available online,","both the Notebook and the data,","so you can play with it and try out","new and more interesting ways to explore your data."]}